{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  8 14:55:15 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A30                     Off | 00000000:01:00.0 Off |                   On |\n",
      "| N/A   31C    P0              29W / 165W |     50MiB / 24576MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                          |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                |        ECC|                       |\n",
      "|==================+================================+===========+=======================|\n",
      "|  0    1   0   0  |              25MiB / 11968MiB  | 28      0 |  2   0    2    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from math import floor \n",
    "from typing import Callable, Tuple, Any\n",
    "\n",
    "import jax\n",
    "from jax import debug\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from chex import ArrayTree\n",
    "from qdax.core.containers.repertoire import Repertoire\n",
    "from qdax.types import Descriptor, ExtraScores, Fitness, Genotype, RNGKey\n",
    "from qdax.environments.base_wrappers import QDEnv\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition, QDMCTransition\n",
    "#from qdax.core.neuroevolution.buffers.trajectory_buffer import TrajectoryBuffer\n",
    "import flashbax as fbx\n",
    "import chex\n",
    "from rein_related import *\n",
    "\n",
    "from qdax.core.emitters.emitter import Emitter, EmitterState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MPLCONFIGDIR'] = '/tmp/matplotlib'\n",
    "os.environ['WANDB_CACHE_DIR'] = '/tmp/wandb_cache'\n",
    "os.environ['JAX_LOG_COMPILATION'] = '1'\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from math import floor\n",
    "from typing import Any, Dict, Tuple, List, Callable\n",
    "import pickle\n",
    "from flax import serialization\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "import hydra\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from hydra.core.config_store import ConfigStore\n",
    "from qdax.core.map_elites import MAPElites\n",
    "from qdax.types import RNGKey, Genotype\n",
    "from qdax.utils.sampling import sampling \n",
    "from qdax.core.containers.mapelites_repertoire import compute_cvt_centroids, MapElitesRepertoire\n",
    "from qdax.core.neuroevolution.networks.networks import MLPMCPG, MLPPPO\n",
    "from qdax.core.emitters.me_mcpg_emitter import MEMCPGConfig, MEMCPGEmitter\n",
    "from qdax.core.emitters.ppo_me_emitter import PPOMEConfig, PPOMEmitter\n",
    "#from qdax.core.emitters.rein_emitter_advanced import REINaiveConfig, REINaiveEmitter\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition, QDMCTransition, PPOTransition\n",
    "from qdax.environments import behavior_descriptor_extractor\n",
    "from qdax.tasks.brax_envs import reset_based_scoring_function_brax_envs as scoring_function\n",
    "from utils import Config, get_env\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "import wandb\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "from qdax.utils.plotting import plot_map_elites_results, plot_2d_map_elites_repertoire\n",
    "import matplotlib.pyplot as plt\n",
    "from set_up_brax import get_reward_offset_brax\n",
    "from qdax import environments_v1, environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import jax.numpy as jnp  # Assuming you are using jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_name):\n",
    "    if env_name == \"hopper_uni\":\n",
    "        episode_length = 1000\n",
    "        \n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "    elif env_name == \"halfcheetah_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "        \n",
    "    elif env_name == \"walker2d_uni\":\n",
    "        episode_length = 1024\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\t\n",
    "    elif env_name == \"ant_uni\":\n",
    "        episode_length = 1024\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=True)\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, exclude_current_positions_from_observation=True)\t\n",
    "    '''\n",
    "    elif env_name == \"ant_omni\":\n",
    "        episode_length = 250\n",
    "        max_bd = 30.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=False)\t\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "        max_bd = 1.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length)\t\n",
    "    else:\n",
    "        ValueError(f\"Environment {env_name} not supported.\")\n",
    "    '''\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration from this experiment script\n",
    "    \"\"\"\n",
    "    # Env config\n",
    "    #alg_name: str\n",
    "    seed: int\n",
    "    env_name: str\n",
    "    episode_length: int\n",
    "    policy_hidden_layer_sizes: Tuple[int, ...]   \n",
    "    # ME config\n",
    "    num_evaluations: int\n",
    "    num_iterations: int\n",
    "    no_agents: int\n",
    "    num_samples: int\n",
    "    fixed_init_state: bool\n",
    "    discard_dead: bool\n",
    "    # Emitter config\n",
    "    iso_sigma: float\n",
    "    line_sigma: float\n",
    "    #crossover_percentage: float\n",
    "    # Grid config \n",
    "    grid_shape: Tuple[int, ...]\n",
    "    num_init_cvt_samples: int\n",
    "    num_centroids: int\n",
    "    # Log config\n",
    "    log_period: int\n",
    "    store_repertoire: bool\n",
    "    store_repertoire_log_period: int\n",
    "    \n",
    "    # REINFORCE Parameters\n",
    "    proportion_mutation_ga : float\n",
    "    buffer_sample_batch_size : int\n",
    "    buffer_add_batch_size: int\n",
    "    adam_optimizer: bool\n",
    "    learning_rate: float\n",
    "    discount_rate: float\n",
    "    #buffer_size: int\n",
    "    clip_param: float\n",
    "    no_epochs: int\n",
    "    \n",
    "    no_neurons: int\n",
    "    activation: str\n",
    "    vf_coef: float\n",
    "    num_minibatches: int\n",
    "    max_grad_norm: float\n",
    "    lecun: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import replace\n",
    "\n",
    "@dataclass\n",
    "class ObsNormalizer:\n",
    "    size: int\n",
    "    mean: jnp.ndarray = None\n",
    "    var: jnp.ndarray = None\n",
    "    count: jnp.ndarray = 1e-4\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.mean is None:\n",
    "            self.mean = jnp.zeros(self.size)\n",
    "        if self.var is None:\n",
    "            self.var = jnp.ones(self.size)\n",
    "            \n",
    "    def update(self, x):\n",
    "        # Flatten the first two dimensions (x, y) to treat as a single batch dimension\n",
    "        flat_x = x.reshape(-1, self.size)\n",
    "        batch_mean = jnp.mean(flat_x, axis=0)\n",
    "        batch_var = jnp.var(flat_x, axis=0)\n",
    "        batch_count = flat_x.shape[0]\n",
    "\n",
    "        new_mean, new_var, new_count = self._update_mean_var_count(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
    "        \n",
    "        return replace(self, mean=new_mean, var=new_var, count=new_count)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        # Normalize maintaining the original shape, using broadcasting\n",
    "        return (x - self.mean) / jnp.sqrt(self.var + 1e-8)\n",
    "\n",
    "    def _update_mean_var_count(self, mean, var, count, batch_mean, batch_var, batch_count):\n",
    "        delta = batch_mean - mean\n",
    "        tot_count = count + batch_count\n",
    "\n",
    "        new_mean = mean + delta * batch_count / tot_count\n",
    "        m_a = var * count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        return new_mean, new_var, new_count\n",
    "\n",
    "\n",
    "    def tree_flatten(self):\n",
    "        return ((self.mean, self.var, self.count), self.size)\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        size = aux_data\n",
    "        mean, var, count = children\n",
    "        return cls(size=size, mean=mean, var=var, count=count)\n",
    "\n",
    "# Register Normalizer as a pytree node with JAX\n",
    "jax.tree_util.register_pytree_node(\n",
    "    ObsNormalizer,\n",
    "    ObsNormalizer.tree_flatten,\n",
    "    ObsNormalizer.tree_unflatten\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RewardNormalizer:\n",
    "    size: int\n",
    "    mean: jnp.ndarray = 0.0\n",
    "    var: jnp.ndarray = 1.0\n",
    "    count: jnp.ndarray = 1e-4\n",
    "    return_val: jnp.ndarray = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.return_val is None:\n",
    "            self.return_val = jnp.zeros((self.size,))\n",
    "\n",
    "         \n",
    "    def update(self, reward, done, gamma=0.99):\n",
    "        \n",
    "        def _update_column_scan(carry, x):\n",
    "            mean, var, count, return_val = carry\n",
    "            (reward, done) = x\n",
    "            \n",
    "            #jax.debug.print(\"Reward shape: {}\", reward.shape)\n",
    "            \n",
    "            # Update the return value\n",
    "            new_return_val = reward + gamma * return_val * (1 - done)\n",
    "            \n",
    "            # Update the mean, var, and count\n",
    "            batch_mean = jnp.mean(new_return_val, axis=0)\n",
    "            batch_var = jnp.var(new_return_val, axis=0)\n",
    "            batch_count = new_return_val.shape[0]\n",
    "            \n",
    "            delta = batch_mean - mean\n",
    "            tot_count = count + batch_count\n",
    "            \n",
    "            new_mean = mean + delta * batch_count / tot_count\n",
    "            m_a = var * count\n",
    "            m_b = batch_var * batch_count\n",
    "            M2 = m_a + m_b + jnp.square(delta) * count * batch_count / tot_count\n",
    "            new_var = M2 / tot_count\n",
    "            new_count = tot_count\n",
    "            \n",
    "            normalized_reward = reward / jnp.sqrt(new_var + 1e-8)\n",
    "            \n",
    "            return (new_mean, new_var, new_count, new_return_val), normalized_reward\n",
    "        \n",
    "        (new_mean, new_var, new_count, _), normalized_rewards = jax.lax.scan(\n",
    "            _update_column_scan,\n",
    "            (self.mean, self.var, self.count, self.return_val),\n",
    "            (reward.T, done.T),\n",
    "        )\n",
    "\n",
    "        \n",
    "        return replace(self, mean=new_mean, var=new_var, count=new_count), normalized_rewards.T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def tree_flatten(self):\n",
    "        return ((self.mean, self.var, self.count, self.return_val), self.size)\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        size = aux_data\n",
    "        mean, var, count, return_val = children\n",
    "        return cls(size=size, mean=mean, var=var, count=count, return_val=return_val)\n",
    "    \n",
    "        \n",
    "jax.tree_util.register_pytree_node(\n",
    "    RewardNormalizer,\n",
    "    RewardNormalizer.tree_flatten,\n",
    "    RewardNormalizer.tree_unflatten\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in policy_network:  11213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2380899/2021952627.py:210: UserWarning: This type of repertoire does not store the extra scores computed by the scoring function\n",
      "  repertoire = MapElitesRepertoire.init(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean fitness: 73.96699523925781\n",
      "mean fitness: 79.9056625366211\n",
      "mean fitness: 169.95632934570312\n",
      "mean fitness: 145.59222412109375\n",
      "mean fitness: 179.7495574951172\n",
      "mean fitness: 152.00436401367188\n",
      "mean fitness: 163.63609313964844\n",
      "mean fitness: 335.499267578125\n",
      "mean fitness: 316.33038330078125\n",
      "mean fitness: 290.455810546875\n",
      "mean fitness: 329.5206604003906\n",
      "mean fitness: 144.17889404296875\n",
      "mean fitness: 114.39218139648438\n",
      "mean fitness: 136.88348388671875\n",
      "mean fitness: 419.93743896484375\n",
      "mean fitness: 488.0634460449219\n",
      "mean fitness: 140.0913543701172\n",
      "mean fitness: 408.1572265625\n",
      "mean fitness: 204.183837890625\n",
      "mean fitness: 393.0503845214844\n",
      "mean fitness: 132.5863494873047\n",
      "mean fitness: 412.3414306640625\n",
      "mean fitness: 92.5159912109375\n",
      "mean fitness: 122.61912536621094\n",
      "mean fitness: 158.54681396484375\n",
      "mean fitness: 126.49113464355469\n",
      "mean fitness: 154.40621948242188\n",
      "mean fitness: 151.46058654785156\n",
      "mean fitness: 181.7580108642578\n",
      "mean fitness: 118.58670043945312\n",
      "mean fitness: 109.83159637451172\n",
      "mean fitness: 116.41444396972656\n",
      "mean fitness: 105.5396728515625\n",
      "mean fitness: 98.32617950439453\n",
      "mean fitness: 97.2394790649414\n",
      "mean fitness: 88.31182098388672\n",
      "mean fitness: 94.98230743408203\n",
      "mean fitness: 95.84747314453125\n",
      "mean fitness: 99.11651611328125\n",
      "mean fitness: 103.64491271972656\n",
      "mean fitness: 118.51243591308594\n",
      "mean fitness: 127.20714569091797\n",
      "mean fitness: 132.14320373535156\n",
      "mean fitness: 280.17578125\n",
      "mean fitness: 463.36614990234375\n",
      "mean fitness: 125.08088684082031\n",
      "mean fitness: 147.2739715576172\n",
      "mean fitness: 418.3797302246094\n",
      "mean fitness: 142.1780548095703\n",
      "mean fitness: 339.3208312988281\n",
      "mean fitness: 462.7637939453125\n",
      "mean fitness: 330.506591796875\n",
      "mean fitness: 278.762939453125\n",
      "mean fitness: 271.6478271484375\n",
      "mean fitness: 264.7679748535156\n",
      "mean fitness: 261.65069580078125\n",
      "mean fitness: 262.8507080078125\n",
      "mean fitness: 263.18084716796875\n",
      "mean fitness: 262.23974609375\n",
      "mean fitness: 261.522216796875\n",
      "mean fitness: 262.3426208496094\n",
      "mean fitness: 260.5389709472656\n",
      "mean fitness: 260.33087158203125\n",
      "mean fitness: 262.4666748046875\n",
      "mean fitness: 258.2049865722656\n",
      "mean fitness: 256.7840576171875\n",
      "mean fitness: 255.96961975097656\n",
      "mean fitness: 256.25738525390625\n",
      "mean fitness: 255.2977752685547\n",
      "mean fitness: 255.82791137695312\n",
      "mean fitness: 256.297607421875\n",
      "mean fitness: 254.3882598876953\n",
      "mean fitness: 257.0760192871094\n",
      "mean fitness: 255.4137420654297\n",
      "mean fitness: 255.2469482421875\n",
      "mean fitness: 256.59552001953125\n",
      "mean fitness: 255.84527587890625\n",
      "mean fitness: 257.8051452636719\n",
      "mean fitness: 258.6781311035156\n",
      "mean fitness: 256.4573974609375\n",
      "mean fitness: 259.93780517578125\n",
      "mean fitness: 260.08599853515625\n",
      "mean fitness: 263.00274658203125\n",
      "mean fitness: 264.7102966308594\n",
      "mean fitness: 266.480224609375\n",
      "mean fitness: 265.72430419921875\n",
      "mean fitness: 269.332763671875\n",
      "mean fitness: 267.8853454589844\n",
      "mean fitness: 268.4993591308594\n",
      "mean fitness: 266.4336853027344\n",
      "mean fitness: 269.7923583984375\n",
      "mean fitness: 267.9550476074219\n",
      "mean fitness: 269.2934265136719\n",
      "mean fitness: 266.5204772949219\n",
      "mean fitness: 270.44744873046875\n",
      "mean fitness: 268.853515625\n",
      "mean fitness: 269.3870849609375\n",
      "mean fitness: 268.41973876953125\n",
      "mean fitness: 268.1668701171875\n",
      "mean fitness: 267.7646484375\n",
      "mean fitness: 269.6741027832031\n",
      "mean fitness: 271.6736145019531\n",
      "mean fitness: 270.3864440917969\n",
      "mean fitness: 269.3608703613281\n",
      "mean fitness: 271.09735107421875\n",
      "mean fitness: 270.65557861328125\n",
      "mean fitness: 269.6900634765625\n",
      "mean fitness: 271.92401123046875\n",
      "mean fitness: 268.9966125488281\n",
      "mean fitness: 269.5343322753906\n",
      "mean fitness: 269.13507080078125\n",
      "mean fitness: 271.7299499511719\n",
      "mean fitness: 271.6911315917969\n",
      "mean fitness: 270.87335205078125\n",
      "mean fitness: 270.19927978515625\n",
      "mean fitness: 270.3817138671875\n",
      "mean fitness: 268.57537841796875\n",
      "mean fitness: 270.15557861328125\n",
      "mean fitness: 272.1668701171875\n",
      "mean fitness: 272.9952392578125\n",
      "mean fitness: 276.7310791015625\n",
      "mean fitness: 277.76873779296875\n",
      "mean fitness: 296.70947265625\n",
      "mean fitness: 297.9256896972656\n",
      "mean fitness: 308.8446044921875\n",
      "mean fitness: 316.40032958984375\n",
      "mean fitness: 322.43206787109375\n",
      "mean fitness: 314.9735107421875\n",
      "mean fitness: 375.5793151855469\n",
      "mean fitness: 314.96966552734375\n",
      "mean fitness: 306.9591979980469\n",
      "mean fitness: 285.1477966308594\n",
      "mean fitness: 276.0477600097656\n",
      "mean fitness: 273.1531677246094\n",
      "mean fitness: 272.31683349609375\n",
      "mean fitness: 276.2274169921875\n",
      "mean fitness: 275.8031921386719\n",
      "mean fitness: 275.7232360839844\n",
      "mean fitness: 305.43035888671875\n",
      "mean fitness: 275.1795959472656\n",
      "mean fitness: 269.02313232421875\n",
      "mean fitness: 266.498291015625\n",
      "mean fitness: 258.52264404296875\n",
      "mean fitness: 278.51641845703125\n",
      "mean fitness: 335.1910400390625\n",
      "mean fitness: 315.5245361328125\n",
      "mean fitness: 340.642333984375\n",
      "mean fitness: 360.7269592285156\n",
      "mean fitness: 132.85047912597656\n",
      "mean fitness: 132.1436309814453\n",
      "mean fitness: 252.02804565429688\n",
      "mean fitness: 203.23797607421875\n",
      "mean fitness: 125.82171630859375\n",
      "mean fitness: 266.291259765625\n",
      "mean fitness: 139.25\n",
      "mean fitness: 412.15777587890625\n",
      "mean fitness: 385.81195068359375\n",
      "mean fitness: 423.64263916015625\n",
      "mean fitness: 127.66207885742188\n",
      "mean fitness: 140.8152313232422\n",
      "mean fitness: 127.33103942871094\n",
      "mean fitness: 345.32501220703125\n",
      "mean fitness: 294.3847961425781\n",
      "mean fitness: 278.31842041015625\n",
      "mean fitness: 258.42254638671875\n",
      "mean fitness: 276.36639404296875\n",
      "mean fitness: 280.9627990722656\n",
      "mean fitness: 284.7388610839844\n",
      "mean fitness: 342.6697692871094\n",
      "mean fitness: 308.6973571777344\n",
      "mean fitness: 350.5684814453125\n",
      "mean fitness: 330.2978515625\n",
      "mean fitness: 344.50164794921875\n",
      "mean fitness: 349.6999816894531\n",
      "mean fitness: 331.44482421875\n",
      "mean fitness: 384.6004333496094\n",
      "mean fitness: 157.03953552246094\n",
      "mean fitness: 232.9003448486328\n",
      "mean fitness: 331.1802978515625\n",
      "mean fitness: 360.640625\n",
      "mean fitness: 340.9559020996094\n",
      "mean fitness: 300.1246032714844\n",
      "mean fitness: 351.3470458984375\n",
      "mean fitness: 303.16180419921875\n",
      "mean fitness: 333.3221435546875\n",
      "mean fitness: 425.73583984375\n",
      "mean fitness: 146.72076416015625\n",
      "mean fitness: 109.83087158203125\n",
      "mean fitness: 92.46825408935547\n",
      "mean fitness: 83.14102935791016\n",
      "mean fitness: 77.83287048339844\n",
      "mean fitness: 77.51952362060547\n",
      "mean fitness: 82.29043579101562\n",
      "mean fitness: 75.89962005615234\n",
      "mean fitness: 79.45780181884766\n",
      "mean fitness: 80.4761962890625\n",
      "mean fitness: 78.88595581054688\n",
      "mean fitness: 79.34060668945312\n",
      "mean fitness: 79.66201782226562\n",
      "mean fitness: 80.58918762207031\n",
      "mean fitness: 80.77317810058594\n",
      "mean fitness: 73.6451187133789\n",
      "mean fitness: 81.2914047241211\n",
      "mean fitness: 80.36845397949219\n",
      "mean fitness: 93.9232177734375\n",
      "mean fitness: 86.56695556640625\n",
      "mean fitness: 97.89900207519531\n",
      "mean fitness: 125.6485366821289\n",
      "mean fitness: 194.7068634033203\n",
      "mean fitness: 181.91598510742188\n",
      "mean fitness: 190.90423583984375\n",
      "mean fitness: 125.34152221679688\n",
      "mean fitness: 123.26477813720703\n",
      "mean fitness: 126.60607147216797\n",
      "mean fitness: 191.59310913085938\n",
      "mean fitness: 491.14984130859375\n",
      "mean fitness: 163.72572326660156\n",
      "mean fitness: 124.78172302246094\n",
      "mean fitness: 167.97634887695312\n",
      "mean fitness: 118.41160583496094\n",
      "mean fitness: 135.35646057128906\n",
      "mean fitness: 245.96282958984375\n",
      "mean fitness: 242.88111877441406\n",
      "mean fitness: 310.8815002441406\n",
      "mean fitness: 280.810791015625\n",
      "mean fitness: 274.18695068359375\n",
      "mean fitness: 290.840576171875\n",
      "mean fitness: 276.207763671875\n",
      "mean fitness: 283.982666015625\n",
      "mean fitness: 269.1844482421875\n",
      "mean fitness: 266.3829345703125\n",
      "mean fitness: 262.1993408203125\n",
      "mean fitness: 250.59661865234375\n",
      "mean fitness: 249.79168701171875\n",
      "mean fitness: 246.42379760742188\n",
      "mean fitness: 253.5825958251953\n",
      "mean fitness: 255.81198120117188\n",
      "mean fitness: 248.68663024902344\n",
      "mean fitness: 245.66162109375\n",
      "mean fitness: 245.5160369873047\n",
      "mean fitness: 250.39486694335938\n",
      "mean fitness: 245.82455444335938\n",
      "mean fitness: 240.38209533691406\n",
      "mean fitness: 240.77210998535156\n",
      "mean fitness: 242.19796752929688\n",
      "mean fitness: 243.08351135253906\n",
      "mean fitness: 241.30020141601562\n",
      "mean fitness: 244.16539001464844\n",
      "mean fitness: 244.05929565429688\n",
      "mean fitness: 246.4604034423828\n",
      "Number of parameters in policy_network:  12625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2380899/2021952627.py:210: UserWarning: This type of repertoire does not store the extra scores computed by the scoring function\n",
      "  repertoire = MapElitesRepertoire.init(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean fitness: 900.4405517578125\n",
      "mean fitness: 270.5308837890625\n",
      "mean fitness: 229.413330078125\n",
      "mean fitness: 32.42040252685547\n",
      "mean fitness: 27.690799713134766\n",
      "mean fitness: 87.607666015625\n",
      "mean fitness: 24.929851531982422\n",
      "mean fitness: 0.6529399156570435\n",
      "mean fitness: 42.85212707519531\n",
      "mean fitness: 18.09977149963379\n",
      "mean fitness: 109.50254821777344\n",
      "mean fitness: 12.309200286865234\n",
      "mean fitness: 24.262470245361328\n",
      "mean fitness: 15.440832138061523\n",
      "mean fitness: 24.750446319580078\n",
      "mean fitness: 21.30231285095215\n",
      "mean fitness: 16.904834747314453\n",
      "mean fitness: 25.967872619628906\n",
      "mean fitness: 6.589447498321533\n",
      "mean fitness: 8.174848556518555\n",
      "mean fitness: 6.11345100402832\n",
      "mean fitness: 4.38013219833374\n",
      "mean fitness: 4.478180885314941\n",
      "mean fitness: 1.7931606769561768\n",
      "mean fitness: 3.423398494720459\n",
      "mean fitness: 0.5623123645782471\n",
      "mean fitness: 2.0097997188568115\n",
      "mean fitness: 0.6213541626930237\n",
      "mean fitness: 0.0\n",
      "mean fitness: 2.120435953140259\n",
      "mean fitness: 0.16271032392978668\n",
      "mean fitness: 1.3646001815795898\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 1.416030764579773\n",
      "mean fitness: 1.3701850175857544\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 1.5539031028747559\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.49346446990966797\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.0\n",
      "mean fitness: 0.6865127682685852\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 248\u001b[0m\n\u001b[1;32m    246\u001b[0m random_keys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(random_key, config\u001b[38;5;241m.\u001b[39mno_agents)\n\u001b[1;32m    247\u001b[0m new_params \u001b[38;5;241m=\u001b[39m ppo_me_emitter\u001b[38;5;241m.\u001b[39memitters[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39memit_mcpg(emitter_state, old_params, random_keys)\n\u001b[0;32m--> 248\u001b[0m fitnesses, descriptors, extra_scores, random_key, obs_normalizer, reward_normalizer \u001b[38;5;241m=\u001b[39m \u001b[43mscoring_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_normalizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_normalizer\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m emitter_state \u001b[38;5;241m=\u001b[39m ppo_me_emitter\u001b[38;5;241m.\u001b[39memitters[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstate_update(\n\u001b[1;32m    252\u001b[0m     emitter_state\u001b[38;5;241m=\u001b[39memitter_state,\n\u001b[1;32m    253\u001b[0m     repertoire\u001b[38;5;241m=\u001b[39mrepertoire,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     extra_scores\u001b[38;5;241m=\u001b[39mextra_scores,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m old_params \u001b[38;5;241m=\u001b[39m new_params\n",
      "File \u001b[0;32m/vol/bitbucket/km2120/me-with-sample-based-drl/venv/lib/python3.10/site-packages/flax/struct.py:149\u001b[0m, in \u001b[0;36mdataclass.<locals>.clz_from_iterable\u001b[0;34m(meta, data)\u001b[0m\n\u001b[1;32m    143\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    144\u001b[0m       (jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mGetAttrKey(name), \u001b[38;5;28mgetattr\u001b[39m(x, name))\n\u001b[1;32m    145\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m data_fields\n\u001b[1;32m    146\u001b[0m   )\n\u001b[1;32m    147\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m data, meta\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclz_from_iterable\u001b[39m(meta, data):\n\u001b[1;32m    150\u001b[0m   meta_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(meta_fields, meta))\n\u001b[1;32m    151\u001b[0m   data_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(data_fields, data))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "no_epochs = [4]\n",
    "\n",
    "envs = [\"walker2d_uni\", \"ant_uni\"]\n",
    "\n",
    "#os.makedirs(\"grad_steps_experiments/reps=16_new/\", exist_ok=True)\n",
    "\n",
    "\n",
    "for env_ in envs:\n",
    "    \n",
    "    env_dir = f\"grad_steps_experiments/value/no_agents=1/epochs=4/{env_}\"\n",
    "    os.makedirs(env_dir, exist_ok=True)\n",
    "    \n",
    "    for no_epoch in no_epochs:\n",
    "        config = Config(\n",
    "            seed=10,\n",
    "            env_name=env_,\n",
    "            episode_length=1024,\n",
    "            policy_hidden_layer_sizes=[128, 128],\n",
    "            num_evaluations=0,\n",
    "            num_iterations=4000,\n",
    "            num_samples=8,\n",
    "            no_agents=1,\n",
    "            fixed_init_state=False,\n",
    "            discard_dead=False,\n",
    "            grid_shape=[50, 50],\n",
    "            num_init_cvt_samples=50000,\n",
    "            num_centroids=1024,\n",
    "            log_period=400,\n",
    "            store_repertoire=True,\n",
    "            store_repertoire_log_period=800,\n",
    "            iso_sigma=0.005,\n",
    "            line_sigma=0.05,\n",
    "            proportion_mutation_ga=0.5,\n",
    "            buffer_sample_batch_size=1,\n",
    "            buffer_add_batch_size=1,\n",
    "            no_epochs=no_epoch,\n",
    "            #buffer_size=64000,\n",
    "            adam_optimizer=True,\n",
    "            learning_rate=3e-4,\n",
    "            discount_rate=0.99,\n",
    "            clip_param=0.2,\n",
    "            no_neurons=64,\n",
    "            vf_coef=0.5,\n",
    "            activation=\"tanh\",\n",
    "            num_minibatches=32,\n",
    "            max_grad_norm=0.5,\n",
    "            lecun=False,\n",
    "        )\n",
    "        random_key = jax.random.PRNGKey(config.seed)\n",
    "\n",
    "        # Init environment\n",
    "        env = get_env(env_)\n",
    "        reset_fn = jax.jit(env.reset)\n",
    "\n",
    "        # Compute the centroids\n",
    "        centroids, random_key = compute_cvt_centroids(\n",
    "            num_descriptors=env.behavior_descriptor_length,\n",
    "            num_init_cvt_samples=config.num_init_cvt_samples,\n",
    "            num_centroids=config.num_centroids,\n",
    "            minval=0,\n",
    "            maxval=1,\n",
    "            random_key=random_key,\n",
    "        )\n",
    "        # Init policy network\n",
    "        \n",
    "        \n",
    "        obs_normalizer = ObsNormalizer(env.observation_size)\n",
    "        reward_normalizer = RewardNormalizer(config.no_agents)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        policy_network = MLPPPO(\n",
    "            action_dim=env.action_size,\n",
    "            activation=config.activation,\n",
    "            no_neurons=config.no_neurons,\n",
    "        )\n",
    "\n",
    "        # Init population of controllers\n",
    "        \n",
    "        # maybe consider adding two random keys for each policy\n",
    "        random_key, subkey = jax.random.split(random_key)\n",
    "        keys = jax.random.split(subkey, num=config.no_agents)\n",
    "        #split_keys = jax.vmap(lambda k: jax.random.split(k, 2))(keys)\n",
    "        #keys1, keys2 = split_keys[:, 0], split_keys[:, 1]\n",
    "        fake_batch_obs = jnp.zeros(shape=(config.no_agents, env.observation_size))\n",
    "        init_params = jax.vmap(policy_network.init)(keys, fake_batch_obs)\n",
    "\n",
    "        param_count = sum(x[0].size for x in jax.tree_util.tree_leaves(init_params))\n",
    "        print(\"Number of parameters in policy_network: \", param_count)\n",
    "\n",
    "        # Define the fonction to play a step with the policy in the environment\n",
    "        @jax.jit\n",
    "        def play_step_fn(env_state, policy_params, random_key):\n",
    "            random_key, subkey = jax.random.split(random_key)\n",
    "            #pi, val = policy_network.apply(policy_params, env_state.obs)\n",
    "            #action = pi.sample(seed=subkey)\n",
    "            pi, action, val = policy_network.apply(policy_params, env_state.obs)\n",
    "            \n",
    "            logp = pi.log_prob(action)\n",
    "            state_desc = env_state.info[\"state_descriptor\"]\n",
    "            next_state = env.step(env_state, action)\n",
    "            _, _, next_val = policy_network.apply(policy_params, next_state.obs)\n",
    "\n",
    "            transition = PPOTransition(\n",
    "                obs=env_state.obs,\n",
    "                next_obs=next_state.obs,\n",
    "                rewards=next_state.reward,\n",
    "                dones=next_state.done,\n",
    "                truncations=next_state.info[\"truncation\"],\n",
    "                actions=action,\n",
    "                state_desc=state_desc,\n",
    "                next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "                val_adv=val,\n",
    "                target=next_val,\n",
    "                logp=logp\n",
    "                #desc=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "                #desc_prime=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "            )\n",
    "\n",
    "            return (next_state, policy_params, random_key), transition\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Prepare the scoring function\n",
    "        bd_extraction_fn = behavior_descriptor_extractor[config.env_name]\n",
    "        scoring_fn = partial(\n",
    "            scoring_function,\n",
    "            episode_length=config.episode_length,\n",
    "            play_reset_fn=reset_fn,\n",
    "            play_step_fn=play_step_fn,  \n",
    "            behavior_descriptor_extractor=bd_extraction_fn,\n",
    "        )\n",
    "        #reward_offset = get_reward_offset_brax(env, config.env_name)\n",
    "        #print(f\"Reward offset: {reward_offset}\")\n",
    "        \n",
    "        me_scoring_fn = partial(\n",
    "        sampling,\n",
    "        scoring_fn=scoring_fn,\n",
    "        num_samples=config.num_samples,\n",
    "    )\n",
    "\n",
    "        reward_offset = 0\n",
    "    \n",
    "                    \n",
    "        \n",
    "            \n",
    "\n",
    "    \n",
    "        '''\n",
    "        def get_n_offspring_added(metrics):\n",
    "            split = jnp.cumsum(jnp.array([emitter.batch_size for emitter in map_elites._emitter.emitters]))\n",
    "            split = jnp.split(metrics[\"is_offspring_added\"], split, axis=-1)[:-1]\n",
    "            qpg_offspring_added, ai_offspring_added = jnp.split(split[0], (split[0].shape[1]-1,), axis=-1)\n",
    "            return (jnp.sum(split[1], axis=-1), jnp.sum(qpg_offspring_added, axis=-1), jnp.sum(ai_offspring_added, axis=-1))\n",
    "        '''\n",
    "        # Get minimum reward value to make sure qd_score are positive\n",
    "        \n",
    "\n",
    "        # Define a metrics function\n",
    "        metrics_function = partial(\n",
    "            default_qd_metrics,\n",
    "            qd_offset=reward_offset * config.episode_length,\n",
    "        )\n",
    "\n",
    "        # Define the PG-emitter config\n",
    "        \n",
    "        ppo_me_config = PPOMEConfig(\n",
    "            proportion_mutation_ga=config.proportion_mutation_ga,\n",
    "            no_agents=config.no_agents,\n",
    "            buffer_sample_batch_size=config.buffer_sample_batch_size,\n",
    "            buffer_add_batch_size=config.buffer_add_batch_size,\n",
    "            #batch_size=config.batch_size,\n",
    "            #mini_batch_size=config.mini_batch_size,\n",
    "            no_epochs=config.no_epochs,\n",
    "            #buffer_size=config.buffer_size,\n",
    "            learning_rate=config.learning_rate,\n",
    "            adam_optimizer=config.adam_optimizer,\n",
    "            clip_param=config.clip_param,\n",
    "            num_minibatches=config.num_minibatches,\n",
    "            vf_coef=config.vf_coef,\n",
    "            max_grad_norm=config.max_grad_norm,\n",
    "        )\n",
    "        \n",
    "        variation_fn = partial(\n",
    "            isoline_variation, iso_sigma=config.iso_sigma, line_sigma=config.line_sigma\n",
    "        )\n",
    "        \n",
    "        ppo_me_emitter = PPOMEmitter(\n",
    "            config=ppo_me_config,\n",
    "            policy_network=policy_network,\n",
    "            env=env,\n",
    "            variation_fn=variation_fn,\n",
    "            )\n",
    "\n",
    "\n",
    "        # Instantiate MAP Elites\n",
    "        map_elites = MAPElites(\n",
    "            scoring_function=scoring_fn,\n",
    "            emitter=ppo_me_emitter,\n",
    "            metrics_function=metrics_function,\n",
    "        )\n",
    "\n",
    "        fitnesses, descriptors, extra_scores, random_key, obs_normalizer, reward_normalizer = scoring_fn(\n",
    "            init_params, random_key, obs_normalizer, reward_normalizer\n",
    "        )\n",
    "        \n",
    "        repertoire = MapElitesRepertoire.init(\n",
    "            genotypes=init_params,\n",
    "            fitnesses=fitnesses,\n",
    "            descriptors=descriptors,\n",
    "            centroids=centroids,\n",
    "            extra_scores=extra_scores,\n",
    "        )\n",
    "        \n",
    "\n",
    "        emitter_state, random_key = ppo_me_emitter.init(\n",
    "            random_key=random_key,\n",
    "            repertoire=repertoire,\n",
    "            genotypes=init_params,\n",
    "            fitnesses=fitnesses,\n",
    "            descriptors=descriptors,\n",
    "            extra_scores=extra_scores,\n",
    "        )\n",
    "        \n",
    "        emitter_state = ppo_me_emitter.state_update(\n",
    "            emitter_state=emitter_state,\n",
    "            repertoire=repertoire,\n",
    "            genotypes=init_params,\n",
    "            fitnesses=fitnesses,\n",
    "            descriptors=descriptors,\n",
    "            extra_scores={**extra_scores}#, **extra_info},\n",
    "        )\n",
    "        \n",
    "        emitter_state = emitter_state.emitter_states[0]\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        returns = []\n",
    "        old_params = init_params\n",
    "        random_key = jax.random.PRNGKey(0)\n",
    "        for _ in range(250):\n",
    "            random_keys = jax.random.split(random_key, config.no_agents)\n",
    "            new_params = ppo_me_emitter.emitters[0].emit_mcpg(emitter_state, old_params, random_keys)\n",
    "            fitnesses, descriptors, extra_scores, random_key, obs_normalizer, reward_normalizer = scoring_fn(\n",
    "                new_params, random_key, obs_normalizer, reward_normalizer\n",
    "            )\n",
    "            emitter_state = ppo_me_emitter.emitters[0].state_update(\n",
    "                emitter_state=emitter_state,\n",
    "                repertoire=repertoire,\n",
    "                genotypes=new_params,\n",
    "                fitnesses=fitnesses,\n",
    "                descriptors=descriptors,\n",
    "                extra_scores=extra_scores,\n",
    "            )\n",
    "            old_params = new_params\n",
    "            print(f\"mean fitness: {fitnesses.mean()}\")\n",
    "            returns.append(fitnesses)\n",
    "            \n",
    "        returns = jnp.array(returns)  # Assuming 'returns' is already defined as a 2D array\n",
    "\n",
    "        # Determine the overall min and max fitness values for setting y-axis limits\n",
    "        ymin = returns.min()\n",
    "        ymax = returns.max()\n",
    "\n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(16, 16, figsize=(32, 64))  # Adjust the subplot grid to 16x16\n",
    "        for i in range(256):  # Loop through 256 plots\n",
    "            ax = axs[i // 16, i % 16]  # This assumes a 16x16 grid of subplots\n",
    "            ax.plot(returns[:, i])\n",
    "            ax.set_title(f\"Policy {i+1}\", fontsize=8)\n",
    "            ax.set_xlabel('Steps', fontsize=6)\n",
    "            ax.set_ylabel('Returns', fontsize=6)\n",
    "            ax.set_ylim([ymin, ymax])  # Set the same y-axis limits for all subplots\n",
    "            ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # Save each plot in the specified directory with the correct filename\n",
    "        plot_filename = f\"{env_dir}/no_GA.png\"\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close(fig)  # Close the plot to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in policy_network:  11213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375020/4285804299.py:210: UserWarning: This type of repertoire does not store the extra scores computed by the scoring function\n",
      "  repertoire = MapElitesRepertoire.init(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean fitness: 175.44656372070312\n",
      "mean fitness: 256.46307373046875\n",
      "mean fitness: 264.683349609375\n",
      "mean fitness: 260.86041259765625\n",
      "mean fitness: 258.6240539550781\n",
      "mean fitness: 255.4115753173828\n",
      "mean fitness: 253.8948974609375\n",
      "mean fitness: 253.1417236328125\n",
      "mean fitness: 251.8593292236328\n",
      "mean fitness: 250.58399963378906\n",
      "mean fitness: 249.24386596679688\n",
      "mean fitness: 247.8236541748047\n",
      "mean fitness: 245.6031494140625\n",
      "mean fitness: 243.05429077148438\n",
      "mean fitness: 240.92770385742188\n",
      "mean fitness: 238.82809448242188\n",
      "mean fitness: 237.3988037109375\n",
      "mean fitness: 236.3238525390625\n",
      "mean fitness: 235.1883087158203\n",
      "mean fitness: 234.16075134277344\n",
      "mean fitness: 233.25027465820312\n",
      "mean fitness: 232.9304962158203\n",
      "mean fitness: 232.72140502929688\n",
      "mean fitness: 232.60238647460938\n",
      "mean fitness: 232.53341674804688\n",
      "mean fitness: 232.27178955078125\n",
      "mean fitness: 231.92001342773438\n",
      "mean fitness: 231.93496704101562\n",
      "mean fitness: 231.7150421142578\n",
      "mean fitness: 231.8817138671875\n",
      "mean fitness: 232.847900390625\n",
      "mean fitness: 233.56263732910156\n",
      "mean fitness: 234.4842529296875\n",
      "mean fitness: 235.49139404296875\n",
      "mean fitness: 236.34312438964844\n",
      "mean fitness: 237.42742919921875\n",
      "mean fitness: 237.49588012695312\n",
      "mean fitness: 237.5269775390625\n",
      "mean fitness: 238.02752685546875\n",
      "mean fitness: 238.8302001953125\n",
      "mean fitness: 239.25588989257812\n",
      "mean fitness: 238.9059600830078\n",
      "mean fitness: 239.46273803710938\n",
      "mean fitness: 239.67218017578125\n",
      "mean fitness: 238.0068359375\n",
      "mean fitness: 239.27549743652344\n",
      "mean fitness: 238.14859008789062\n",
      "mean fitness: 237.19094848632812\n",
      "mean fitness: 237.60302734375\n",
      "mean fitness: 236.8770751953125\n",
      "mean fitness: 235.93707275390625\n",
      "mean fitness: 233.42489624023438\n",
      "mean fitness: 233.96702575683594\n",
      "mean fitness: 233.02642822265625\n",
      "mean fitness: 233.14834594726562\n",
      "mean fitness: 232.53048706054688\n",
      "mean fitness: 232.0142822265625\n",
      "mean fitness: 232.0780029296875\n",
      "mean fitness: 230.87271118164062\n",
      "mean fitness: 229.53890991210938\n",
      "mean fitness: 230.828369140625\n",
      "mean fitness: 230.62738037109375\n",
      "mean fitness: 230.3463134765625\n",
      "mean fitness: 230.02984619140625\n",
      "mean fitness: 230.60743713378906\n",
      "mean fitness: 230.72421264648438\n",
      "mean fitness: 230.62393188476562\n",
      "mean fitness: 227.81283569335938\n",
      "mean fitness: 229.1226348876953\n",
      "mean fitness: 230.45887756347656\n",
      "mean fitness: 229.15113830566406\n",
      "mean fitness: 231.5550537109375\n",
      "mean fitness: 232.87786865234375\n",
      "mean fitness: 233.43310546875\n",
      "mean fitness: 234.63009643554688\n",
      "mean fitness: 229.665283203125\n",
      "mean fitness: 233.2115478515625\n",
      "mean fitness: 232.0850830078125\n",
      "mean fitness: 228.99273681640625\n",
      "mean fitness: 230.73500061035156\n",
      "mean fitness: 233.357421875\n",
      "mean fitness: 230.0449981689453\n",
      "mean fitness: 232.04818725585938\n",
      "mean fitness: 232.3060302734375\n",
      "mean fitness: 231.40570068359375\n",
      "mean fitness: 231.16024780273438\n",
      "mean fitness: 231.9551544189453\n",
      "mean fitness: 233.57843017578125\n",
      "mean fitness: 233.6118927001953\n",
      "mean fitness: 233.04568481445312\n",
      "mean fitness: 231.9834442138672\n",
      "mean fitness: 233.20318603515625\n",
      "mean fitness: 232.85028076171875\n",
      "mean fitness: 232.0266571044922\n",
      "mean fitness: 230.84762573242188\n",
      "mean fitness: 236.47003173828125\n",
      "mean fitness: 231.52227783203125\n",
      "mean fitness: 230.91592407226562\n",
      "mean fitness: 233.75645446777344\n",
      "mean fitness: 229.96786499023438\n",
      "mean fitness: 231.90020751953125\n",
      "mean fitness: 233.93748474121094\n",
      "mean fitness: 233.53399658203125\n",
      "mean fitness: 237.36163330078125\n",
      "mean fitness: 230.50473022460938\n",
      "mean fitness: 231.64273071289062\n",
      "mean fitness: 229.97787475585938\n",
      "mean fitness: 229.39044189453125\n",
      "mean fitness: 234.60870361328125\n",
      "mean fitness: 231.60899353027344\n",
      "mean fitness: 235.14959716796875\n",
      "mean fitness: 229.82833862304688\n",
      "mean fitness: 233.1024169921875\n",
      "mean fitness: 232.6158447265625\n",
      "mean fitness: 231.49661254882812\n",
      "mean fitness: 231.69383239746094\n",
      "mean fitness: 231.75502014160156\n",
      "mean fitness: 232.31521606445312\n",
      "mean fitness: 233.49473571777344\n",
      "mean fitness: 233.06231689453125\n",
      "mean fitness: 234.21600341796875\n",
      "mean fitness: 231.42430114746094\n",
      "mean fitness: 234.2583770751953\n",
      "mean fitness: 234.67681884765625\n",
      "mean fitness: 233.7166748046875\n",
      "mean fitness: 233.32334899902344\n",
      "mean fitness: 233.80006408691406\n",
      "mean fitness: 230.97286987304688\n",
      "mean fitness: 228.33749389648438\n",
      "mean fitness: 229.40237426757812\n",
      "mean fitness: 230.82540893554688\n",
      "mean fitness: 230.22125244140625\n",
      "mean fitness: 234.01211547851562\n",
      "mean fitness: 231.27899169921875\n",
      "mean fitness: 223.99169921875\n",
      "mean fitness: 232.3658447265625\n",
      "mean fitness: 226.21641540527344\n",
      "mean fitness: 225.80349731445312\n",
      "mean fitness: 227.81991577148438\n",
      "mean fitness: 227.32415771484375\n",
      "mean fitness: 228.64248657226562\n",
      "mean fitness: 228.12689208984375\n",
      "mean fitness: 229.4698028564453\n",
      "mean fitness: 228.5041961669922\n",
      "mean fitness: 225.0855712890625\n",
      "mean fitness: 229.3315887451172\n",
      "mean fitness: 226.21240234375\n",
      "mean fitness: 227.1067352294922\n",
      "mean fitness: 223.03170776367188\n",
      "mean fitness: 226.3411407470703\n",
      "mean fitness: 228.25811767578125\n",
      "mean fitness: 225.39254760742188\n",
      "mean fitness: 228.06597900390625\n",
      "mean fitness: 223.0496826171875\n",
      "mean fitness: 222.6706085205078\n",
      "mean fitness: 226.20962524414062\n",
      "mean fitness: 224.7165985107422\n",
      "mean fitness: 225.1230926513672\n",
      "mean fitness: 223.61538696289062\n",
      "mean fitness: 222.69956970214844\n",
      "mean fitness: 225.84796142578125\n",
      "mean fitness: 225.45703125\n",
      "mean fitness: 227.90589904785156\n",
      "mean fitness: 225.20925903320312\n",
      "mean fitness: 225.78614807128906\n",
      "mean fitness: 225.23866271972656\n",
      "mean fitness: 221.31451416015625\n",
      "mean fitness: 220.65289306640625\n",
      "mean fitness: 222.31707763671875\n",
      "mean fitness: 224.95681762695312\n",
      "mean fitness: 223.3465576171875\n",
      "mean fitness: 222.4957275390625\n",
      "mean fitness: 221.0025177001953\n",
      "mean fitness: 225.16436767578125\n",
      "mean fitness: 221.22430419921875\n",
      "mean fitness: 222.39524841308594\n",
      "mean fitness: 222.74847412109375\n",
      "mean fitness: 221.1611328125\n",
      "mean fitness: 223.4172821044922\n",
      "mean fitness: 221.30615234375\n",
      "mean fitness: 224.20065307617188\n",
      "mean fitness: 220.6632080078125\n",
      "mean fitness: 222.5113067626953\n",
      "mean fitness: 219.3446044921875\n",
      "mean fitness: 224.4642333984375\n",
      "mean fitness: 225.9150390625\n",
      "mean fitness: 224.0464630126953\n",
      "mean fitness: 222.95635986328125\n",
      "mean fitness: 221.64634704589844\n",
      "mean fitness: 222.78001403808594\n",
      "mean fitness: 222.6684112548828\n",
      "mean fitness: 220.3133544921875\n",
      "mean fitness: 220.39013671875\n",
      "mean fitness: 224.9805908203125\n",
      "mean fitness: 221.61001586914062\n",
      "mean fitness: 221.98342895507812\n",
      "mean fitness: 224.14923095703125\n",
      "mean fitness: 221.41607666015625\n",
      "mean fitness: 219.79391479492188\n",
      "mean fitness: 221.7779998779297\n",
      "mean fitness: 221.11529541015625\n",
      "mean fitness: 218.7510223388672\n",
      "mean fitness: 219.12063598632812\n",
      "mean fitness: 222.86087036132812\n",
      "mean fitness: 225.27828979492188\n",
      "mean fitness: 221.6953582763672\n",
      "mean fitness: 221.4185791015625\n",
      "mean fitness: 218.7797088623047\n",
      "mean fitness: 218.06201171875\n",
      "mean fitness: 219.46588134765625\n",
      "mean fitness: 221.8724365234375\n",
      "mean fitness: 223.66787719726562\n",
      "mean fitness: 222.031494140625\n",
      "mean fitness: 218.48526000976562\n",
      "mean fitness: 219.63427734375\n",
      "mean fitness: 221.9166259765625\n",
      "mean fitness: 219.37094116210938\n",
      "mean fitness: 220.214599609375\n",
      "mean fitness: 221.9847412109375\n",
      "mean fitness: 221.2083740234375\n",
      "mean fitness: 219.10964965820312\n",
      "mean fitness: 216.0284423828125\n",
      "mean fitness: 219.24752807617188\n",
      "mean fitness: 222.34823608398438\n",
      "mean fitness: 219.57432556152344\n",
      "mean fitness: 219.11953735351562\n",
      "mean fitness: 220.77825927734375\n",
      "mean fitness: 218.47772216796875\n",
      "mean fitness: 219.12603759765625\n",
      "mean fitness: 220.18310546875\n",
      "mean fitness: 218.24691772460938\n",
      "mean fitness: 218.4798583984375\n",
      "mean fitness: 220.0389404296875\n",
      "mean fitness: 217.39450073242188\n",
      "mean fitness: 220.76724243164062\n",
      "mean fitness: 221.61712646484375\n",
      "mean fitness: 220.05914306640625\n",
      "mean fitness: 218.15560913085938\n",
      "mean fitness: 219.6289825439453\n",
      "mean fitness: 217.7585906982422\n",
      "mean fitness: 220.4261474609375\n",
      "mean fitness: 220.02989196777344\n",
      "mean fitness: 221.27218627929688\n",
      "mean fitness: 221.5684051513672\n",
      "mean fitness: 223.685302734375\n",
      "mean fitness: 217.7791748046875\n",
      "mean fitness: 221.36669921875\n",
      "mean fitness: 221.93124389648438\n",
      "mean fitness: 222.7355194091797\n",
      "mean fitness: 223.99404907226562\n",
      "Number of parameters in policy_network:  12625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375020/4285804299.py:210: UserWarning: This type of repertoire does not store the extra scores computed by the scoring function\n",
      "  repertoire = MapElitesRepertoire.init(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean fitness: 985.8655395507812\n",
      "mean fitness: 949.7366943359375\n",
      "mean fitness: 948.462158203125\n",
      "mean fitness: 928.8270263671875\n",
      "mean fitness: 898.0606689453125\n",
      "mean fitness: 862.854248046875\n",
      "mean fitness: 839.592529296875\n",
      "mean fitness: 821.0694580078125\n",
      "mean fitness: 795.7606811523438\n",
      "mean fitness: 751.163330078125\n",
      "mean fitness: 695.0880126953125\n",
      "mean fitness: 618.5626220703125\n",
      "mean fitness: 550.2085571289062\n",
      "mean fitness: 503.65948486328125\n",
      "mean fitness: 462.3690185546875\n",
      "mean fitness: 434.7431945800781\n",
      "mean fitness: 401.716796875\n",
      "mean fitness: 373.7886962890625\n",
      "mean fitness: 345.31884765625\n",
      "mean fitness: 309.61181640625\n",
      "mean fitness: 265.2862854003906\n",
      "mean fitness: 225.62075805664062\n",
      "mean fitness: 197.2504425048828\n",
      "mean fitness: 163.7965545654297\n",
      "mean fitness: 154.7968292236328\n",
      "mean fitness: 127.79566192626953\n",
      "mean fitness: 104.03175354003906\n",
      "mean fitness: 89.88478088378906\n",
      "mean fitness: 79.24995422363281\n",
      "mean fitness: 61.73939514160156\n",
      "mean fitness: 53.6400260925293\n",
      "mean fitness: 42.874881744384766\n",
      "mean fitness: 41.63152313232422\n",
      "mean fitness: 33.292808532714844\n",
      "mean fitness: 27.939430236816406\n",
      "mean fitness: 19.900299072265625\n",
      "mean fitness: 17.311872482299805\n",
      "mean fitness: 17.99720001220703\n",
      "mean fitness: 16.049652099609375\n",
      "mean fitness: 15.509201049804688\n",
      "mean fitness: 12.604109764099121\n",
      "mean fitness: 14.249288558959961\n",
      "mean fitness: 13.07181167602539\n",
      "mean fitness: 12.471938133239746\n",
      "mean fitness: 12.270431518554688\n",
      "mean fitness: 11.328773498535156\n",
      "mean fitness: 11.775663375854492\n",
      "mean fitness: 11.132970809936523\n",
      "mean fitness: 12.75101089477539\n",
      "mean fitness: 11.218362808227539\n",
      "mean fitness: 11.373924255371094\n",
      "mean fitness: 11.663978576660156\n",
      "mean fitness: 9.77782917022705\n",
      "mean fitness: 9.679941177368164\n",
      "mean fitness: 9.763131141662598\n",
      "mean fitness: 8.56854248046875\n",
      "mean fitness: 11.001934051513672\n",
      "mean fitness: 10.493459701538086\n",
      "mean fitness: 9.871175765991211\n",
      "mean fitness: 11.48446273803711\n",
      "mean fitness: 11.783611297607422\n",
      "mean fitness: 12.0405912399292\n",
      "mean fitness: 11.899942398071289\n",
      "mean fitness: 12.214879035949707\n",
      "mean fitness: 12.885366439819336\n",
      "mean fitness: 14.305456161499023\n",
      "mean fitness: 12.88228988647461\n",
      "mean fitness: 14.447118759155273\n",
      "mean fitness: 16.131633758544922\n",
      "mean fitness: 17.108991622924805\n",
      "mean fitness: 15.153369903564453\n",
      "mean fitness: 14.878841400146484\n",
      "mean fitness: 20.361940383911133\n",
      "mean fitness: 18.234174728393555\n",
      "mean fitness: 17.565263748168945\n",
      "mean fitness: 17.353179931640625\n",
      "mean fitness: 16.220224380493164\n",
      "mean fitness: 18.498397827148438\n",
      "mean fitness: 18.899852752685547\n",
      "mean fitness: 20.704883575439453\n",
      "mean fitness: 16.29155731201172\n",
      "mean fitness: 17.032535552978516\n",
      "mean fitness: 16.257354736328125\n",
      "mean fitness: 18.781240463256836\n",
      "mean fitness: 16.838703155517578\n",
      "mean fitness: 17.808298110961914\n",
      "mean fitness: 18.061647415161133\n",
      "mean fitness: 15.910082817077637\n",
      "mean fitness: 16.236785888671875\n",
      "mean fitness: 18.22467041015625\n",
      "mean fitness: 17.872394561767578\n",
      "mean fitness: 18.15890884399414\n",
      "mean fitness: 19.29715347290039\n",
      "mean fitness: 23.133577346801758\n",
      "mean fitness: 18.764636993408203\n",
      "mean fitness: 19.226181030273438\n",
      "mean fitness: 18.878204345703125\n",
      "mean fitness: 20.683612823486328\n",
      "mean fitness: 19.516923904418945\n",
      "mean fitness: 17.24265480041504\n",
      "mean fitness: 18.158714294433594\n",
      "mean fitness: 19.04241180419922\n",
      "mean fitness: 19.903383255004883\n",
      "mean fitness: 20.884428024291992\n",
      "mean fitness: 22.82276153564453\n",
      "mean fitness: 22.824874877929688\n",
      "mean fitness: 20.201801300048828\n",
      "mean fitness: 22.652530670166016\n",
      "mean fitness: 23.169105529785156\n",
      "mean fitness: 18.092525482177734\n",
      "mean fitness: 21.13755989074707\n",
      "mean fitness: 18.10479164123535\n",
      "mean fitness: 19.413883209228516\n",
      "mean fitness: 17.643985748291016\n",
      "mean fitness: 23.461666107177734\n",
      "mean fitness: 18.94765853881836\n",
      "mean fitness: 20.523880004882812\n",
      "mean fitness: 19.906326293945312\n",
      "mean fitness: 19.943092346191406\n",
      "mean fitness: 24.066787719726562\n",
      "mean fitness: 21.89417266845703\n",
      "mean fitness: 18.25031280517578\n",
      "mean fitness: 27.138038635253906\n",
      "mean fitness: 19.724332809448242\n",
      "mean fitness: 20.248653411865234\n",
      "mean fitness: 19.67694091796875\n",
      "mean fitness: 19.630460739135742\n",
      "mean fitness: 19.969423294067383\n",
      "mean fitness: 18.910316467285156\n",
      "mean fitness: 19.26769256591797\n",
      "mean fitness: 19.389936447143555\n",
      "mean fitness: 19.14615821838379\n",
      "mean fitness: 22.209375381469727\n",
      "mean fitness: 23.917049407958984\n",
      "mean fitness: 20.99261474609375\n",
      "mean fitness: 20.11791229248047\n",
      "mean fitness: 21.962677001953125\n",
      "mean fitness: 23.874134063720703\n",
      "mean fitness: 18.29803466796875\n",
      "mean fitness: 19.049888610839844\n",
      "mean fitness: 20.251440048217773\n",
      "mean fitness: 19.019664764404297\n",
      "mean fitness: 23.490774154663086\n",
      "mean fitness: 19.687164306640625\n",
      "mean fitness: 19.821720123291016\n",
      "mean fitness: 19.645706176757812\n",
      "mean fitness: 22.836490631103516\n",
      "mean fitness: 19.941757202148438\n",
      "mean fitness: 18.417072296142578\n",
      "mean fitness: 24.31247329711914\n",
      "mean fitness: 21.71072769165039\n",
      "mean fitness: 19.88232421875\n",
      "mean fitness: 17.603885650634766\n",
      "mean fitness: 18.48971939086914\n",
      "mean fitness: 22.386978149414062\n",
      "mean fitness: 18.322378158569336\n",
      "mean fitness: 20.602230072021484\n",
      "mean fitness: 24.62198829650879\n",
      "mean fitness: 17.59019660949707\n",
      "mean fitness: 22.29570770263672\n",
      "mean fitness: 19.203876495361328\n",
      "mean fitness: 19.927715301513672\n",
      "mean fitness: 19.821887969970703\n",
      "mean fitness: 18.408010482788086\n",
      "mean fitness: 19.632884979248047\n",
      "mean fitness: 19.422107696533203\n",
      "mean fitness: 22.83956527709961\n",
      "mean fitness: 18.88959503173828\n",
      "mean fitness: 23.411714553833008\n",
      "mean fitness: 20.26640510559082\n",
      "mean fitness: 22.64944076538086\n",
      "mean fitness: 23.028244018554688\n",
      "mean fitness: 18.93790626525879\n",
      "mean fitness: 22.69891357421875\n",
      "mean fitness: 20.585147857666016\n",
      "mean fitness: 21.42510986328125\n",
      "mean fitness: 22.599079132080078\n",
      "mean fitness: 17.358932495117188\n",
      "mean fitness: 19.745471954345703\n",
      "mean fitness: 22.583206176757812\n",
      "mean fitness: 21.457719802856445\n",
      "mean fitness: 18.93150520324707\n",
      "mean fitness: 21.405521392822266\n",
      "mean fitness: 20.16724967956543\n",
      "mean fitness: 20.88840103149414\n",
      "mean fitness: 20.218305587768555\n",
      "mean fitness: 19.780967712402344\n",
      "mean fitness: 20.631980895996094\n",
      "mean fitness: 25.178133010864258\n",
      "mean fitness: 20.472152709960938\n",
      "mean fitness: 20.90652847290039\n",
      "mean fitness: 21.771373748779297\n",
      "mean fitness: 23.290584564208984\n",
      "mean fitness: 18.27170753479004\n",
      "mean fitness: 21.50282096862793\n",
      "mean fitness: 17.987642288208008\n",
      "mean fitness: 23.632633209228516\n",
      "mean fitness: 21.31264877319336\n",
      "mean fitness: 21.001827239990234\n",
      "mean fitness: 19.967914581298828\n",
      "mean fitness: 20.865501403808594\n",
      "mean fitness: 22.14400291442871\n",
      "mean fitness: 21.894317626953125\n",
      "mean fitness: 22.660219192504883\n",
      "mean fitness: 21.89954376220703\n",
      "mean fitness: 22.48108673095703\n",
      "mean fitness: 21.08932113647461\n",
      "mean fitness: 23.388776779174805\n",
      "mean fitness: 18.580793380737305\n",
      "mean fitness: 22.848770141601562\n",
      "mean fitness: 19.04094123840332\n",
      "mean fitness: 18.638416290283203\n",
      "mean fitness: 20.976043701171875\n",
      "mean fitness: 21.914329528808594\n",
      "mean fitness: 20.59522247314453\n",
      "mean fitness: 20.226179122924805\n",
      "mean fitness: 21.89773941040039\n",
      "mean fitness: 22.15212631225586\n",
      "mean fitness: 19.475536346435547\n",
      "mean fitness: 21.768627166748047\n",
      "mean fitness: 20.770557403564453\n",
      "mean fitness: 21.919662475585938\n",
      "mean fitness: 22.74103546142578\n",
      "mean fitness: 18.688478469848633\n",
      "mean fitness: 18.78069496154785\n",
      "mean fitness: 22.229476928710938\n",
      "mean fitness: 21.01102638244629\n",
      "mean fitness: 21.46342658996582\n",
      "mean fitness: 19.350540161132812\n",
      "mean fitness: 19.363765716552734\n",
      "mean fitness: 22.76626205444336\n",
      "mean fitness: 20.56977081298828\n",
      "mean fitness: 18.374826431274414\n",
      "mean fitness: 24.18057632446289\n",
      "mean fitness: 21.288724899291992\n",
      "mean fitness: 23.64590072631836\n",
      "mean fitness: 21.2147216796875\n",
      "mean fitness: 20.32769203186035\n",
      "mean fitness: 20.407909393310547\n",
      "mean fitness: 24.988479614257812\n",
      "mean fitness: 24.267295837402344\n",
      "mean fitness: 24.872039794921875\n",
      "mean fitness: 20.325408935546875\n",
      "mean fitness: 21.93068504333496\n",
      "mean fitness: 24.33344841003418\n",
      "mean fitness: 21.256549835205078\n",
      "mean fitness: 21.868633270263672\n",
      "mean fitness: 23.607898712158203\n",
      "mean fitness: 22.842266082763672\n",
      "mean fitness: 23.933568954467773\n"
     ]
    }
   ],
   "source": [
    "no_epochs = [4]\n",
    "\n",
    "envs = [\"walker2d_uni\", \"ant_uni\"]\n",
    "\n",
    "#os.makedirs(\"grad_steps_experiments/reps=16_new/\", exist_ok=True)\n",
    "\n",
    "\n",
    "for env_ in envs:\n",
    "    \n",
    "    env_dir = f\"grad_steps_experiments/value/sample=8/epochs=4/{env_}\"\n",
    "    os.makedirs(env_dir, exist_ok=True)\n",
    "    \n",
    "    for no_epoch in no_epochs:\n",
    "        config = Config(\n",
    "            seed=10,\n",
    "            env_name=env_,\n",
    "            episode_length=1024,\n",
    "            policy_hidden_layer_sizes=[128, 128],\n",
    "            num_evaluations=0,\n",
    "            num_iterations=4000,\n",
    "            num_samples=8,\n",
    "            no_agents=256,\n",
    "            fixed_init_state=False,\n",
    "            discard_dead=False,\n",
    "            grid_shape=[50, 50],\n",
    "            num_init_cvt_samples=50000,\n",
    "            num_centroids=1024,\n",
    "            log_period=400,\n",
    "            store_repertoire=True,\n",
    "            store_repertoire_log_period=800,\n",
    "            iso_sigma=0.005,\n",
    "            line_sigma=0.05,\n",
    "            proportion_mutation_ga=0.5,\n",
    "            buffer_sample_batch_size=8,\n",
    "            buffer_add_batch_size=256,\n",
    "            no_epochs=no_epoch,\n",
    "            #buffer_size=64000,\n",
    "            adam_optimizer=True,\n",
    "            learning_rate=3e-4,\n",
    "            discount_rate=0.99,\n",
    "            clip_param=0.2,\n",
    "            no_neurons=64,\n",
    "            vf_coef=0.5,\n",
    "            activation=\"tanh\",\n",
    "            num_minibatches=32,\n",
    "            max_grad_norm=0.5,\n",
    "            lecun=False,\n",
    "        )\n",
    "        random_key = jax.random.PRNGKey(config.seed)\n",
    "\n",
    "        # Init environment\n",
    "        env = get_env(env_)\n",
    "        reset_fn = jax.jit(env.reset)\n",
    "\n",
    "        # Compute the centroids\n",
    "        centroids, random_key = compute_cvt_centroids(\n",
    "            num_descriptors=env.behavior_descriptor_length,\n",
    "            num_init_cvt_samples=config.num_init_cvt_samples,\n",
    "            num_centroids=config.num_centroids,\n",
    "            minval=0,\n",
    "            maxval=1,\n",
    "            random_key=random_key,\n",
    "        )\n",
    "        # Init policy network\n",
    "        \n",
    "        \n",
    "        obs_normalizer = ObsNormalizer(env.observation_size)\n",
    "        reward_normalizer = RewardNormalizer(config.no_agents)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        policy_network = MLPPPO(\n",
    "            action_dim=env.action_size,\n",
    "            activation=config.activation,\n",
    "            no_neurons=config.no_neurons,\n",
    "        )\n",
    "\n",
    "        # Init population of controllers\n",
    "        \n",
    "        # maybe consider adding two random keys for each policy\n",
    "        random_key, subkey = jax.random.split(random_key)\n",
    "        keys = jax.random.split(subkey, num=config.no_agents)\n",
    "        #split_keys = jax.vmap(lambda k: jax.random.split(k, 2))(keys)\n",
    "        #keys1, keys2 = split_keys[:, 0], split_keys[:, 1]\n",
    "        fake_batch_obs = jnp.zeros(shape=(config.no_agents, env.observation_size))\n",
    "        init_params = jax.vmap(policy_network.init)(keys, fake_batch_obs)\n",
    "\n",
    "        param_count = sum(x[0].size for x in jax.tree_util.tree_leaves(init_params))\n",
    "        print(\"Number of parameters in policy_network: \", param_count)\n",
    "\n",
    "        # Define the fonction to play a step with the policy in the environment\n",
    "        @jax.jit\n",
    "        def play_step_fn(env_state, policy_params, random_key):\n",
    "            random_key, subkey = jax.random.split(random_key)\n",
    "            #pi, val = policy_network.apply(policy_params, env_state.obs)\n",
    "            #action = pi.sample(seed=subkey)\n",
    "            pi, action, val = policy_network.apply(policy_params, env_state.obs)\n",
    "            \n",
    "            logp = pi.log_prob(action)\n",
    "            state_desc = env_state.info[\"state_descriptor\"]\n",
    "            next_state = env.step(env_state, action)\n",
    "            _, _, next_val = policy_network.apply(policy_params, next_state.obs)\n",
    "\n",
    "            transition = PPOTransition(\n",
    "                obs=env_state.obs,\n",
    "                next_obs=next_state.obs,\n",
    "                rewards=next_state.reward,\n",
    "                dones=next_state.done,\n",
    "                truncations=next_state.info[\"truncation\"],\n",
    "                actions=action,\n",
    "                state_desc=state_desc,\n",
    "                next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "                val_adv=val,\n",
    "                target=next_val,\n",
    "                logp=logp\n",
    "                #desc=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "                #desc_prime=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "            )\n",
    "\n",
    "            return (next_state, policy_params, random_key), transition\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Prepare the scoring function\n",
    "        bd_extraction_fn = behavior_descriptor_extractor[config.env_name]\n",
    "        scoring_fn = partial(\n",
    "            scoring_function,\n",
    "            episode_length=config.episode_length,\n",
    "            play_reset_fn=reset_fn,\n",
    "            play_step_fn=play_step_fn,  \n",
    "            behavior_descriptor_extractor=bd_extraction_fn,\n",
    "        )\n",
    "        #reward_offset = get_reward_offset_brax(env, config.env_name)\n",
    "        #print(f\"Reward offset: {reward_offset}\")\n",
    "        \n",
    "        me_scoring_fn = partial(\n",
    "        sampling,\n",
    "        scoring_fn=scoring_fn,\n",
    "        num_samples=config.num_samples,\n",
    "    )\n",
    "\n",
    "        reward_offset = 0\n",
    "    \n",
    "                    \n",
    "        \n",
    "            \n",
    "\n",
    "    \n",
    "        '''\n",
    "        def get_n_offspring_added(metrics):\n",
    "            split = jnp.cumsum(jnp.array([emitter.batch_size for emitter in map_elites._emitter.emitters]))\n",
    "            split = jnp.split(metrics[\"is_offspring_added\"], split, axis=-1)[:-1]\n",
    "            qpg_offspring_added, ai_offspring_added = jnp.split(split[0], (split[0].shape[1]-1,), axis=-1)\n",
    "            return (jnp.sum(split[1], axis=-1), jnp.sum(qpg_offspring_added, axis=-1), jnp.sum(ai_offspring_added, axis=-1))\n",
    "        '''\n",
    "        # Get minimum reward value to make sure qd_score are positive\n",
    "        \n",
    "\n",
    "        # Define a metrics function\n",
    "        metrics_function = partial(\n",
    "            default_qd_metrics,\n",
    "            qd_offset=reward_offset * config.episode_length,\n",
    "        )\n",
    "\n",
    "        # Define the PG-emitter config\n",
    "        \n",
    "        ppo_me_config = PPOMEConfig(\n",
    "            proportion_mutation_ga=config.proportion_mutation_ga,\n",
    "            no_agents=config.no_agents,\n",
    "            buffer_sample_batch_size=config.buffer_sample_batch_size,\n",
    "            buffer_add_batch_size=config.buffer_add_batch_size,\n",
    "            #batch_size=config.batch_size,\n",
    "            #mini_batch_size=config.mini_batch_size,\n",
    "            no_epochs=config.no_epochs,\n",
    "            #buffer_size=config.buffer_size,\n",
    "            learning_rate=config.learning_rate,\n",
    "            adam_optimizer=config.adam_optimizer,\n",
    "            clip_param=config.clip_param,\n",
    "            num_minibatches=config.num_minibatches,\n",
    "            vf_coef=config.vf_coef,\n",
    "            max_grad_norm=config.max_grad_norm,\n",
    "        )\n",
    "        \n",
    "        variation_fn = partial(\n",
    "            isoline_variation, iso_sigma=config.iso_sigma, line_sigma=config.line_sigma\n",
    "        )\n",
    "        \n",
    "        ppo_me_emitter = PPOMEmitter(\n",
    "            config=ppo_me_config,\n",
    "            policy_network=policy_network,\n",
    "            env=env,\n",
    "            variation_fn=variation_fn,\n",
    "            )\n",
    "\n",
    "\n",
    "        # Instantiate MAP Elites\n",
    "        map_elites = MAPElites(\n",
    "            scoring_function=scoring_fn,\n",
    "            emitter=ppo_me_emitter,\n",
    "            metrics_function=metrics_function,\n",
    "        )\n",
    "\n",
    "        fitnesses, descriptors, extra_scores, random_key, obs_normalizer, reward_normalizer = scoring_fn(\n",
    "            init_params, random_key, obs_normalizer, reward_normalizer\n",
    "        )\n",
    "        \n",
    "        repertoire = MapElitesRepertoire.init(\n",
    "            genotypes=init_params,\n",
    "            fitnesses=fitnesses,\n",
    "            descriptors=descriptors,\n",
    "            centroids=centroids,\n",
    "            extra_scores=extra_scores,\n",
    "        )\n",
    "        \n",
    "\n",
    "        emitter_state, random_key = ppo_me_emitter.init(\n",
    "            random_key=random_key,\n",
    "            repertoire=repertoire,\n",
    "            genotypes=init_params,\n",
    "            fitnesses=fitnesses,\n",
    "            descriptors=descriptors,\n",
    "            extra_scores=extra_scores,\n",
    "        )\n",
    "        \n",
    "        emitter_state = ppo_me_emitter.state_update(\n",
    "            emitter_state=emitter_state,\n",
    "            repertoire=repertoire,\n",
    "            genotypes=init_params,\n",
    "            fitnesses=fitnesses,\n",
    "            descriptors=descriptors,\n",
    "            extra_scores={**extra_scores}#, **extra_info},\n",
    "        )\n",
    "        \n",
    "        emitter_state = emitter_state.emitter_states[0]\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        returns = []\n",
    "        old_params = init_params\n",
    "        random_key = jax.random.PRNGKey(0)\n",
    "        for _ in range(250):\n",
    "            random_keys = jax.random.split(random_key, config.no_agents)\n",
    "            new_params = ppo_me_emitter.emitters[0].emit_mcpg(emitter_state, old_params, random_keys)\n",
    "            fitnesses, descriptors, extra_scores, random_key, obs_normalizer, reward_normalizer = scoring_fn(\n",
    "                new_params, random_key, obs_normalizer, reward_normalizer\n",
    "            )\n",
    "            emitter_state = ppo_me_emitter.emitters[0].state_update(\n",
    "                emitter_state=emitter_state,\n",
    "                repertoire=repertoire,\n",
    "                genotypes=new_params,\n",
    "                fitnesses=fitnesses,\n",
    "                descriptors=descriptors,\n",
    "                extra_scores=extra_scores,\n",
    "            )\n",
    "            old_params = new_params\n",
    "            print(f\"mean fitness: {fitnesses.mean()}\")\n",
    "            returns.append(fitnesses)\n",
    "            \n",
    "        returns = jnp.array(returns)  # Assuming 'returns' is already defined as a 2D array\n",
    "\n",
    "        # Determine the overall min and max fitness values for setting y-axis limits\n",
    "        ymin = returns.min()\n",
    "        ymax = returns.max()\n",
    "\n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(16, 16, figsize=(32, 64))  # Adjust the subplot grid to 16x16\n",
    "        for i in range(256):  # Loop through 256 plots\n",
    "            ax = axs[i // 16, i % 16]  # This assumes a 16x16 grid of subplots\n",
    "            ax.plot(returns[:, i])\n",
    "            ax.set_title(f\"Policy {i+1}\", fontsize=8)\n",
    "            ax.set_xlabel('Steps', fontsize=6)\n",
    "            ax.set_ylabel('Returns', fontsize=6)\n",
    "            ax.set_ylim([ymin, ymax])  # Set the same y-axis limits for all subplots\n",
    "            ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # Save each plot in the specified directory with the correct filename\n",
    "        plot_filename = f\"{env_dir}/no_GA.png\"\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close(fig)  # Close the plot to free up memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

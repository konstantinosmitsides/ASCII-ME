{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 28 23:10:06 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A30                     Off | 00000000:01:00.0 Off |                   On |\n",
      "| N/A   30C    P0              25W / 165W |     50MiB / 24576MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                          |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                |        ECC|                       |\n",
      "|==================+================================+===========+=======================|\n",
      "|  0    1   0   0  |              25MiB / 11968MiB  | 28      0 |  2   0    2    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Union, Tuple, Callable, Optional\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "\n",
    "from qdax import environments, environments_v1\n",
    "from jax import random\n",
    "import wandb\n",
    "\n",
    "import pickle\n",
    "from optax import exponential_decay\n",
    "from IPython.display import HTML\n",
    "from brax.io import html\n",
    "import os\n",
    "import jax.debug\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_name):\n",
    "    if env_name == \"hopper_uni\":\n",
    "        episode_length = 1000\n",
    "        \n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "    elif env_name == \"halfcheetah_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "        \n",
    "    elif env_name == \"walker2d_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\t\n",
    "    elif env_name == \"ant_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=True)\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, exclude_current_positions_from_observation=True)\t\n",
    "    '''\n",
    "    elif env_name == \"ant_omni\":\n",
    "        episode_length = 250\n",
    "        max_bd = 30.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=False)\t\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "        max_bd = 1.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length)\t\n",
    "    else:\n",
    "        ValueError(f\"Environment {env_name} not supported.\")\n",
    "    '''\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    \"\"\"MCPG MLP module\"\"\"\n",
    "    hidden_layers_size: Tuple[int, ...]\n",
    "    activation: Callable[[jnp.ndarray], jnp.ndarray] = nn.tanh\n",
    "    bias_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.zeros\n",
    "    hidden_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.lecun_uniform()\n",
    "    value_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.lecun_uniform()\n",
    "    \n",
    "    def setup(self):\n",
    "        self.hidden_layers = [nn.Dense(features, kernel_init=self.hidden_init, bias_init=self.bias_init) for features in self.hidden_layers_size]\n",
    "        self.value = nn.Dense(1, kernel_init=self.value_init, bias_init=self.bias_init)\n",
    "        \n",
    "    def __call__(self, obs: jnp.ndarray):\n",
    "        hidden = obs\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            hidden = self.activation(hidden_layer(hidden))\n",
    "            \n",
    "        value = self.value(hidden)\n",
    "        \n",
    "        return value\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MCPG MLP module\"\"\"\n",
    "    hidden_layers_size: Tuple[int, ...]\n",
    "    action_size: int\n",
    "    activation: Callable[[jnp.ndarray], jnp.ndarray] = nn.tanh\n",
    "    bias_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.zeros\n",
    "    hidden_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.lecun_uniform()\n",
    "    mean_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.lecun_uniform()\n",
    "    \n",
    "    def setup(self):\n",
    "        self.hidden_layers = [nn.Dense(features, kernel_init=self.hidden_init, bias_init=self.bias_init) for features in self.hidden_layers_size]\n",
    "        self.mean = nn.Dense(self.action_size, kernel_init=self.mean_init, bias_init=self.bias_init)\n",
    "        self.log_std = self.param(\"log_std\", lambda _, shape: jnp.log(0.5)*jnp.ones(shape), (self.action_size,))\n",
    "        \n",
    "    def distribution_params(self, obs: jnp.ndarray):\n",
    "        hidden = obs\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            hidden = self.activation(hidden_layer(hidden))\n",
    "            \n",
    "        mean = self.mean(hidden)\n",
    "        log_std = self.log_std\n",
    "        std = jnp.exp(log_std)\n",
    "        \n",
    "        return mean, log_std, std\n",
    "    \n",
    "    def logp(self, obs: jnp.ndarray, action: jnp.ndarray):\n",
    "        mean, _, std = self.distribution_params(obs)\n",
    "        logp = jax.scipy.stats.norm.logpdf(action, mean, std)\n",
    "        return logp.sum(axis=-1)\n",
    "    \n",
    "    def __call__(self, random_key: Any, obs: jnp.ndarray):\n",
    "        mean, _, std = self.distribution_params(obs)\n",
    "        \n",
    "        # Sample action\n",
    "        rnd = jax.random.normal(random_key, shape = (self.action_size,))\n",
    "        action = jax.lax.stop_gradient(mean + rnd * std)\n",
    "        \n",
    "        logp = jnp.sum(jax.scipy.stats.norm.logpdf(action, mean, std), axis=-1) \n",
    "        \n",
    "        return action, logp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    no_agents: int = 1\n",
    "    batch_size: int = 2048\n",
    "    mini_batch_size: int = 64\n",
    "    no_epochs: int = 10\n",
    "    learning_rate: float = 3e-4\n",
    "    discount_rate: float = 0.99\n",
    "    clip_param: float = 0.2\n",
    "    vf_coef: float = 0.5\n",
    "    gae_lambda: float = 0.95\n",
    "    env_name: str = \"ant_uni\"\n",
    "    \n",
    "class MCPG:\n",
    "    \n",
    "    def __init__(self, config, policy, val_net, env):\n",
    "        self._config = config\n",
    "        self._policy = policy\n",
    "        self._value = val_net\n",
    "        self._env = env\n",
    "        \n",
    "    def init(self, random_key):\n",
    "        random_key_1, random_key_2, random_key_3 = jax.random.split(random_key, 3)\n",
    "        fake_obs = jnp.zeros(shape=(self._env.observation_size,))\n",
    "        policy_params = self._policy.init(random_key_1, random_key_2, fake_obs)\n",
    "        tx = optax.adam(learning_rate=self._config.learning_rate, eps=1e-5)\n",
    "        value_params = self._value.init(random_key_3, fake_obs)\n",
    "        train_state_policy = TrainState.create(apply_fn=self._policy.apply, params=policy_params, tx=tx)\n",
    "        train_state_value = TrainState.create(apply_fn=self._value.apply, params=value_params, tx=tx)\n",
    "        \n",
    "        return train_state_policy, train_state_value\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def logp_fn(self, params, obs, action):\n",
    "        return self._policy.apply(params, obs, action, method=self._policy.logp)\n",
    "    \n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def sample_step(self, random_key, train_state_policy, train_state_value, env_state):\n",
    "        \"\"\"Samples one step in the environment and returns the next state, action and \n",
    "        log-prob of the action.\n",
    "        \"\"\"\n",
    "            # Reset the environment if done is True\n",
    "            \n",
    "        #key, subkey = jax.random.split(random_key)\n",
    "        \n",
    "        #key, subkey = jax.random.split(random_key)\n",
    "\n",
    "        # Use jax.lax.cond to conditionally print\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        action, action_logp = train_state_policy.apply_fn(train_state_policy.params, random_key, env_state.obs)\n",
    "        value = train_state_value.apply_fn(train_state_value.params, env_state.obs)\n",
    "        \n",
    "        next_env_state = self._env.step(env_state, action)\n",
    "        \n",
    "        return next_env_state, action, action_logp, value\n",
    "    \n",
    "\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",\"evaluate\"))\n",
    "    def sample_trajectory(self, random_key, train_state_policy, train_state_value, evaluate=False):\n",
    "        \"\"\"Samples a full trajectory using the environment and policy.\"\"\"\n",
    "        if evaluate:\n",
    "            length = self._env.episode_length\n",
    "        else:\n",
    "            length = int(self._config.batch_size / self._config.no_agents) + 1\n",
    "            \n",
    "        random_keys = jax.random.split(random_key, length+1)\n",
    "        env_state_init = self._env.reset(random_keys[-1])\n",
    "        \n",
    "        mean = 0.0\n",
    "        SSD = 0.0\n",
    "        count = 1\n",
    "        \n",
    "        prev_R = 0\n",
    "        def _scan_sample_step(carry, x):\n",
    "            (train_state_policy, train_state_value, env_state, prev_R, mean, SSD, count) = carry\n",
    "            (random_key,) = x\n",
    "            \n",
    "            next_env_state, action, action_logp, value = self.sample_step(random_key, train_state_policy, train_state_value, env_state)\n",
    "            R = next_env_state.reward + self._config.discount_rate * prev_R\n",
    "            prev_R = R\n",
    "        \n",
    "            new_mean = mean + (R - mean) / (count + 1)\n",
    "            new_SSD = SSD + (R - mean) * (R - new_mean)\n",
    "            \n",
    "            std_dev = jnp.sqrt(new_SSD / (count + 1))\n",
    "            \n",
    "            normalized_reward = next_env_state.reward / (std_dev + EPS)\n",
    "            actual_reward = next_env_state.reward\n",
    "\n",
    "            # Calculate the standard deviation of the dynamically sliced rewards\n",
    "            \n",
    "            return (train_state_policy, train_state_value, next_env_state, prev_R, new_mean, new_SSD, count+1), (env_state.obs, action, action_logp, value, normalized_reward, actual_reward, env_state.done, env_state.info[\"state_descriptor\"])\n",
    "        \n",
    "        _, (obs, action, action_logp, values, normalized_reward, actual_reward, done, state_desc) = jax.lax.scan(\n",
    "            _scan_sample_step, \n",
    "            (train_state_policy, train_state_value, env_state_init, prev_R, mean, SSD, count), \n",
    "            (random_keys[:length],),\n",
    "            length=length,\n",
    "            )\n",
    "        \n",
    "        #jax.debug.print(\"done: {}\", jnp.sum(done))\n",
    "        if evaluate:\n",
    "            mask = 1. - jnp.clip(jnp.cumsum(done), a_min=0., a_max=1.)\n",
    "        else:\n",
    "            mask = 1. - jnp.clip(jnp.cumsum(done), a_min=0., a_max=1.)\n",
    "            #mask = 1. - done\n",
    "        #jax.debug.print(\"Mask: {}\", jnp.sum(mask))\n",
    "        '''\n",
    "        next_action = train_state_policy.apply_fn(train_state_policy.params, random_keys[-2], obs[-1])\n",
    "        next_state = self._env.step(final_env_state, next_action)\n",
    "        next_obs = next_state.obs\n",
    "        next_done = next_state.done\n",
    "        next_value = train_state_value.apply_fn(train_state_value.params, next_obs)\n",
    "        '''\n",
    "        \n",
    "        #jax.debug.print(\"Reward shape: {}\", reward[:-1].shape)\n",
    "        \n",
    "        \n",
    "        return obs[:-1], action[:-1], action_logp[:-1], values[:-1], normalized_reward[:-1], actual_reward[:-1], state_desc[:-1], mask[:-1], values[-1], 1. - mask[-1]\n",
    "        \n",
    "        \n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return(self, reward):\n",
    "        \"\"\" Computes the discounted return for each step in the trajectory.\n",
    "        \"\"\"\n",
    "        \n",
    "        def _body(carry, x):\n",
    "            (next_return,) = carry\n",
    "            (reward,) = x\n",
    "            current_return = reward + self._config.discount_rate * next_return\n",
    "            return (current_return,), (current_return,)\n",
    "        \n",
    "        _, (return_,) = jax.lax.scan(\n",
    "            _body,\n",
    "            (jnp.array(0.),),\n",
    "            (reward,),\n",
    "            length=int(self._config.batch_size / self._config.no_agents),\n",
    "            reverse=True,\n",
    "            )\n",
    "            \n",
    "        return return_\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def standardize(self, return_):\n",
    "        return jax.nn.standardize(return_, axis=0, variance=1, epsilon=EPS)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return_standardize(self, reward, mask):\n",
    "        \"\"\"Standardizes the return values for stability in training\n",
    "        \"\"\"\n",
    "        return_ = jax.vmap(self.get_return)(reward * mask)\n",
    "        return self.standardize(return_)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def loss_rein(self, params, obs, action, mask, return_standardized):\n",
    "        \"\"\" REINFORCE loss function.\n",
    "        \"\"\"\n",
    "        logp_ = self.logp_fn(params, jax.lax.stop_gradient(obs), jax.lax.stop_gradient(action))\n",
    "        return -jnp.mean(jnp.multiply(logp_ * mask, jax.lax.stop_gradient(return_standardized)))\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def pg_loss(self, params, obs, action, logp, mask, advantages):\n",
    "        \"\"\" PPO loss function.\n",
    "        \"\"\"\n",
    "        \n",
    "        logp_ = self.logp_fn(params, jax.lax.stop_gradient(obs), jax.lax.stop_gradient(action))\n",
    "        ratio = jnp.exp(logp_ - jax.lax.stop_gradient(logp))\n",
    "        \n",
    "        pg_loss_1 = jnp.multiply(ratio, jax.lax.stop_gradient(advantages))\n",
    "        pg_loss_2 = jax.lax.stop_gradient(advantages) * jax.lax.clamp(1. - self._config.clip_param, ratio, 1. + self._config.clip_param)\n",
    "        \n",
    "        # change the normalizer constant later\n",
    "        return jnp.mean(jnp.maximum(-pg_loss_1, -pg_loss_2))\n",
    "    \n",
    "    \n",
    "    # let's do the unclipped version first\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def value_loss(self, params, train_state_value, obs, returns):\n",
    "        value = train_state_value.apply_fn(params, obs)\n",
    "        return 0.5 * jnp.mean((value - returns) ** 2)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def total_loss(self, policy_params, value_params, obs, action, logp, mask, returns, advantages, train_state_value):\n",
    "        \"\"\" Total loss function.\n",
    "        \"\"\"\n",
    "        return self.pg_loss(policy_params, obs, action, logp, mask, advantages) + self.value_loss(value_params, train_state_value, obs, returns) * self._config.vf_coef\n",
    "\n",
    "    \n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def flatten_trajectory(self, obs, action, logp, mask, returns, advantages):\n",
    "        # Calculate the total number of elements in the combined first two dimensions\n",
    "        total_elements = obs.shape[0] * obs.shape[1]\n",
    "        \n",
    "        new_obs_shape = (total_elements,) + obs.shape[2:]  # obs.shape[2:] should be unpacked if it's a tuple\n",
    "        new_action_shape = (total_elements,) + action.shape[2:]  # Same handling as for obs\n",
    "            \n",
    "        # Flatten the first two dimensions\n",
    "        obs = jnp.reshape(obs, new_obs_shape)\n",
    "        action = jnp.reshape(action, new_action_shape)\n",
    "        logp = jnp.reshape(logp, (total_elements,))\n",
    "        mask = jnp.reshape(mask, (total_elements,))\n",
    "        #return_standardized = jnp.reshape(return_standardized, (total_elements,))\n",
    "        returns = jnp.reshape(returns, (total_elements,))\n",
    "        advantages = jnp.reshape(advantages, (total_elements,))\n",
    "        \n",
    "        print(f\"Shape of obs: {obs.shape}\")\n",
    "        print(f\"Shape of action: {action.shape}\")\n",
    "        print(f\"Shape of logp: {logp.shape}\")\n",
    "        print(f\"Shape of mask: {mask.shape}\")\n",
    "        print(f\"Shape of returns: {returns.shape}\")\n",
    "        print(f\"Shape of advantages: {advantages.shape}\")\n",
    "        \n",
    "                \n",
    "        return obs, action, logp, mask, returns, advantages\n",
    "    \n",
    "    '''\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def compute_gae_and_returns(self, rewards, values, masks, next_value):\n",
    "        #print(values.shape)\n",
    "        #print()\n",
    "        #print(masks.shape)\n",
    "        #print()\n",
    "        #print(next_value.shape)\n",
    "        #print()\n",
    "        #print(jnp.append(values[1:], next_value).shape)\n",
    "        #print()\n",
    "        #print(jnp.append(masks[1:], 1.).shape)\n",
    "        \n",
    "        next_value = next_value.reshape((next_value.shape[0], 1))\n",
    "        values_added = jnp.concatenate((values, next_value), axis=1)\n",
    "        mask_added = jnp.concatenate((masks, jnp.ones((masks.shape[0], 1))), axis=1)\n",
    "        print(f\"Values added: {values_added.shape}\")\n",
    "        print(f\"Mask added: {mask_added.shape}\")\n",
    "        values_next = values_added * mask_added\n",
    "        deltas = rewards + self._config.discount_rate * values_next[:, 1:] - values\n",
    "        \n",
    "        def gae_scan_fn(carry, delta_mask):\n",
    "            gae, _ = carry\n",
    "            delta, mask = delta_mask\n",
    "            gae = delta + self._config.discount_rate * self._config.gae_lambda * mask * gae\n",
    "            \n",
    "            return (gae, mask), gae\n",
    "        \n",
    "        \n",
    "        \n",
    "        last_gae = deltas[-1]\n",
    "        all_but_last = jnp.stack([deltas, masks], axis=-1)\n",
    "        print(f\"all_but_last: {all_but_last.shape}\")\n",
    "        \n",
    "        final_advantages, _ = jax.lax.scan(\n",
    "            gae_scan_fn,\n",
    "            (last_gae, masks[-1]),\n",
    "            all_but_last,\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        advantages = jnp.append(final_advantages, last_gae)\n",
    "        \n",
    "        returns = advantages + values\n",
    "        \n",
    "        return advantages, returns\n",
    "        \n",
    "    '''\n",
    "        \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def compute_gae_and_returns(self, rewards, values, masks, next_value, next_done):\n",
    "        # Ensure next_value is properly shaped to be concatenated\n",
    "        next_value = next_value.reshape((1,))  # Assuming next_value is a single scalar\n",
    "        next_done = next_done.reshape((1,))\n",
    "\n",
    "        # Extend values with next_value at the end for correct future value alignment\n",
    "        next_values = jnp.concatenate((values[1:], next_value), axis=0)\n",
    "        next_masks = jnp.concatenate((masks[1:], 1. - next_done), axis=0)\n",
    "        \n",
    "        # Append 1 to masks at the end to handle terminal states correctly\n",
    "        #masks_extended = jnp.concatenate((masks, jnp.ones((masks.shape[0], 1))), axis=1)\n",
    "        \n",
    "        # Calculate deltas using the extended values and masks\n",
    "        #deltas = rewards + self._config.discount_rate * values_extended[:, 1:] * masks_extended[:, 1:] - values\n",
    "\n",
    "        # GAE calculation setup\n",
    "        #last_advantage = rewards[-1] + self._config.discount_rate * next_value * masks[-1] - values[-1]\n",
    "        \n",
    "        print(\"Rewards shape:\", rewards.shape)\n",
    "        print(\"Values shape:\", values.shape)\n",
    "        print(\"Masks shape:\", masks.shape)\n",
    "        print(\"Next values shape:\", next_values.shape)\n",
    "        print(\"Next masks shape:\", next_masks.shape)\n",
    "        print(next_value)\n",
    "        def gae_scan_fn(carry, x):\n",
    "            (next_advantage,) = carry\n",
    "            (reward, value, next_value, mask) = x\n",
    "            \n",
    "            current_delta = reward + self._config.discount_rate * next_value * mask - value\n",
    "            advantage = current_delta + self._config.discount_rate * self._config.gae_lambda * mask * next_advantage\n",
    "            return (advantage,), (advantage,)\n",
    "            \n",
    "\n",
    "\n",
    "        # Transpose deltas and masks to iterate over the second dimension\n",
    "\n",
    "        # Perform the scan\n",
    "        _, (advantages,) = jax.lax.scan(\n",
    "            gae_scan_fn,\n",
    "            (jnp.array(0.),),\n",
    "            (rewards, values, next_values, next_masks),\n",
    "            length=rewards.shape[0],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        print(type(advantages))\n",
    "        print(type(values))\n",
    "        # Calculate returns by adding values to advantages\n",
    "        returns = advantages + values\n",
    "\n",
    "        return advantages, returns\n",
    "        \n",
    "        \n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def train_step(self, random_key, train_state_policy, train_state_value):\n",
    "        # Sample trajectories\n",
    "        random_keys = jax.random.split(random_key, self._config.no_agents+1)\n",
    "        start_time = time.time()\n",
    "        obs, action, logp, values, reward, actual_reward, _, mask, next_value, next_done = jax.vmap(self.sample_trajectory, in_axes=(0, None, None))(random_keys[:self._config.no_agents], train_state_policy, train_state_value)\n",
    "        values= jnp.squeeze(values, axis=-1)\n",
    "       \n",
    "        \n",
    "        #next_value = train_state_value.apply_fn(train_state_value.params, obs[:, -1])\n",
    "        #next_value = jnp.squeeze(next_value, axis=-1)\n",
    "        \n",
    "        advantages, returns = jax.vmap(self.compute_gae_and_returns, in_axes=(0, 0, 0, 0, 0))(reward, values, mask, next_value, next_done)\n",
    "         \n",
    "        \n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        \n",
    "        # Compute standaerdized return\n",
    "        #print(f\"Reward before passing through the get_return_standardize{obs.shape}\")\n",
    "        #return_standardized = self.get_return_standardize(reward, mask)\n",
    "        \n",
    "       # print(f\"Before flattening{obs.shape}\")\n",
    "        \n",
    "        obs_, action_, logp_, mask_, returns_, advantages_ = self.flatten_trajectory(obs, action, logp, mask, returns, advantages)\n",
    "        \n",
    "        #print(f\"After flattening{obs_.shape}\")\n",
    "        #b_inds = random.permutation(random_keys[-1], self._config.batch_size)\n",
    "        \n",
    "        random_keys_ = jax.random.split(random_keys[-1], self._config.no_epochs)\n",
    "        \n",
    "        def _scan_epoch_train(carry, x):\n",
    "            (train_state_policy, train_state_value) = carry\n",
    "            (random_key,) = x\n",
    "            \n",
    "            (train_state_policy, train_state_value), losses = self.epoch_train(random_key, train_state_policy, train_state_value, obs_, action_, logp_, mask_, returns_, advantages_)\n",
    "            \n",
    "            return (train_state_policy, train_state_value), losses\n",
    "        \n",
    "        #print(\"Before _scan_epoch_train\")\n",
    "        \n",
    "        #print(type(train_state))\n",
    "        (final_train_state_policy, final_train_state_value), losses = jax.lax.scan(\n",
    "            _scan_epoch_train,\n",
    "            (train_state_policy, train_state_value),\n",
    "            (random_keys_,),\n",
    "            length=self._config.no_epochs,\n",
    "            )\n",
    "        \n",
    "        #print(\"After _scan_epoch_train\")\n",
    "        #print(type(final_train_state[0]))\n",
    "        metrics = {\n",
    "            \"loss\" : losses,\n",
    "            \"reward\" : actual_reward * mask,\n",
    "            \"mask\" : mask\n",
    "        }\n",
    "        jax.debug.print(\"Mean Loss: {}\", jnp.mean(metrics[\"loss\"]))\n",
    "        #jax.debug.print(\"Reward shape: {}\", metrics['reward'].shape)\n",
    "        jax.debug.print(\"Mean Reward: {}\", jnp.mean(jnp.sum(metrics[\"reward\"], axis=-1)))\n",
    "        jax.debug.print(\"Mean Mask: {}\", jnp.mean(metrics[\"mask\"]))\n",
    "        jax.debug.print(\"-\" * 50)\n",
    "        \n",
    "        return (final_train_state_policy, final_train_state_value), (metrics,)\n",
    "    \n",
    "    '''\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def epoch_train(self, random_key, train_state, obs, action, logp, mask, return_standardized):\n",
    "        b_inds = random.permutation(random_key, self._config.batch_size)\n",
    "        \n",
    "        \n",
    "        def _scan_mini_train(carry, _):\n",
    "            (train_state, counter) = carry\n",
    "        \n",
    "            idx = b_inds[counter * self._config.mini_batch_size : (counter+1) * self._config.mini_batch_size]\n",
    "            loss, grad = jax.value_and_grad(self.loss_ppo)(train_state.params, obs[idx], action[idx], logp[idx], mask[idx], return_standardized[idx])\n",
    "            new_train_state = train_state.apply_gradients(grads=grad)  \n",
    "            return (new_train_state, counter+1), loss\n",
    "        \n",
    "        final_train_state, losses = jax.lax.scan(\n",
    "            _scan_mini_train,\n",
    "            (train_state, 0),\n",
    "            None,\n",
    "            length=self._config.batch_size // self._config.mini_batch_size,\n",
    "            )\n",
    "        \n",
    "        return (final_train_state,), (losses,)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def epoch_train(self, random_key, train_state_policy, train_state_value, obs, action, logp, mask, returns, advantages):\n",
    "        total_size = self._config.batch_size\n",
    "        \n",
    "        shuffled_indices = jax.random.permutation(random_key, total_size)\n",
    "        \n",
    "        num_batches = self._config.batch_size // self._config.mini_batch_size\n",
    "        batch_indices = jnp.array([shuffled_indices[i * self._config.mini_batch_size:(i + 1) * self._config.mini_batch_size] for i in range(num_batches)])\n",
    "        #jax.debug.print(\"Returns: {}\", return_standardized)\n",
    "        #jax.debug.print(\"Returns shape: {}\", return_standardized.shape)\n",
    "        def _scan_mini_train(carry, x):\n",
    "            (train_state_policy, train_state_value, counter) = carry\n",
    "            (idx,) = x\n",
    "            #jax.debug.print(\"Returns_: {}\", return_standardized[idx])\n",
    "            #jax.debug.print(\"Returns_ shape: {}\", return_standardized[idx].shape)\n",
    "            #jax.debug.print(\"Obs: {}\", obs[idx].shape)\n",
    "            \n",
    "            #value = train_state_value.apply_fn(train_state_value.params, obs[idx])\n",
    "            '''\n",
    "            loss, grad = jax.value_and_grad(self.total_loss)(train_state_policy.params, obs[idx], action[idx], logp[idx], mask[idx], value, returns[idx], advantages[idx])\n",
    "            new_train_state_policy = train_state_policy.apply_gradients(grads=grad)  \n",
    "            new_train_state_value = train_state_value.apply_gradients(grads=grad)\n",
    "            \n",
    "            return (new_train_state_policy, new_train_state_value, counter+1), loss\n",
    "            '''\n",
    "            #jax.debug.print(\"idx: {}\", idx)\n",
    "            loss, (policy_grads, value_grads) = jax.value_and_grad(self.total_loss, argnums=(0, 1))(train_state_policy.params, train_state_value.params, obs[idx], action[idx], logp[idx], mask[idx], returns[idx], advantages[idx], train_state_value)\n",
    "            \n",
    "            #loss, policy_grads = jax.value_and_grad(self.pg_loss)(train_state_policy.params, obs[idx], action[idx], logp[idx], mask[idx], advantages[idx])\n",
    "            #loss_, value_grads = jax.value_and_grad(self.value_loss)(train_state_value.params, train_state_value, obs[idx], returns[idx])\n",
    "            \n",
    "            #policy_loss, policy_grad = jax.value_and_grad(self.pg_loss)(train_state_policy.params, obs[idx], action[idx], logp[idx], mask[idx], advantages[idx])\n",
    "            #value_loss, value_grad = jax.value_and_grad(self.value_loss)(train_state_value.params, value, returns[idx])\n",
    "\n",
    "            # Apply gradients to each network separately\n",
    "            new_train_state_policy = train_state_policy.apply_gradients(grads=policy_grads)\n",
    "            new_train_state_value = train_state_value.apply_gradients(grads=value_grads)\n",
    "            \n",
    "            return (new_train_state_policy, new_train_state_value, counter + 1), loss\n",
    "        \n",
    "        #print('Before _scan_mini_train')\n",
    "        (final_train_state_policy, final_train_state_value, _), losses = jax.lax.scan(\n",
    "            _scan_mini_train,\n",
    "            (train_state_policy, train_state_value, 0),\n",
    "            (batch_indices,),\n",
    "            length=num_batches,\n",
    "            )\n",
    "        \n",
    "        #print('After _scan_mini_train')\n",
    "        \n",
    "        #print(final_train_state)\n",
    "        \n",
    "        return (final_train_state_policy, final_train_state_value), losses\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\", \"no_steps\", \"eval\"))\n",
    "    def train(self, random_key, train_state_policy, train_state_value, no_steps, eval=False):\n",
    "        \"\"\"Trains the policy for a number of steps.\"\"\"\n",
    "        \n",
    "        random_keys = jax.random.split(random_key, no_steps+1)\n",
    "    \n",
    "\n",
    "        def _scan_train_step(carry, x):\n",
    "            (train_state_policy, train_state_value) = carry\n",
    "            (random_key,) = x\n",
    "            \n",
    "            (train_state_policy, train_state_value), (metrics,) = self.train_step(random_key, train_state_policy, train_state_value)\n",
    "            \n",
    "            return (train_state_policy, train_state_value), (metrics,)\n",
    "        \n",
    "        #print('Before  _scan_train_step')\n",
    "        \n",
    "        (train_state_policy, train_state_value), (metrics,) = jax.lax.scan(\n",
    "            _scan_train_step,\n",
    "            (train_state_policy, train_state_value),\n",
    "            (random_keys[:no_steps],),\n",
    "            length=no_steps,\n",
    "            )\n",
    "        \n",
    "        #print('After  _scan_train_step')\n",
    "        \n",
    "        if eval:\n",
    "            mean_reward = self.evaluate(random_keys[-1], train_state_policy, train_state_value)\n",
    "            jax.debug.print(\"Mean Reward over 20 episodes: {}\", mean_reward)\n",
    "            #return mean_reward\n",
    "        \n",
    "        \n",
    "        return train_state_policy, train_state_value, metrics\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def evaluate(self, random_key, train_state_policy, train_state_value):\n",
    "        \"\"\"Evaluates the policy in the environment.\"\"\"\n",
    "        random_keys = jax.random.split(random_key, 20)\n",
    "        \n",
    "        def _scan_evaluate(carry, _):\n",
    "            (train_state_policy, train_state_value) = carry\n",
    "            \n",
    "            _, _, _, _, reward, _, mask, _, _ = jax.vmap(self.sample_trajectory, in_axes=(0, None, None, None))(random_keys, train_state_policy, train_state_value, True)\n",
    "            return (train_state_policy, train_state_value), (reward * mask,)\n",
    "        \n",
    "        (train_state_policy, train_state_value), (reward,) = jax.lax.scan(\n",
    "            _scan_evaluate,\n",
    "            (train_state_policy, train_state_value),\n",
    "            None,\n",
    "            length=20,\n",
    "            )\n",
    "        \n",
    "        return jnp.mean(jnp.sum(reward, axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_mitsides\u001b[0m (\u001b[33mmitsides\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/wandb/run-20240728_231013-60bnpvhb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mitsides/mcpg/runs/60bnpvhb' target=\"_blank\">PPOish</a></strong> to <a href='https://wandb.ai/mitsides/mcpg' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mitsides/mcpg' target=\"_blank\">https://wandb.ai/mitsides/mcpg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mitsides/mcpg/runs/60bnpvhb' target=\"_blank\">https://wandb.ai/mitsides/mcpg/runs/60bnpvhb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluations per training step: 32\n",
      "Rewards shape: (1000,)\n",
      "Values shape: (1000,)\n",
      "Masks shape: (1000,)\n",
      "Next values shape: (1000,)\n",
      "Next masks shape: (1000,)\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=5/0)>\n",
      "<class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>\n",
      "<class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>\n",
      "Shape of obs: (32000, 19)\n",
      "Shape of action: (32000, 6)\n",
      "Shape of logp: (32000,)\n",
      "Shape of mask: (32000,)\n",
      "Shape of returns: (32000,)\n",
      "Shape of advantages: (32000,)\n",
      "Mean Loss: -0.3751281201839447\n",
      "Mean Reward: 227.6048583984375\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: -0.34584346413612366\n",
      "Mean Reward: 220.39080810546875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: -0.2665552794933319\n",
      "Mean Reward: 207.3115692138672\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: -0.09995461255311966\n",
      "Mean Reward: 208.86026000976562\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: -0.077603280544281\n",
      "Mean Reward: 194.46490478515625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: -0.022055653855204582\n",
      "Mean Reward: 233.114501953125\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: -0.06437928229570389\n",
      "Mean Reward: 215.60496520996094\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.047743894159793854\n",
      "Mean Reward: 208.30392456054688\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.07476633042097092\n",
      "Mean Reward: 222.58596801757812\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.0260646753013134\n",
      "Mean Reward: 214.0915069580078\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Step 10, Time: 32.865591049194336\n",
      "Mean Loss: 0.07876376062631607\n",
      "Mean Reward: 206.2412109375\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1214771717786789\n",
      "Mean Reward: 251.10751342773438\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.12060312926769257\n",
      "Mean Reward: 239.311767578125\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.08023425191640854\n",
      "Mean Reward: 239.06161499023438\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1109682023525238\n",
      "Mean Reward: 240.70773315429688\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1848255693912506\n",
      "Mean Reward: 244.6688232421875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.18940848112106323\n",
      "Mean Reward: 251.62136840820312\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.15878863632678986\n",
      "Mean Reward: 246.40478515625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.17910076677799225\n",
      "Mean Reward: 267.24505615234375\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.144684299826622\n",
      "Mean Reward: 265.54876708984375\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Step 20, Time: 17.260655641555786\n",
      "Mean Loss: 0.14903734624385834\n",
      "Mean Reward: 263.22808837890625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.24051031470298767\n",
      "Mean Reward: 283.5880126953125\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.17819581925868988\n",
      "Mean Reward: 280.0904846191406\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.16819578409194946\n",
      "Mean Reward: 277.17889404296875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.17942583560943604\n",
      "Mean Reward: 296.026123046875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.22617113590240479\n",
      "Mean Reward: 298.4344177246094\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.10987920314073563\n",
      "Mean Reward: 260.287841796875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1282748430967331\n",
      "Mean Reward: 302.81866455078125\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.17293959856033325\n",
      "Mean Reward: 296.5783386230469\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.14151443541049957\n",
      "Mean Reward: 296.6677551269531\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Step 30, Time: 17.255568027496338\n",
      "Mean Loss: 0.2167087197303772\n",
      "Mean Reward: 336.7820739746094\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.12913252413272858\n",
      "Mean Reward: 295.94647216796875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.19896838068962097\n",
      "Mean Reward: 305.31036376953125\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.15364743769168854\n",
      "Mean Reward: 318.013916015625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1792527586221695\n",
      "Mean Reward: 323.27734375\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1844256967306137\n",
      "Mean Reward: 333.8360595703125\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.20226134359836578\n",
      "Mean Reward: 329.4991760253906\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.15443862974643707\n",
      "Mean Reward: 326.71246337890625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1481863111257553\n",
      "Mean Reward: 342.75311279296875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1720166653394699\n",
      "Mean Reward: 358.81939697265625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Step 40, Time: 17.244336128234863\n",
      "Mean Loss: 0.15675006806850433\n",
      "Mean Reward: 336.49078369140625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.17616915702819824\n",
      "Mean Reward: 344.0032958984375\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.18327157199382782\n",
      "Mean Reward: 349.5532531738281\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.12332914769649506\n",
      "Mean Reward: 334.60614013671875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "config_dict = {\n",
    "    \"no_agents\": 32,\n",
    "    \"batch_size\": 32 * 1000,\n",
    "    \"mini_batch_size\": 32000,\n",
    "    \"no_epochs\": 10,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"discount_rate\": 0.99,\n",
    "    \"clip_param\": 0.2,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"env_name\": \"halfcheetah_uni\",\n",
    "}\n",
    "\n",
    "# Initialize wandb with the configuration dictionary\n",
    "wandb.init(project=\"mcpg\", name='PPOish', config=config_dict)\n",
    "\n",
    "env = get_env(config_dict[\"env_name\"])\n",
    "\n",
    "\n",
    "policy_hidden_layers = [64, 64]\n",
    "value_hidden_layers = [64, 64]\n",
    "\n",
    "policy = MLP(\n",
    "    hidden_layers_size=policy_hidden_layers,\n",
    "    action_size=env.action_size,\n",
    "    activation=nn.tanh,\n",
    "    hidden_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "    mean_init=jax.nn.initializers.orthogonal(scale=0.01),\n",
    ")\n",
    "\n",
    "value_net = ValueNet(\n",
    "    hidden_layers_size=value_hidden_layers,\n",
    "    hidden_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "    value_init=jax.nn.initializers.orthogonal(scale=1.),\n",
    "    activation=nn.tanh,\n",
    ")\n",
    "\n",
    "agent = MCPG(Config(**wandb.config), policy, value_net, env)\n",
    "\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "train_state_policy, train_state_value = agent.init(random_key)\n",
    "\n",
    "num_steps = 1000\n",
    "log_period = 10\n",
    "\n",
    "metrics_wandb = dict.fromkeys([\"mean loss\", \"mean reward\", \"mask\", \"evaluation\", 'time'], jnp.array([]))\n",
    "eval_num = config_dict[\"no_agents\"]\n",
    "print(f\"Number of evaluations per training step: {eval_num}\")\n",
    "start_time = time.time()\n",
    "for i in range(num_steps // log_period):\n",
    "    random_key, subkey = jax.random.split(random_key)\n",
    "    train_state_policy, train_state_value, current_metrics = agent.train(subkey, train_state_policy, train_state_value, log_period, eval=False)\n",
    "    timelapse = time.time() - start_time\n",
    "    print(f\"Step {(i+1) * log_period}, Time: {timelapse}\")\n",
    "    \n",
    "    current_metrics[\"evaluation\"] = jnp.arange(log_period*eval_num*(i+1), log_period*eval_num*(i+2), dtype=jnp.int32)\n",
    "    current_metrics[\"time\"] = jnp.repeat(timelapse, log_period)\n",
    "    current_metrics[\"mean loss\"] = jnp.repeat(jnp.mean(current_metrics[\"loss\"]), log_period)\n",
    "    current_metrics[\"mean reward\"] = jnp.repeat(jnp.mean(jnp.sum(current_metrics[\"reward\"], axis=-1)), log_period)\n",
    "    current_metrics[\"mask\"] = jnp.repeat(jnp.mean(current_metrics[\"mask\"]), log_period)\n",
    "    '''\n",
    "    metrics_wandb = jax.tree_util.tree_map(lambda metric, current_metric: jnp.concatenate([metric, current_metric], axis=0), metrics_wandb, current_metrics)\n",
    "    \n",
    "    log_metrics = jax.tree_util.tree_map(lambda metric: metric[-1], metrics_wandb)\n",
    "    \n",
    "    wandb.log(log_metrics)\n",
    "    '''\n",
    "    \n",
    "    def update_metrics(old_metrics, new_metrics):\n",
    "        updated_metrics = {}\n",
    "        for key in old_metrics:\n",
    "            if key in new_metrics:\n",
    "                # Check if old metrics for key is empty, and initialize properly if so\n",
    "                if old_metrics[key].size == 0:\n",
    "                    updated_metrics[key] = new_metrics[key]\n",
    "                else:\n",
    "                    updated_metrics[key] = jnp.concatenate([old_metrics[key], new_metrics[key]], axis=0)\n",
    "            else:\n",
    "                raise KeyError(f\"Key {key} not found in new metrics.\")\n",
    "        return updated_metrics\n",
    "\n",
    "    # In your training loop:\n",
    "    try:\n",
    "        metrics_wandb = update_metrics(metrics_wandb, current_metrics)\n",
    "        log_metrics = {k: v[-1] for k, v in metrics_wandb.items()}  # Assuming you want the latest entry\n",
    "        wandb.log(log_metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating metrics: {e}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

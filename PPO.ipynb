{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Union, Tuple, Callable, Optional\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "\n",
    "from qdax import environments, environments_v1\n",
    "from jax import random\n",
    "import wandb\n",
    "\n",
    "import pickle\n",
    "from optax import exponential_decay\n",
    "from IPython.display import HTML\n",
    "from brax.io import html\n",
    "import os\n",
    "import jax.debug\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_name):\n",
    "    if env_name == \"hopper_uni\":\n",
    "        episode_length = 1000\n",
    "        \n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "    elif env_name == \"halfcheetah_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "        \n",
    "    elif env_name == \"walker2d_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\t\n",
    "    elif env_name == \"ant_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=True)\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, exclude_current_positions_from_observation=True)\t\n",
    "    '''\n",
    "    elif env_name == \"ant_omni\":\n",
    "        episode_length = 250\n",
    "        max_bd = 30.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=False)\t\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "        max_bd = 1.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length)\t\n",
    "    else:\n",
    "        ValueError(f\"Environment {env_name} not supported.\")\n",
    "    '''\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MCPG MLP module\"\"\"\n",
    "    hidden_layers_size: Tuple[int, ...]\n",
    "    action_size: int\n",
    "    activation: Callable[[jnp.ndarray], jnp.ndarray] = nn.tanh\n",
    "    bias_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.zeros\n",
    "    hidden_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.lecun_uniform()\n",
    "    mean_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.lecun_uniform()\n",
    "    \n",
    "    def setup(self):\n",
    "        self.hidden_layers = [nn.Dense(features, kernel_init=self.hidden_init, bias_init=self.bias_init) for features in self.hidden_layers_size]\n",
    "        self.mean = nn.Dense(self.action_size, kernel_init=self.mean_init, bias_init=self.bias_init)\n",
    "        self.log_std = self.param(\"log_std\", lambda _, shape: jnp.log(0.5)*jnp.ones(shape), (self.action_size,))\n",
    "        \n",
    "    def distribution_params(self, obs: jnp.ndarray):\n",
    "        hidden = obs\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            hidden = self.activation(hidden_layer(hidden))\n",
    "            \n",
    "        mean = self.mean(hidden)\n",
    "        log_std = self.log_std\n",
    "        std = jnp.exp(log_std)\n",
    "        \n",
    "        return mean, log_std, std\n",
    "    \n",
    "    def logp(self, obs: jnp.ndarray, action: jnp.ndarray):\n",
    "        mean, _, std = self.distribution_params(obs)\n",
    "        logp = jax.scipy.stats.norm.logpdf(action, mean, std)\n",
    "        return logp.sum(axis=-1)\n",
    "    \n",
    "    def __call__(self, random_key: Any, obs: jnp.ndarray):\n",
    "        mean, _, std = self.distribution_params(obs)\n",
    "        \n",
    "        # Sample action\n",
    "        rnd = jax.random.normal(random_key, shape = (self.action_size,))\n",
    "        action = jax.lax.stop_gradient(mean + rnd * std)\n",
    "        \n",
    "        logp = jnp.sum(jax.scipy.stats.norm.logpdf(action, mean, std), axis=-1) \n",
    "                \n",
    "        return action, logp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    no_agents: int = 256\n",
    "    batch_size: int = 256000\n",
    "    mini_batch_size: int = 32000\n",
    "    no_epochs: int = 10\n",
    "    learning_rate: float = 3e-4\n",
    "    discount_rate: float = 0.99\n",
    "    clip_param: float = 0.2\n",
    "    env_name: str = \"ant_uni\"\n",
    "    \n",
    "class MCPG:\n",
    "    \n",
    "    def __init__(self, config, policy, env):\n",
    "        self._config = config\n",
    "        self._policy = policy\n",
    "        self._env = env\n",
    "        \n",
    "    def init(self, random_key):\n",
    "        random_key_1, random_key_2 = jax.random.split(random_key)\n",
    "        fake_obs = jnp.zeros(shape=(self._env.observation_size,))\n",
    "        params = self._policy.init(random_key_1, random_key_2, fake_obs)\n",
    "        tx = optax.adam(learning_rate=self._config.learning_rate)\n",
    "        \n",
    "        return TrainState.create(apply_fn=self._policy.apply, params=params, tx=tx)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def logp_fn(self, params, obs, action):\n",
    "        return self._policy.apply(params, obs, action, method=self._policy.logp)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def sample_step(self, random_key, train_state, env_state):\n",
    "        \"\"\"Samples one step in the environment and returns the next state, action and \n",
    "        log-prob of the action.\n",
    "        \"\"\"\n",
    "        \n",
    "        action, action_logp = train_state.apply_fn(train_state.params, random_key, env_state.obs)\n",
    "        \n",
    "        next_env_state = self._env.step(env_state, action)\n",
    "        \n",
    "        return next_env_state, action, action_logp\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",\"evaluate\"))\n",
    "    def sample_trajectory(self, random_key, train_state, evaluate=False):\n",
    "        \"\"\"Samples a full trajectory using the environment and policy.\"\"\"\n",
    "        if evaluate:\n",
    "            length = self._env.episode_length\n",
    "        else:\n",
    "            length = int(self._config.batch_size / self._config.no_agents)\n",
    "            \n",
    "        random_keys = jax.random.split(random_key, length+1)\n",
    "        env_state_init = self._env.reset(random_keys[-1])\n",
    "        \n",
    "        def _scan_sample_step(carry, x):\n",
    "            (train_state, env_state,) = carry\n",
    "            (random_key, ) = x\n",
    "            \n",
    "            next_env_state, action, action_logp = self.sample_step(random_key, train_state, env_state)\n",
    "            return (train_state, next_env_state), (env_state.obs, action, action_logp, next_env_state.reward, env_state.done, env_state.info[\"state_descriptor\"])\n",
    "        \n",
    "        _, (obs, action, action_logp, reward, done, state_desc) = jax.lax.scan(\n",
    "            _scan_sample_step, \n",
    "            (train_state, env_state_init), \n",
    "            (random_keys[:length],),\n",
    "            length=length,\n",
    "            )\n",
    "        \n",
    "        mask = 1. - jnp.clip(jnp.cumsum(done), a_min=0., a_max=1.)\n",
    "        \n",
    "        return obs, action, action_logp, reward, state_desc, mask\n",
    "    \n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return(self, reward):\n",
    "        \"\"\" Computes the discounted return for each step in the trajectory.\n",
    "        \"\"\"\n",
    "        \n",
    "        def _body(carry, x):\n",
    "            (next_return,) = carry\n",
    "            (reward,) = x\n",
    "            \n",
    "            current_return = reward + self._config.discount_rate * next_return\n",
    "            return (current_return,), (current_return,)\n",
    "        \n",
    "        _, (return_,) = jax.lax.scan(\n",
    "            _body,\n",
    "            (jnp.array(0.),),\n",
    "            (reward,),\n",
    "            length=int(self._config.batch_size / self._config.no_agents),\n",
    "            reverse=True,\n",
    "            )\n",
    "            \n",
    "        return return_\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def standardize(self, return_):\n",
    "        return jax.nn.standardize(return_, axis=0, variance=1, epsilon=EPS)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return_standardize(self, reward, mask):\n",
    "        \"\"\"Standardizes the return values for stability in training\n",
    "        \"\"\"\n",
    "        return_ = jax.vmap(self.get_return)(reward * mask)\n",
    "        return self.standardize(return_)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def loss_rein(self, params, obs, action, mask, return_standardized):\n",
    "        \"\"\" REINFORCE loss function.\n",
    "        \"\"\"\n",
    "        logp_ = self.logp_fn(params, jax.lax.stop_gradient(obs), jax.lax.stop_gradient(action))\n",
    "        return -jnp.mean(jnp.multiply(logp_ * mask, jax.lax.stop_gradient(return_standardized)))\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def loss_ppo(self, params, obs, action, logp, mask, return_standardized):\n",
    "        \"\"\" PPO loss function.\n",
    "        \"\"\"\n",
    "        \n",
    "        logp_ = self.logp_fn(params, jax.lax.stop_gradient(obs), jax.lax.stop_gradient(action))\n",
    "        ratio = jnp.exp(logp_ - jax.lax.stop_gradient(logp))\n",
    "        \n",
    "        pg_loss_1 = jnp.multiply(ratio * mask, jax.lax.stop_gradient(return_standardized))\n",
    "        pg_loss_2 = jax.lax.stop_gradient(return_standardized) * jax.lax.clamp(1. - self._config.clip_param, ratio, 1. + self._config.clip_param) * mask\n",
    "        return -jnp.mean(jnp.minimum(pg_loss_1, pg_loss_2))\n",
    "    \n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def flatten_trajectory(self, obs, action, logp, mask, return_standardized):\n",
    "        # Calculate the total number of elements in the combined first two dimensions\n",
    "        total_elements = obs.shape[0] * obs.shape[1]\n",
    "        \n",
    "        new_obs_shape = (total_elements,) + obs.shape[2:]  # obs.shape[2:] should be unpacked if it's a tuple\n",
    "        new_action_shape = (total_elements,) + action.shape[2:]  # Same handling as for obs\n",
    "            \n",
    "        # Flatten the first two dimensions\n",
    "        obs = jnp.reshape(obs, new_obs_shape)\n",
    "        action = jnp.reshape(action, new_action_shape)\n",
    "        logp = jnp.reshape(logp, (total_elements,))\n",
    "        mask = jnp.reshape(mask, (total_elements,))\n",
    "        return_standardized = jnp.reshape(return_standardized, (total_elements,))\n",
    "                \n",
    "        return obs, action, logp, mask, return_standardized\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def train_step(self, random_key, train_state):\n",
    "        # Sample trajectories\n",
    "        random_keys = jax.random.split(random_key, self._config.no_agents+1)\n",
    "        start_time = time.time()\n",
    "        obs, action, logp, reward, _, mask = jax.vmap(self.sample_trajectory, in_axes=(0, None))(random_keys[:self._config.no_agents], train_state)\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        \n",
    "        # Compute standaerdized return\n",
    "        print(f\"Reward before passing through the get_return_standardize{obs.shape}\")\n",
    "        return_standardized = self.get_return_standardize(reward, mask)\n",
    "        \n",
    "       # print(f\"Before flattening{obs.shape}\")\n",
    "        \n",
    "        obs_, action_, logp_, mask_, return_standardized_ = self.flatten_trajectory(obs, action, logp, mask, return_standardized)\n",
    "        \n",
    "        #print(f\"After flattening{obs_.shape}\")\n",
    "        #b_inds = random.permutation(random_keys[-1], self._config.batch_size)\n",
    "        \n",
    "        random_keys_ = jax.random.split(random_keys[-1], self._config.no_epochs)\n",
    "        \n",
    "        def _scan_epoch_train(carry, x):\n",
    "            (train_state,) = carry\n",
    "            (random_key,) = x\n",
    "            \n",
    "            (train_state,), losses = self.epoch_train(random_key, train_state, obs_, action_, logp_, mask_, return_standardized_)\n",
    "            \n",
    "            return (train_state,), losses\n",
    "        \n",
    "        #print(\"Before _scan_epoch_train\")\n",
    "        \n",
    "        #print(type(train_state))\n",
    "        final_train_state, losses = jax.lax.scan(\n",
    "            _scan_epoch_train,\n",
    "            (train_state,),\n",
    "            (random_keys_,),\n",
    "            length=self._config.no_epochs,\n",
    "            )\n",
    "        \n",
    "        #print(\"After _scan_epoch_train\")\n",
    "        #print(type(final_train_state[0]))\n",
    "        metrics = {\n",
    "            \"loss\" : losses,\n",
    "            \"reward\" : reward * mask,\n",
    "            \"mask\" : mask\n",
    "        }\n",
    "        jax.debug.print(\"Mean Loss: {}\", jnp.mean(metrics[\"loss\"]))\n",
    "        jax.debug.print(\"Mean Reward: {}\", jnp.mean(jnp.sum(metrics[\"reward\"], axis=-1)))\n",
    "        jax.debug.print(\"-\" * 50)\n",
    "        \n",
    "        return (final_train_state[0],), (metrics,)\n",
    "    \n",
    "    '''\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def epoch_train(self, random_key, train_state, obs, action, logp, mask, return_standardized):\n",
    "        b_inds = random.permutation(random_key, self._config.batch_size)\n",
    "        \n",
    "        \n",
    "        def _scan_mini_train(carry, _):\n",
    "            (train_state, counter) = carry\n",
    "        \n",
    "            idx = b_inds[counter * self._config.mini_batch_size : (counter+1) * self._config.mini_batch_size]\n",
    "            loss, grad = jax.value_and_grad(self.loss_ppo)(train_state.params, obs[idx], action[idx], logp[idx], mask[idx], return_standardized[idx])\n",
    "            new_train_state = train_state.apply_gradients(grads=grad)  \n",
    "            return (new_train_state, counter+1), loss\n",
    "        \n",
    "        final_train_state, losses = jax.lax.scan(\n",
    "            _scan_mini_train,\n",
    "            (train_state, 0),\n",
    "            None,\n",
    "            length=self._config.batch_size // self._config.mini_batch_size,\n",
    "            )\n",
    "        \n",
    "        return (final_train_state,), (losses,)\n",
    "    '''\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def epoch_train(self, random_key, train_state, obs, action, logp, mask, return_standardized):\n",
    "        total_size = self._config.batch_size\n",
    "        \n",
    "        shuffled_indices = jax.random.permutation(random_key, total_size)\n",
    "        \n",
    "        num_batches = self._config.batch_size // self._config.mini_batch_size\n",
    "        batch_indices = jnp.array([shuffled_indices[i * self._config.mini_batch_size:(i + 1) * self._config.mini_batch_size] for i in range(num_batches)])\n",
    "        #jax.debug.print(\"Returns: {}\", return_standardized)\n",
    "        #jax.debug.print(\"Returns shape: {}\", return_standardized.shape)\n",
    "        def _scan_mini_train(carry, idx):\n",
    "            (train_state, counter) = carry\n",
    "            #jax.debug.print(\"Returns_: {}\", return_standardized[idx])\n",
    "            #jax.debug.print(\"Returns_ shape: {}\", return_standardized[idx].shape)\n",
    "            #jax.debug.print(\"Obs: {}\", obs[idx].shape)\n",
    "            loss, grad = jax.value_and_grad(self.loss_ppo)(train_state.params, obs[idx], action[idx], logp[idx], mask[idx], return_standardized[idx])\n",
    "            new_train_state = train_state.apply_gradients(grads=grad)  \n",
    "            return (new_train_state, counter+1), loss\n",
    "        \n",
    "        #print('Before _scan_mini_train')\n",
    "        final_train_state, losses = jax.lax.scan(\n",
    "            _scan_mini_train,\n",
    "            (train_state, 0),\n",
    "            batch_indices,\n",
    "            length=num_batches,\n",
    "            )\n",
    "        \n",
    "        #print('After _scan_mini_train')\n",
    "        \n",
    "        return (final_train_state[0],), losses\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\", \"no_steps\", \"eval\"))\n",
    "    def train(self, random_key, train_state, no_steps, eval=False):\n",
    "        \"\"\"Trains the policy for a number of steps.\"\"\"\n",
    "        \n",
    "        random_keys = jax.random.split(random_key, no_steps+1)\n",
    "    \n",
    "\n",
    "        def _scan_train_step(carry, x):\n",
    "            (train_state,) = carry\n",
    "            (random_key,) = x\n",
    "            \n",
    "            (train_state,), (metrics,) = self.train_step(random_key, train_state)\n",
    "            \n",
    "            return (train_state,), (metrics,)\n",
    "        \n",
    "        #print('Before  _scan_train_step')\n",
    "        \n",
    "        (train_state,), (metrics,) = jax.lax.scan(\n",
    "            _scan_train_step,\n",
    "            (train_state,),\n",
    "            (random_keys[:no_steps],),\n",
    "            length=no_steps,\n",
    "            )\n",
    "        \n",
    "        #print('After  _scan_train_step')\n",
    "        if eval:\n",
    "            mean_reward = self.evaluate(random_keys[-1], train_state)\n",
    "            jax.debug.print(\"Mean Reward over 20 episodes: {}\", mean_reward)\n",
    "            return mean_reward\n",
    "        \n",
    "        return train_state, metrics\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def evaluate(self, random_key, train_state):\n",
    "        \"\"\"Evaluates the policy in the environment.\"\"\"\n",
    "        random_keys = jax.random.split(random_key, 20)\n",
    "        \n",
    "        def _scan_evaluate(carry, _):\n",
    "            (train_state,) = carry\n",
    "            \n",
    "            obs, _, _, reward, _, mask = jax.vmap(self.sample_trajectory, in_axes=(0, None, None))(random_keys, train_state, True)\n",
    "            return (train_state,), (reward * mask,)\n",
    "        \n",
    "        (train_state,), (reward,) = jax.lax.scan(\n",
    "            _scan_evaluate,\n",
    "            (train_state,),\n",
    "            None,\n",
    "            length=20,\n",
    "            )\n",
    "        \n",
    "        return jnp.mean(jnp.sum(reward, axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:igw730yt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8cc0e382df46b78acb523be32d1f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PPOish</strong> at: <a href='https://wandb.ai/mitsides/mcpg/runs/igw730yt' target=\"_blank\">https://wandb.ai/mitsides/mcpg/runs/igw730yt</a><br/> View project at: <a href='https://wandb.ai/mitsides/mcpg' target=\"_blank\">https://wandb.ai/mitsides/mcpg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240701_163507-igw730yt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:igw730yt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac34f4a6b14427ea58b924ff92fe0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112276277627745, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/wandb/run-20240701_164721-0tyev5vv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mitsides/mcpg/runs/0tyev5vv' target=\"_blank\">PPOish</a></strong> to <a href='https://wandb.ai/mitsides/mcpg' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mitsides/mcpg' target=\"_blank\">https://wandb.ai/mitsides/mcpg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mitsides/mcpg/runs/0tyev5vv' target=\"_blank\">https://wandb.ai/mitsides/mcpg/runs/0tyev5vv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluations per training step: 256\n",
      "Reward before passing through the get_return_standardize(256, 1024, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 16:47:32.687663: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error:\n",
      "INTERNAL: CpuCallback error: KeyboardInterrupt: <EMPTY MESSAGE>\n",
      "\n",
      "At:\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py(2366): _wrapped_callback\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py(1151): __call__\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/profiler.py(336): wrapper\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(1185): _pjit_call_impl_python\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(1229): call_impl_cache_miss\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(1245): _pjit_call_impl\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/core.py(935): process_primitive\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/core.py(447): bind_with_trace\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/core.py(2740): bind\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(168): _python_pjit_helper\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(257): cache_miss\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/traceback_util.py(179): reraise_with_filtered_traceback\n",
      "  /tmp/ipykernel_1436758/4078604604.py(42): <module>\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3577): run_code\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3517): run_ast_nodes\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3334): run_cell_async\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/async_helpers.py(129): _pseudo_sync_runner\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3130): _run_cell\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3075): run_cell\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/zmqshell.py(549): run_cell\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/ipkernel.py(449): do_execute\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(778): execute_request\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/ipkernel.py(362): execute_request\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(437): dispatch_shell\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(534): process_one\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(545): dispatch_queue\n",
      "  /usr/lib/python3.10/asyncio/events.py(80): _run\n",
      "  /usr/lib/python3.10/asyncio/base_events.py(1909): _run_once\n",
      "  /usr/lib/python3.10/asyncio/base_events.py(603): run_forever\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/tornado/platform/asyncio.py(205): start\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelapp.py(739): start\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/traitlets/config/application.py(1075): launch_instance\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel_launcher.py(18): <module>\n",
      "  /usr/lib/python3.10/runpy.py(86): _run_code\n",
      "  /usr/lib/python3.10/runpy.py(196): _run_module_as_main\n",
      "\n",
      "2024-07-01 16:47:32.687723: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2732] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: CpuCallback error: KeyboardInterrupt: <EMPTY MESSAGE>\n",
      "\n",
      "At:\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py(2366): _wrapped_callback\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py(1151): __call__\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/profiler.py(336): wrapper\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(1185): _pjit_call_impl_python\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(1229): call_impl_cache_miss\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(1245): _pjit_call_impl\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/core.py(935): process_primitive\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/core.py(447): bind_with_trace\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/core.py(2740): bind\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(168): _python_pjit_helper\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(257): cache_miss\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/traceback_util.py(179): reraise_with_filtered_traceback\n",
      "  /tmp/ipykernel_1436758/4078604604.py(42): <module>\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3577): run_code\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3517): run_ast_nodes\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3334): run_cell_async\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/async_helpers.py(129): _pseudo_sync_runner\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3130): _run_cell\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3075): run_cell\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/zmqshell.py(549): run_cell\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/ipkernel.py(449): do_execute\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(778): execute_request\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/ipkernel.py(362): execute_request\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(437): dispatch_shell\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(534): process_one\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(545): dispatch_queue\n",
      "  /usr/lib/python3.10/asyncio/events.py(80): _run\n",
      "  /usr/lib/python3.10/asyncio/base_events.py(1909): _run_once\n",
      "  /usr/lib/python3.10/asyncio/base_events.py(603): run_forever\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/tornado/platform/asyncio.py(205): start\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelapp.py(739): start\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/traitlets/config/application.py(1075): launch_instance\n",
      "  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel_launcher.py(18): <module>\n",
      "  /usr/lib/python3.10/runpy.py(86): _run_code\n",
      "  /usr/lib/python3.10/runpy.py(196): _run_module_as_main\n",
      "; current tracing scope: custom-call.32; current profiling annotation: XlaModule:#prefix=jit(train)/jit(main),hlo_module=jit_train,program_id=857#.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: CpuCallback error: KeyboardInterrupt: <EMPTY MESSAGE>\n\nAt:\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py(2366): _wrapped_callback\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py(1151): __call__\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/profiler.py(336): wrapper\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(1185): _pjit_call_impl_python\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(1229): call_impl_cache_miss\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(1245): _pjit_call_impl\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/core.py(935): process_primitive\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/core.py(447): bind_with_trace\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/core.py(2740): bind\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(168): _python_pjit_helper\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(257): cache_miss\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/traceback_util.py(179): reraise_with_filtered_traceback\n  /tmp/ipykernel_1436758/4078604604.py(42): <module>\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3577): run_code\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3517): run_ast_nodes\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3334): run_cell_async\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/async_helpers.py(129): _pseudo_sync_runner\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3130): _run_cell\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3075): run_cell\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/zmqshell.py(549): run_cell\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/ipkernel.py(449): do_execute\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(778): execute_request\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/ipkernel.py(362): execute_request\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(437): dispatch_shell\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(534): process_one\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(545): dispatch_queue\n  /usr/lib/python3.10/asyncio/events.py(80): _run\n  /usr/lib/python3.10/asyncio/base_events.py(1909): _run_once\n  /usr/lib/python3.10/asyncio/base_events.py(603): run_forever\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/tornado/platform/asyncio.py(205): start\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelapp.py(739): start\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/traitlets/config/application.py(1075): launch_instance\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel_launcher.py(18): <module>\n  /usr/lib/python3.10/runpy.py(86): _run_code\n  /usr/lib/python3.10/runpy.py(196): _run_module_as_main\n; current tracing scope: custom-call.32; current profiling annotation: XlaModule:#prefix=jit(train)/jit(main),hlo_module=jit_train,program_id=857#.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m log_period):\n\u001b[1;32m     41\u001b[0m     random_key, subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(random_key)\n\u001b[0;32m---> 42\u001b[0m     train_state, current_metrics \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_period\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     timelapse \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mlog_period\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimelapse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1151\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mordered_effects \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_unordered_effects\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_host_callbacks):\n\u001b[1;32m   1150\u001b[0m   input_bufs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_tokens_to_inputs(input_bufs)\n\u001b[0;32m-> 1151\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   1153\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m   result_token_bufs \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_prefix_into_single_device_arrays(\n\u001b[1;32m   1155\u001b[0m       \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mordered_effects))\n\u001b[1;32m   1156\u001b[0m   sharded_runtime_token \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mconsume_token()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: CpuCallback error: KeyboardInterrupt: <EMPTY MESSAGE>\n\nAt:\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py(2366): _wrapped_callback\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py(1151): __call__\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/profiler.py(336): wrapper\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(1185): _pjit_call_impl_python\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(1229): call_impl_cache_miss\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(1245): _pjit_call_impl\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/core.py(935): process_primitive\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/core.py(447): bind_with_trace\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/core.py(2740): bind\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(168): _python_pjit_helper\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/pjit.py(257): cache_miss\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/traceback_util.py(179): reraise_with_filtered_traceback\n  /tmp/ipykernel_1436758/4078604604.py(42): <module>\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3577): run_code\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3517): run_ast_nodes\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3334): run_cell_async\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/async_helpers.py(129): _pseudo_sync_runner\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3130): _run_cell\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3075): run_cell\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/zmqshell.py(549): run_cell\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/ipkernel.py(449): do_execute\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(778): execute_request\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/ipkernel.py(362): execute_request\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(437): dispatch_shell\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(534): process_one\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(545): dispatch_queue\n  /usr/lib/python3.10/asyncio/events.py(80): _run\n  /usr/lib/python3.10/asyncio/base_events.py(1909): _run_once\n  /usr/lib/python3.10/asyncio/base_events.py(603): run_forever\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/tornado/platform/asyncio.py(205): start\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel/kernelapp.py(739): start\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/traitlets/config/application.py(1075): launch_instance\n  /vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/ipykernel_launcher.py(18): <module>\n  /usr/lib/python3.10/runpy.py(86): _run_code\n  /usr/lib/python3.10/runpy.py(196): _run_module_as_main\n; current tracing scope: custom-call.32; current profiling annotation: XlaModule:#prefix=jit(train)/jit(main),hlo_module=jit_train,program_id=857#."
     ]
    }
   ],
   "source": [
    "config_dict = {\n",
    "    \"no_agents\": 256,\n",
    "    \"batch_size\": 1024 * 256,\n",
    "    \"mini_batch_size\": 1024 * 256,\n",
    "    \"no_epochs\": 16,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"discount_rate\": 0.99,\n",
    "    \"clip_param\": 0.2,\n",
    "    \"env_name\": \"ant_uni\",\n",
    "}\n",
    "\n",
    "# Initialize wandb with the configuration dictionary\n",
    "wandb.init(project=\"mcpg\", name='PPOish', config=config_dict)\n",
    "\n",
    "env = get_env(config_dict[\"env_name\"])\n",
    "\n",
    "\n",
    "policy_hidden_layers = [128, 128]\n",
    "\n",
    "policy = MLP(\n",
    "    hidden_layers_size=policy_hidden_layers,\n",
    "    action_size=env.action_size,\n",
    "    activation=nn.tanh,\n",
    "    hidden_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "    mean_init=jax.nn.initializers.orthogonal(scale=0.01),\n",
    ")\n",
    "\n",
    "agent = MCPG(Config(**wandb.config), policy, env)\n",
    "\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "train_state = agent.init(random_key)\n",
    "\n",
    "num_steps = 1000\n",
    "log_period = 10\n",
    "\n",
    "metrics_wandb = dict.fromkeys([\"mean loss\", \"mean reward\", \"mask\", \"evaluation\", 'time'], jnp.array([]))\n",
    "eval_num = config_dict[\"no_agents\"]\n",
    "print(f\"Number of evaluations per training step: {eval_num}\")\n",
    "start_time = time.time()\n",
    "for i in range(num_steps // log_period):\n",
    "    random_key, subkey = jax.random.split(random_key)\n",
    "    train_state, current_metrics = agent.train(subkey, train_state, log_period)\n",
    "    timelapse = time.time() - start_time\n",
    "    print(f\"Step {(i+1) * log_period}, Time: {timelapse}\")\n",
    "    \n",
    "    current_metrics[\"evaluation\"] = jnp.arange(log_period*eval_num*(i+1), log_period*eval_num*(i+2), dtype=jnp.int32)\n",
    "    current_metrics[\"time\"] = jnp.repeat(timelapse, log_period)\n",
    "    current_metrics[\"mean loss\"] = jnp.repeat(jnp.mean(current_metrics[\"loss\"]), log_period)\n",
    "    current_metrics[\"mean reward\"] = jnp.repeat(jnp.mean(jnp.sum(current_metrics[\"reward\"], axis=-1)), log_period)\n",
    "    current_metrics[\"mask\"] = jnp.repeat(jnp.mean(current_metrics[\"mask\"]), log_period)\n",
    "    '''\n",
    "    metrics_wandb = jax.tree_util.tree_map(lambda metric, current_metric: jnp.concatenate([metric, current_metric], axis=0), metrics_wandb, current_metrics)\n",
    "    \n",
    "    log_metrics = jax.tree_util.tree_map(lambda metric: metric[-1], metrics_wandb)\n",
    "    \n",
    "    wandb.log(log_metrics)\n",
    "    '''\n",
    "    \n",
    "    def update_metrics(old_metrics, new_metrics):\n",
    "        updated_metrics = {}\n",
    "        for key in old_metrics:\n",
    "            if key in new_metrics:\n",
    "                # Check if old metrics for key is empty, and initialize properly if so\n",
    "                if old_metrics[key].size == 0:\n",
    "                    updated_metrics[key] = new_metrics[key]\n",
    "                else:\n",
    "                    updated_metrics[key] = jnp.concatenate([old_metrics[key], new_metrics[key]], axis=0)\n",
    "            else:\n",
    "                raise KeyError(f\"Key {key} not found in new metrics.\")\n",
    "        return updated_metrics\n",
    "\n",
    "    # In your training loop:\n",
    "    try:\n",
    "        metrics_wandb = update_metrics(metrics_wandb, current_metrics)\n",
    "        log_metrics = {k: v[-1] for k, v in metrics_wandb.items()}  # Assuming you want the latest entry\n",
    "        wandb.log(log_metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating metrics: {e}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

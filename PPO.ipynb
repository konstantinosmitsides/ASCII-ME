{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 25 17:15:35 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A30                     Off | 00000000:02:00.0 Off |                   On |\n",
      "| N/A   28C    P0              28W / 165W |   2734MiB / 24576MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                          |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                |        ECC|                       |\n",
      "|==================+================================+===========+=======================|\n",
      "|  0    1   0   0  |            2708MiB / 11968MiB  | 28      0 |  2   0    2    0    0 |\n",
      "|                  |               2MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0    1    0     897026      C   ...-sample-based-drl/my_env/bin/python     2676MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Union, Tuple, Callable, Optional\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "\n",
    "from qdax import environments, environments_v1\n",
    "from jax import random\n",
    "import wandb\n",
    "\n",
    "import pickle\n",
    "from optax import exponential_decay\n",
    "from IPython.display import HTML\n",
    "from brax.io import html\n",
    "import os\n",
    "import jax.debug\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_name):\n",
    "    if env_name == \"hopper_uni\":\n",
    "        episode_length = 1000\n",
    "        \n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "    elif env_name == \"halfcheetah_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "        \n",
    "    elif env_name == \"walker2d_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\t\n",
    "    elif env_name == \"ant_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=True)\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, exclude_current_positions_from_observation=True)\t\n",
    "    '''\n",
    "    elif env_name == \"ant_omni\":\n",
    "        episode_length = 250\n",
    "        max_bd = 30.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=False)\t\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "        max_bd = 1.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length)\t\n",
    "    else:\n",
    "        ValueError(f\"Environment {env_name} not supported.\")\n",
    "    '''\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env('walker2d_uni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 6 2\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_size, env.action_size, env.state_descriptor_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    \"\"\"MCPG MLP module\"\"\"\n",
    "    hidden_layers_size: Tuple[int, ...]\n",
    "    activation: Callable[[jnp.ndarray], jnp.ndarray] = nn.tanh\n",
    "    bias_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.zeros\n",
    "    hidden_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.lecun_uniform()\n",
    "    value_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.lecun_uniform()\n",
    "    \n",
    "    def setup(self):\n",
    "        self.hidden_layers = [nn.Dense(features, kernel_init=self.hidden_init, bias_init=self.bias_init) for features in self.hidden_layers_size]\n",
    "        self.value = nn.Dense(1, kernel_init=self.value_init, bias_init=self.bias_init)\n",
    "        \n",
    "    def __call__(self, obs: jnp.ndarray):\n",
    "        hidden = obs\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            hidden = self.activation(hidden_layer(hidden))\n",
    "            \n",
    "        value = self.value(hidden)\n",
    "        \n",
    "        return value\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MCPG MLP module\"\"\"\n",
    "    hidden_layers_size: Tuple[int, ...]\n",
    "    action_size: int\n",
    "    activation: Callable[[jnp.ndarray], jnp.ndarray] = nn.tanh\n",
    "    bias_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.zeros\n",
    "    hidden_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.lecun_uniform()\n",
    "    mean_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.lecun_uniform()\n",
    "    \n",
    "    def setup(self):\n",
    "        self.hidden_layers = [nn.Dense(features, kernel_init=self.hidden_init, bias_init=self.bias_init) for features in self.hidden_layers_size]\n",
    "        self.mean = nn.Dense(self.action_size, kernel_init=self.mean_init, bias_init=self.bias_init)\n",
    "        self.log_std = self.param(\"log_std\", lambda _, shape: jnp.log(0.5)*jnp.ones(shape), (self.action_size,))\n",
    "        \n",
    "    def distribution_params(self, obs: jnp.ndarray):\n",
    "        hidden = obs\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            hidden = self.activation(hidden_layer(hidden))\n",
    "            \n",
    "        mean = self.mean(hidden)\n",
    "        log_std = self.log_std\n",
    "        std = jnp.exp(log_std)\n",
    "        \n",
    "        return mean, log_std, std\n",
    "    \n",
    "    def logp(self, obs: jnp.ndarray, action: jnp.ndarray):\n",
    "        mean, _, std = self.distribution_params(obs)\n",
    "        logp = jax.scipy.stats.norm.logpdf(action, mean, std)\n",
    "        return logp.sum(axis=-1)\n",
    "    \n",
    "    def __call__(self, random_key: Any, obs: jnp.ndarray):\n",
    "        mean, _, std = self.distribution_params(obs)\n",
    "        \n",
    "        # Sample action\n",
    "        rnd = jax.random.normal(random_key, shape = (self.action_size,))\n",
    "        action = jax.lax.stop_gradient(mean + rnd * std)\n",
    "        \n",
    "        logp = jnp.sum(jax.scipy.stats.norm.logpdf(action, mean, std), axis=-1) \n",
    "        \n",
    "        return action, logp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    no_agents: int = 1\n",
    "    batch_size: int = 2048\n",
    "    mini_batch_size: int = 64\n",
    "    no_epochs: int = 10\n",
    "    learning_rate: float = 3e-4\n",
    "    discount_rate: float = 0.99\n",
    "    clip_param: float = 0.2\n",
    "    vf_coef: float = 0.5\n",
    "    gae_lambda: float = 0.95\n",
    "    env_name: str = \"ant_uni\"\n",
    "    \n",
    "class MCPG:\n",
    "    \n",
    "    def __init__(self, config, policy, val_net, env):\n",
    "        self._config = config\n",
    "        self._policy = policy\n",
    "        self._value = val_net\n",
    "        self._env = env\n",
    "        \n",
    "    def init(self, random_key):\n",
    "        random_key_1, random_key_2, random_key_3 = jax.random.split(random_key, 3)\n",
    "        fake_obs = jnp.zeros(shape=(self._env.observation_size,))\n",
    "        policy_params = self._policy.init(random_key_1, random_key_2, fake_obs)\n",
    "        tx = optax.adam(learning_rate=self._config.learning_rate)\n",
    "        value_params = self._value.init(random_key_3, fake_obs)\n",
    "        train_state_policy = TrainState.create(apply_fn=self._policy.apply, params=policy_params, tx=tx)\n",
    "        train_state_value = TrainState.create(apply_fn=self._value.apply, params=value_params, tx=tx)\n",
    "        \n",
    "        return train_state_policy, train_state_value\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def logp_fn(self, params, obs, action):\n",
    "        return self._policy.apply(params, obs, action, method=self._policy.logp)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def sample_step(self, random_key, train_state_policy, train_state_value, env_state):\n",
    "        \"\"\"Samples one step in the environment and returns the next state, action and \n",
    "        log-prob of the action.\n",
    "        \"\"\"\n",
    "        \n",
    "        action, action_logp = train_state_policy.apply_fn(train_state_policy.params, random_key, env_state.obs)\n",
    "        value = train_state_value.apply_fn(train_state_value.params, env_state.obs)\n",
    "        \n",
    "        next_env_state = self._env.step(env_state, action)\n",
    "        \n",
    "        return next_env_state, action, action_logp, value\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",\"evaluate\"))\n",
    "    def sample_trajectory(self, random_key, train_state_policy, train_state_value, evaluate=False):\n",
    "        \"\"\"Samples a full trajectory using the environment and policy.\"\"\"\n",
    "        if evaluate:\n",
    "            length = self._env.episode_length\n",
    "        else:\n",
    "            length = int(self._config.batch_size / self._config.no_agents)\n",
    "            \n",
    "        random_keys = jax.random.split(random_key, length+1)\n",
    "        env_state_init = self._env.reset(random_keys[-1])\n",
    "        \n",
    "        def _scan_sample_step(carry, x):\n",
    "            (train_state_policy, train_state_value, env_state,) = carry\n",
    "            (random_key, ) = x\n",
    "            \n",
    "            next_env_state, action, action_logp, value = self.sample_step(random_key, train_state_policy, train_state_value, env_state)\n",
    "            return (train_state_policy, train_state_value, next_env_state), (env_state.obs, action, action_logp, value, next_env_state.reward, env_state.done, env_state.info[\"state_descriptor\"])\n",
    "        \n",
    "        _, (obs, action, action_logp, values, reward, done, state_desc) = jax.lax.scan(\n",
    "            _scan_sample_step, \n",
    "            (train_state_policy, train_state_value, env_state_init), \n",
    "            (random_keys[:length],),\n",
    "            length=length,\n",
    "            )\n",
    "        \n",
    "        mask = 1. - jnp.clip(jnp.cumsum(done), a_min=0., a_max=1.)\n",
    "        \n",
    "        return obs, action, action_logp, values, reward, state_desc, mask\n",
    "    \n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return(self, reward):\n",
    "        \"\"\" Computes the discounted return for each step in the trajectory.\n",
    "        \"\"\"\n",
    "        \n",
    "        def _body(carry, x):\n",
    "            (next_return,) = carry\n",
    "            (reward,) = x\n",
    "            current_return = reward + self._config.discount_rate * next_return\n",
    "            return (current_return,), (current_return,)\n",
    "        \n",
    "        _, (return_,) = jax.lax.scan(\n",
    "            _body,\n",
    "            (jnp.array(0.),),\n",
    "            (reward,),\n",
    "            length=int(self._config.batch_size / self._config.no_agents),\n",
    "            reverse=True,\n",
    "            )\n",
    "            \n",
    "        return return_\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def standardize(self, return_):\n",
    "        return jax.nn.standardize(return_, axis=0, variance=1, epsilon=EPS)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return_standardize(self, reward, mask):\n",
    "        \"\"\"Standardizes the return values for stability in training\n",
    "        \"\"\"\n",
    "        return_ = jax.vmap(self.get_return)(reward * mask)\n",
    "        return self.standardize(return_)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def loss_rein(self, params, obs, action, mask, return_standardized):\n",
    "        \"\"\" REINFORCE loss function.\n",
    "        \"\"\"\n",
    "        logp_ = self.logp_fn(params, jax.lax.stop_gradient(obs), jax.lax.stop_gradient(action))\n",
    "        return -jnp.mean(jnp.multiply(logp_ * mask, jax.lax.stop_gradient(return_standardized)))\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def pg_loss(self, params, obs, action, logp, mask, advantages):\n",
    "        \"\"\" PPO loss function.\n",
    "        \"\"\"\n",
    "        \n",
    "        logp_ = self.logp_fn(params, jax.lax.stop_gradient(obs), jax.lax.stop_gradient(action))\n",
    "        ratio = jnp.exp(logp_ - jax.lax.stop_gradient(logp))\n",
    "        \n",
    "        pg_loss_1 = jnp.multiply(ratio * mask, jax.lax.stop_gradient(advantages))\n",
    "        pg_loss_2 = jax.lax.stop_gradient(advantages) * jax.lax.clamp(1. - self._config.clip_param, ratio, 1. + self._config.clip_param) * mask\n",
    "        \n",
    "        # change the normalizer constant later\n",
    "        return -jnp.mean(jnp.minimum(pg_loss_1, pg_loss_2))\n",
    "    \n",
    "    \n",
    "    # let's do the unclipped version first\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def value_loss(self, params, train_state_value, obs, returns):\n",
    "        value = train_state_value.apply_fn(params, obs)\n",
    "        return 0.5 * jnp.mean((value - returns) ** 2)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def total_loss(self, policy_params, value_params, obs, action, logp, mask, returns, advantages, train_state_value):\n",
    "        \"\"\" Total loss function.\n",
    "        \"\"\"\n",
    "        return self.pg_loss(policy_params, obs, action, logp, mask, advantages) + self.value_loss(value_params, train_state_value, obs, returns) * self._config.vf_coef\n",
    "\n",
    "    \n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def flatten_trajectory(self, obs, action, logp, mask, returns, advantages):\n",
    "        # Calculate the total number of elements in the combined first two dimensions\n",
    "        total_elements = obs.shape[0] * obs.shape[1]\n",
    "        \n",
    "        new_obs_shape = (total_elements,) + obs.shape[2:]  # obs.shape[2:] should be unpacked if it's a tuple\n",
    "        new_action_shape = (total_elements,) + action.shape[2:]  # Same handling as for obs\n",
    "            \n",
    "        # Flatten the first two dimensions\n",
    "        obs = jnp.reshape(obs, new_obs_shape)\n",
    "        action = jnp.reshape(action, new_action_shape)\n",
    "        logp = jnp.reshape(logp, (total_elements,))\n",
    "        mask = jnp.reshape(mask, (total_elements,))\n",
    "        #return_standardized = jnp.reshape(return_standardized, (total_elements,))\n",
    "        returns = jnp.reshape(returns, (total_elements,))\n",
    "        advantages = jnp.reshape(advantages, (total_elements,))\n",
    "        \n",
    "        print(f\"Shape of obs: {obs.shape}\")\n",
    "        print(f\"Shape of action: {action.shape}\")\n",
    "        print(f\"Shape of logp: {logp.shape}\")\n",
    "        print(f\"Shape of mask: {mask.shape}\")\n",
    "        print(f\"Shape of returns: {returns.shape}\")\n",
    "        print(f\"Shape of advantages: {advantages.shape}\")\n",
    "        \n",
    "                \n",
    "        return obs, action, logp, mask, returns, advantages\n",
    "    \n",
    "    '''\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def compute_gae_and_returns(self, rewards, values, masks, next_value):\n",
    "        #print(values.shape)\n",
    "        #print()\n",
    "        #print(masks.shape)\n",
    "        #print()\n",
    "        #print(next_value.shape)\n",
    "        #print()\n",
    "        #print(jnp.append(values[1:], next_value).shape)\n",
    "        #print()\n",
    "        #print(jnp.append(masks[1:], 1.).shape)\n",
    "        \n",
    "        next_value = next_value.reshape((next_value.shape[0], 1))\n",
    "        values_added = jnp.concatenate((values, next_value), axis=1)\n",
    "        mask_added = jnp.concatenate((masks, jnp.ones((masks.shape[0], 1))), axis=1)\n",
    "        print(f\"Values added: {values_added.shape}\")\n",
    "        print(f\"Mask added: {mask_added.shape}\")\n",
    "        values_next = values_added * mask_added\n",
    "        deltas = rewards + self._config.discount_rate * values_next[:, 1:] - values\n",
    "        \n",
    "        def gae_scan_fn(carry, delta_mask):\n",
    "            gae, _ = carry\n",
    "            delta, mask = delta_mask\n",
    "            gae = delta + self._config.discount_rate * self._config.gae_lambda * mask * gae\n",
    "            \n",
    "            return (gae, mask), gae\n",
    "        \n",
    "        \n",
    "        \n",
    "        last_gae = deltas[-1]\n",
    "        all_but_last = jnp.stack([deltas, masks], axis=-1)\n",
    "        print(f\"all_but_last: {all_but_last.shape}\")\n",
    "        \n",
    "        final_advantages, _ = jax.lax.scan(\n",
    "            gae_scan_fn,\n",
    "            (last_gae, masks[-1]),\n",
    "            all_but_last,\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        advantages = jnp.append(final_advantages, last_gae)\n",
    "        \n",
    "        returns = advantages + values\n",
    "        \n",
    "        return advantages, returns\n",
    "        \n",
    "    '''\n",
    "        \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def compute_gae_and_returns(self, rewards, values, masks, next_value):\n",
    "        # Ensure next_value is properly shaped to be concatenated\n",
    "        next_value = next_value.reshape((1,))  # Assuming next_value is a single scalar\n",
    "\n",
    "        # Extend values with next_value at the end for correct future value alignment\n",
    "        next_values = jnp.concatenate((values[1:], next_value), axis=0)\n",
    "        \n",
    "        # Append 1 to masks at the end to handle terminal states correctly\n",
    "        #masks_extended = jnp.concatenate((masks, jnp.ones((masks.shape[0], 1))), axis=1)\n",
    "        \n",
    "        # Calculate deltas using the extended values and masks\n",
    "        #deltas = rewards + self._config.discount_rate * values_extended[:, 1:] * masks_extended[:, 1:] - values\n",
    "\n",
    "        # GAE calculation setup\n",
    "        #last_advantage = rewards[-1] + self._config.discount_rate * next_value * masks[-1] - values[-1]\n",
    "        \n",
    "        print(\"Rewards shape:\", rewards.shape)\n",
    "        print(\"Values shape:\", values.shape)\n",
    "        print(\"Masks shape:\", masks.shape)\n",
    "        print(\"Next value shape:\", next_value.shape)\n",
    "        print(next_value)\n",
    "        def gae_scan_fn(carry, x):\n",
    "            (next_advantage,) = carry\n",
    "            (reward, value, next_value, mask) = x\n",
    "            \n",
    "            current_delta = reward + self._config.discount_rate * next_value * mask - value\n",
    "            advantage = current_delta + self._config.discount_rate * self._config.gae_lambda * mask * next_advantage\n",
    "            return (advantage,), (advantage,)\n",
    "            \n",
    "\n",
    "\n",
    "        # Transpose deltas and masks to iterate over the second dimension\n",
    "\n",
    "        # Perform the scan\n",
    "        _, (advantages,) = jax.lax.scan(\n",
    "            gae_scan_fn,\n",
    "            (jnp.array(0.),),\n",
    "            (rewards, values, next_values, masks),\n",
    "            length=rewards.shape[0],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        print(type(advantages))\n",
    "        print(type(values))\n",
    "        # Calculate returns by adding values to advantages\n",
    "        returns = advantages + values\n",
    "\n",
    "        return advantages, returns\n",
    "        \n",
    "        \n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def train_step(self, random_key, train_state_policy, train_state_value):\n",
    "        # Sample trajectories\n",
    "        random_keys = jax.random.split(random_key, self._config.no_agents+1)\n",
    "        start_time = time.time()\n",
    "        obs, action, logp, values, reward, _, mask = jax.vmap(self.sample_trajectory, in_axes=(0, None, None))(random_keys[:self._config.no_agents], train_state_policy, train_state_value)\n",
    "        values= jnp.squeeze(values, axis=-1)\n",
    "\n",
    "        \n",
    "        next_value = train_state_value.apply_fn(train_state_value.params, obs[:, -1])\n",
    "        next_value = jnp.squeeze(next_value, axis=-1)\n",
    "        \n",
    "        advantages, returns = jax.vmap(self.compute_gae_and_returns, in_axes=(0, 0, 0, 0))(reward, values, mask, next_value)\n",
    "         \n",
    "        \n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        \n",
    "        # Compute standaerdized return\n",
    "        print(f\"Reward before passing through the get_return_standardize{obs.shape}\")\n",
    "        return_standardized = self.get_return_standardize(reward, mask)\n",
    "        \n",
    "       # print(f\"Before flattening{obs.shape}\")\n",
    "        \n",
    "        obs_, action_, logp_, mask_, returns_, advantages_ = self.flatten_trajectory(obs, action, logp, mask, returns, advantages)\n",
    "        \n",
    "        #print(f\"After flattening{obs_.shape}\")\n",
    "        #b_inds = random.permutation(random_keys[-1], self._config.batch_size)\n",
    "        \n",
    "        random_keys_ = jax.random.split(random_keys[-1], self._config.no_epochs)\n",
    "        \n",
    "        def _scan_epoch_train(carry, x):\n",
    "            (train_state_policy, train_state_value) = carry\n",
    "            (random_key,) = x\n",
    "            \n",
    "            (train_state_policy, train_state_value), losses = self.epoch_train(random_key, train_state_policy, train_state_value, obs_, action_, logp_, mask_, returns_, advantages_)\n",
    "            \n",
    "            return (train_state_policy, train_state_value), losses\n",
    "        \n",
    "        #print(\"Before _scan_epoch_train\")\n",
    "        \n",
    "        #print(type(train_state))\n",
    "        (final_train_state_policy, final_train_state_value), losses = jax.lax.scan(\n",
    "            _scan_epoch_train,\n",
    "            (train_state_policy, train_state_value),\n",
    "            (random_keys_,),\n",
    "            length=self._config.no_epochs,\n",
    "            )\n",
    "        \n",
    "        #print(\"After _scan_epoch_train\")\n",
    "        #print(type(final_train_state[0]))\n",
    "        metrics = {\n",
    "            \"loss\" : losses,\n",
    "            \"reward\" : reward * mask,\n",
    "            \"mask\" : mask\n",
    "        }\n",
    "        jax.debug.print(\"Mean Loss: {}\", jnp.mean(metrics[\"loss\"]))\n",
    "        jax.debug.print(\"Mean Reward: {}\", jnp.mean(jnp.sum(metrics[\"reward\"], axis=-1)))\n",
    "        jax.debug.print(\"Mean Mask: {}\", jnp.mean(metrics[\"mask\"]))\n",
    "        jax.debug.print(\"-\" * 50)\n",
    "        \n",
    "        return (final_train_state_policy, final_train_state_value), (metrics,)\n",
    "    \n",
    "    '''\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def epoch_train(self, random_key, train_state, obs, action, logp, mask, return_standardized):\n",
    "        b_inds = random.permutation(random_key, self._config.batch_size)\n",
    "        \n",
    "        \n",
    "        def _scan_mini_train(carry, _):\n",
    "            (train_state, counter) = carry\n",
    "        \n",
    "            idx = b_inds[counter * self._config.mini_batch_size : (counter+1) * self._config.mini_batch_size]\n",
    "            loss, grad = jax.value_and_grad(self.loss_ppo)(train_state.params, obs[idx], action[idx], logp[idx], mask[idx], return_standardized[idx])\n",
    "            new_train_state = train_state.apply_gradients(grads=grad)  \n",
    "            return (new_train_state, counter+1), loss\n",
    "        \n",
    "        final_train_state, losses = jax.lax.scan(\n",
    "            _scan_mini_train,\n",
    "            (train_state, 0),\n",
    "            None,\n",
    "            length=self._config.batch_size // self._config.mini_batch_size,\n",
    "            )\n",
    "        \n",
    "        return (final_train_state,), (losses,)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def epoch_train(self, random_key, train_state_policy, train_state_value, obs, action, logp, mask, returns, advantages):\n",
    "        total_size = self._config.batch_size\n",
    "        \n",
    "        shuffled_indices = jax.random.permutation(random_key, total_size)\n",
    "        \n",
    "        num_batches = self._config.batch_size // self._config.mini_batch_size\n",
    "        batch_indices = jnp.array([shuffled_indices[i * self._config.mini_batch_size:(i + 1) * self._config.mini_batch_size] for i in range(num_batches)])\n",
    "        #jax.debug.print(\"Returns: {}\", return_standardized)\n",
    "        #jax.debug.print(\"Returns shape: {}\", return_standardized.shape)\n",
    "        def _scan_mini_train(carry, idx):\n",
    "            (train_state_policy, train_state_value, counter) = carry\n",
    "            #jax.debug.print(\"Returns_: {}\", return_standardized[idx])\n",
    "            #jax.debug.print(\"Returns_ shape: {}\", return_standardized[idx].shape)\n",
    "            #jax.debug.print(\"Obs: {}\", obs[idx].shape)\n",
    "            \n",
    "            #value = train_state_value.apply_fn(train_state_value.params, obs[idx])\n",
    "            '''\n",
    "            loss, grad = jax.value_and_grad(self.total_loss)(train_state_policy.params, obs[idx], action[idx], logp[idx], mask[idx], value, returns[idx], advantages[idx])\n",
    "            new_train_state_policy = train_state_policy.apply_gradients(grads=grad)  \n",
    "            new_train_state_value = train_state_value.apply_gradients(grads=grad)\n",
    "            \n",
    "            return (new_train_state_policy, new_train_state_value, counter+1), loss\n",
    "            '''\n",
    "            loss, (policy_grads, value_grads) = jax.value_and_grad(self.total_loss, argnums=(0, 1))(train_state_policy.params, train_state_value.params, obs[idx], action[idx], logp[idx], mask[idx], returns[idx], advantages[idx], train_state_value)\n",
    "            \n",
    "            #policy_loss, policy_grad = jax.value_and_grad(self.pg_loss)(train_state_policy.params, obs[idx], action[idx], logp[idx], mask[idx], advantages[idx])\n",
    "            #value_loss, value_grad = jax.value_and_grad(self.value_loss)(train_state_value.params, value, returns[idx])\n",
    "\n",
    "            # Apply gradients to each network separately\n",
    "            new_train_state_policy = train_state_policy.apply_gradients(grads=policy_grads)\n",
    "            new_train_state_value = train_state_value.apply_gradients(grads=value_grads)\n",
    "            \n",
    "            return (new_train_state_policy, new_train_state_value, counter + 1), loss\n",
    "        \n",
    "        #print('Before _scan_mini_train')\n",
    "        (final_train_state_policy, final_train_state_value, _), losses = jax.lax.scan(\n",
    "            _scan_mini_train,\n",
    "            (train_state_policy, train_state_value, 0),\n",
    "            batch_indices,\n",
    "            length=num_batches,\n",
    "            )\n",
    "        \n",
    "        #print('After _scan_mini_train')\n",
    "        \n",
    "        #print(final_train_state)\n",
    "        \n",
    "        return (final_train_state_policy, final_train_state_value), losses\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\", \"no_steps\", \"eval\"))\n",
    "    def train(self, random_key, train_state_policy, train_state_value, no_steps, eval=False):\n",
    "        \"\"\"Trains the policy for a number of steps.\"\"\"\n",
    "        \n",
    "        random_keys = jax.random.split(random_key, no_steps+1)\n",
    "    \n",
    "\n",
    "        def _scan_train_step(carry, x):\n",
    "            (train_state_policy, train_state_value) = carry\n",
    "            (random_key,) = x\n",
    "            \n",
    "            (train_state_policy, train_state_value), (metrics,) = self.train_step(random_key, train_state_policy, train_state_value)\n",
    "            \n",
    "            return (train_state_policy, train_state_value), (metrics,)\n",
    "        \n",
    "        #print('Before  _scan_train_step')\n",
    "        \n",
    "        (train_state_policy, train_state_value), (metrics,) = jax.lax.scan(\n",
    "            _scan_train_step,\n",
    "            (train_state_policy, train_state_value),\n",
    "            (random_keys[:no_steps],),\n",
    "            length=no_steps,\n",
    "            )\n",
    "        \n",
    "        #print('After  _scan_train_step')\n",
    "        '''\n",
    "        if eval:\n",
    "            mean_reward = self.evaluate(random_keys[-1], train_state)\n",
    "            jax.debug.print(\"Mean Reward over 20 episodes: {}\", mean_reward)\n",
    "            return mean_reward\n",
    "        '''\n",
    "        \n",
    "        return train_state_policy, train_state_value, metrics\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def evaluate(self, random_key, train_state):\n",
    "        \"\"\"Evaluates the policy in the environment.\"\"\"\n",
    "        random_keys = jax.random.split(random_key, 20)\n",
    "        \n",
    "        def _scan_evaluate(carry, _):\n",
    "            (train_state,) = carry\n",
    "            \n",
    "            obs, _, _, reward, _, mask = jax.vmap(self.sample_trajectory, in_axes=(0, None, None))(random_keys, train_state, True)\n",
    "            return (train_state,), (reward * mask,)\n",
    "        \n",
    "        (train_state,), (reward,) = jax.lax.scan(\n",
    "            _scan_evaluate,\n",
    "            (train_state,),\n",
    "            None,\n",
    "            length=20,\n",
    "            )\n",
    "        \n",
    "        return jnp.mean(jnp.sum(reward, axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_mitsides\u001b[0m (\u001b[33mmitsides\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m config_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_agents\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwalker2d_uni\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m }\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize wandb with the configuration dictionary\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmcpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPPOish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m env \u001b[38;5;241m=\u001b[39m get_env(config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     20\u001b[0m policy_hidden_layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m]\n",
      "File \u001b[0;32m/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1164\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, settings)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[1;32m   1163\u001b[0m     wi\u001b[38;5;241m.\u001b[39msetup(kwargs)\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:747\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    744\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunicating run to backend with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m second timeout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    746\u001b[0m run_init_handle \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_run(run)\n\u001b[0;32m--> 747\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_init_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_progress_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcancel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    753\u001b[0m     run_result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mrun_result\n",
      "File \u001b[0;32m/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_and_clear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Always update progress to 100% when done\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on_progress \u001b[38;5;129;01mand\u001b[39;00m progress_handle \u001b[38;5;129;01mand\u001b[39;00m progress_sent:\n",
      "File \u001b[0;32m/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_and_clear\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[pb\u001b[38;5;241m.\u001b[39mResult], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m    129\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m             found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config_dict = {\n",
    "    \"no_agents\": 256,\n",
    "    \"batch_size\": 256 * 1000,\n",
    "    \"mini_batch_size\": 128,\n",
    "    \"no_epochs\": 10,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"discount_rate\": 0.99,\n",
    "    \"clip_param\": 0.2,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"env_name\": \"walker2d_uni\",\n",
    "}\n",
    "\n",
    "# Initialize wandb with the configuration dictionary\n",
    "wandb.init(project=\"mcpg\", name='PPOish', config=config_dict)\n",
    "\n",
    "env = get_env(config_dict[\"env_name\"])\n",
    "\n",
    "\n",
    "policy_hidden_layers = [64, 64]\n",
    "value_hidden_layers = [64, 64]\n",
    "\n",
    "policy = MLP(\n",
    "    hidden_layers_size=policy_hidden_layers,\n",
    "    action_size=env.action_size,\n",
    "    activation=nn.tanh,\n",
    "    hidden_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "    mean_init=jax.nn.initializers.orthogonal(scale=0.01),\n",
    ")\n",
    "\n",
    "value_net = ValueNet(\n",
    "    hidden_layers_size=value_hidden_layers,\n",
    "    hidden_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "    value_init=jax.nn.initializers.orthogonal(scale=1.),\n",
    "    activation=nn.tanh,\n",
    ")\n",
    "\n",
    "agent = MCPG(Config(**wandb.config), policy, value_net, env)\n",
    "\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "train_state_policy, train_state_value = agent.init(random_key)\n",
    "\n",
    "num_steps = 1000\n",
    "log_period = 10\n",
    "\n",
    "metrics_wandb = dict.fromkeys([\"mean loss\", \"mean reward\", \"mask\", \"evaluation\", 'time'], jnp.array([]))\n",
    "eval_num = config_dict[\"no_agents\"]\n",
    "print(f\"Number of evaluations per training step: {eval_num}\")\n",
    "start_time = time.time()\n",
    "for i in range(num_steps // log_period):\n",
    "    random_key, subkey = jax.random.split(random_key)\n",
    "    train_state_policy, train_state_value, current_metrics = agent.train(subkey, train_state_policy, train_state_value, log_period)\n",
    "    timelapse = time.time() - start_time\n",
    "    print(f\"Step {(i+1) * log_period}, Time: {timelapse}\")\n",
    "    \n",
    "    current_metrics[\"evaluation\"] = jnp.arange(log_period*eval_num*(i+1), log_period*eval_num*(i+2), dtype=jnp.int32)\n",
    "    current_metrics[\"time\"] = jnp.repeat(timelapse, log_period)\n",
    "    current_metrics[\"mean loss\"] = jnp.repeat(jnp.mean(current_metrics[\"loss\"]), log_period)\n",
    "    current_metrics[\"mean reward\"] = jnp.repeat(jnp.mean(jnp.sum(current_metrics[\"reward\"], axis=-1)), log_period)\n",
    "    current_metrics[\"mask\"] = jnp.repeat(jnp.mean(current_metrics[\"mask\"]), log_period)\n",
    "    '''\n",
    "    metrics_wandb = jax.tree_util.tree_map(lambda metric, current_metric: jnp.concatenate([metric, current_metric], axis=0), metrics_wandb, current_metrics)\n",
    "    \n",
    "    log_metrics = jax.tree_util.tree_map(lambda metric: metric[-1], metrics_wandb)\n",
    "    \n",
    "    wandb.log(log_metrics)\n",
    "    '''\n",
    "    \n",
    "    def update_metrics(old_metrics, new_metrics):\n",
    "        updated_metrics = {}\n",
    "        for key in old_metrics:\n",
    "            if key in new_metrics:\n",
    "                # Check if old metrics for key is empty, and initialize properly if so\n",
    "                if old_metrics[key].size == 0:\n",
    "                    updated_metrics[key] = new_metrics[key]\n",
    "                else:\n",
    "                    updated_metrics[key] = jnp.concatenate([old_metrics[key], new_metrics[key]], axis=0)\n",
    "            else:\n",
    "                raise KeyError(f\"Key {key} not found in new metrics.\")\n",
    "        return updated_metrics\n",
    "\n",
    "    # In your training loop:\n",
    "    try:\n",
    "        metrics_wandb = update_metrics(metrics_wandb, current_metrics)\n",
    "        log_metrics = {k: v[-1] for k, v in metrics_wandb.items()}  # Assuming you want the latest entry\n",
    "        wandb.log(log_metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating metrics: {e}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

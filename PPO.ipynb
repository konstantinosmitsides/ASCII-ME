{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Union, Tuple, Callable, Optional\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "\n",
    "from qdax import environments, environments_v1\n",
    "from jax import random\n",
    "\n",
    "import pickle\n",
    "from optax import exponential_decay\n",
    "from IPython.display import HTML\n",
    "from brax.io import html\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_name):\n",
    "    if env_name == \"hopper_uni\":\n",
    "        episode_length = 1000\n",
    "        \n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "    elif env_name == \"halfcheetah_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "        \n",
    "    elif env_name == \"walker2d_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\t\n",
    "    elif env_name == \"ant_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=True)\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, exclude_current_positions_from_observation=True)\t\n",
    "    '''\n",
    "    elif env_name == \"ant_omni\":\n",
    "        episode_length = 250\n",
    "        max_bd = 30.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=False)\t\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "        max_bd = 1.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length)\t\n",
    "    else:\n",
    "        ValueError(f\"Environment {env_name} not supported.\")\n",
    "    '''\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MCPG MLP module\"\"\"\n",
    "    hidden_layers_size: Tuple[int, ...]\n",
    "    action_size: int\n",
    "    activation: Callable[[jnp.ndarray], jnp.ndarray] = nn.tanh\n",
    "    bias_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.zeros\n",
    "    hidden_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.lecun_uniform()\n",
    "    mean_init: Callable[[jnp.ndarray, Any], jnp.ndarray] = jax.nn.initializers.lecun_uniform()\n",
    "    \n",
    "    def setup(self):\n",
    "        self.hidden_layers = [nn.Dense(features, kernel_init=self.hidden_init, bias_init=self.bias_init) for features in self.hidden_layers_size]\n",
    "        self.mean = nn.Dense(self.action_size, kernel_init=self.mean_init, bias_init=self.bias_init)\n",
    "        self.log_std = self.param(\"log_std\", lambda _, shape: jnp.log(0.5)*jnp.ones(shape), (self.action_size,))\n",
    "        \n",
    "    def distribution_params(self, obs: jnp.ndarray):\n",
    "        hidden = obs\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            hidden = self.activation(hidden_layer(hidden))\n",
    "            \n",
    "        mean = self.mean(hidden)\n",
    "        log_std = self.log_std\n",
    "        std = jnp.exp(log_std)\n",
    "        \n",
    "        return mean, log_std, std\n",
    "    \n",
    "    def logp(self, obs: jnp.ndarray, action: jnp.ndarray):\n",
    "        mean, _, std = self.distribution_params(obs)\n",
    "        logp = jax.scipy.stats.norm.logpdf(action, mean, std)\n",
    "        return logp.sum(axis=-1)\n",
    "    \n",
    "    def __call__(self, random_key: Any, obs: jnp.ndarray):\n",
    "        mean, _, std = self.distribution_params(obs)\n",
    "        \n",
    "        # Sample action\n",
    "        rnd = jax.random.normal(random_key, shape = (self.action_size,))\n",
    "        action = jax.lax.stop_gradient(mean + rnd * std)\n",
    "        \n",
    "        logp = jnp.sum(jax.scipy.stats.norm.logpdf(action, mean, std), axis=-1) \n",
    "                \n",
    "        return action, logp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    no_agents: int = 2\n",
    "    batch_size: int = 2024\n",
    "    mini_batch_size: int = 64\n",
    "    no_epochs: int = 10\n",
    "    learning_rate: float = 3e-4\n",
    "    discount_rate: float = 0.99\n",
    "    clip_param: float = 0.2\n",
    "    grad_steps: int = 10\n",
    "    \n",
    "class MCPG:\n",
    "    \n",
    "    def __init__(self, config, policy, env):\n",
    "        self._config = config\n",
    "        self._policy = policy\n",
    "        self._env = env\n",
    "        \n",
    "    def init(self, random_key):\n",
    "        random_key_1, random_key_2 = jax.random.split(random_key)\n",
    "        fake_obs = jnp.zeros(shape=(self._env.observation_size,))\n",
    "        params = self._policy.init(random_key_1, random_key_2, fake_obs)\n",
    "        tx = optax.adam(learning_rate=self._config.learning_rate)\n",
    "        \n",
    "        return TrainState.create(apply_fn=self._policy.apply, params=params, tx=tx)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def logp_fn(self, params, obs, action):\n",
    "        return self._policy.apply(params, obs, action, method=self._policy.logp)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def sample_step(self, random_key, train_state, env_state):\n",
    "        \"\"\"Samples one step in the environment and returns the next state, action and \n",
    "        log-prob of the action.\n",
    "        \"\"\"\n",
    "        \n",
    "        action, action_logp = train_state.apply_fn(train_state.params, random_key, env_state.obs)\n",
    "        \n",
    "        next_env_state = self._env.step(env_state, action)\n",
    "        \n",
    "        return next_env_state, action, action_logp\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def sample_trajectory(self, random_key, train_state, evaluate=False):\n",
    "        \"\"\"Samples a full trajectory using the environment and policy.\"\"\"\n",
    "        if evaluate:\n",
    "            length = self._env.episode_length\n",
    "        else:\n",
    "            length = int(self._config.batch_size / self._config.no_agents)\n",
    "            \n",
    "        random_keys = jax.random.split(random_key, length+1)\n",
    "        env_state_init = self._env.reset(random_keys[-1])\n",
    "        \n",
    "        def _scan_sample_step(carry, x):\n",
    "            (train_state, env_state,) = carry\n",
    "            (random_key, ) = x\n",
    "            \n",
    "            next_env_state, action, action_logp = self.sample_step(random_key, train_state, env_state)\n",
    "            return (train_state, next_env_state), (env_state.obs, action, action_logp, next_env_state.reward, env_state.done, env_state.info[\"state_descriptor\"])\n",
    "        \n",
    "        _, (obs, action, action_logp, reward, done, state_desc) = jax.lax.scan(\n",
    "            _scan_sample_step, \n",
    "            (train_state, env_state_init), \n",
    "            (random_keys[:length],),\n",
    "            length=length,\n",
    "            )\n",
    "        \n",
    "        mask = 1. - jnp.clip(jnp.cumsum(done), a_min=0., a_max=1.)\n",
    "        \n",
    "        return obs, action, action_logp, reward, state_desc, mask\n",
    "    \n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return(self, reward):\n",
    "        \"\"\" Computes the discounted return for each step in the trajectory.\n",
    "        \"\"\"\n",
    "        \n",
    "        def _body(carry, x):\n",
    "            (next_return,) = carry\n",
    "            (reward,) = x\n",
    "            \n",
    "            current_return = reward + self._config.discount_rate * next_return\n",
    "            return (current_return,), (current_return,)\n",
    "        \n",
    "        _, (return_,) = jax.lax.scan(\n",
    "            _body,\n",
    "            (jnp.array(0.),),\n",
    "            (reward,),\n",
    "            length=int(self._config.batch_size / self._config.no_agents),\n",
    "            reverse=True,\n",
    "            )\n",
    "            \n",
    "        return return_\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def standardize(self, return_):\n",
    "        return jax.nn.standardize(return_, axis=0, variance=1, epsilon=EPS)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return_standardize(self, reward, mask):\n",
    "        \"\"\"Standardizes the return values for stability in training\n",
    "        \"\"\"\n",
    "        return_ = jax.vmap(self.get_return)(reward * mask)\n",
    "        return self.standardize(return_)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def loss_rein(self, params, obs, action, mask, return_standardized):\n",
    "        \"\"\" REINFORCE loss function.\n",
    "        \"\"\"\n",
    "        logp_ = self.logp_fn(params, jax.lax.stop_gradient(obs), jax.lax.stop_gradient(action))\n",
    "        return -jnp.mean(jnp.multiply(logp_ * mask, jax.lax.stop_gradient(return_standardized)))\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def loss_ppo(self, params, obs, action, logp, mask, return_standardized):\n",
    "        \"\"\" PPO loss function.\n",
    "        \"\"\"\n",
    "        \n",
    "        logp_ = self.logp_fn(params, jax.lax.stop_gradient(obs), jax.lax.stop_gradient(action))\n",
    "        ratio = jnp.exp(logp_ - jax.lax.stop_gradient(logp))\n",
    "        \n",
    "        pg_loss_1 = jnp.multiply(ratio * mask, jax.lax.stop_gradient(return_standardized))\n",
    "        pg_loss_2 = jax.lax.stop_gradient(return_standardized) * jax.lax.clamp(1. - self._config.clip_param, ratio, 1. + self._config.clip_param) * mask\n",
    "        return -jnp.mean(jnp.minimum(pg_loss_1, pg_loss_2))\n",
    "    \n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def flatten_trajectory(self, obs, action, logp, mask, return_standardized):\n",
    "        # Calculate the total number of elements in the combined first two dimensions\n",
    "        total_elements = obs.shape[0] * obs.shape[1]\n",
    "        \n",
    "        new_obs_shape = (total_elements,) + obs.shape[2:]  # obs.shape[2:] should be unpacked if it's a tuple\n",
    "        new_action_shape = (total_elements,) + action.shape[2:]  # Same handling as for obs\n",
    "            \n",
    "        # Flatten the first two dimensions\n",
    "        obs = jnp.reshape(obs, new_obs_shape)\n",
    "        action = jnp.reshape(action, new_action_shape)\n",
    "        logp = jnp.reshape(logp, (total_elements,))\n",
    "        mask = jnp.reshape(mask, (total_elements,))\n",
    "        return_standardized = jnp.reshape(return_standardized, (total_elements,))\n",
    "                \n",
    "        return obs, action, logp, mask, return_standardized\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def train_step(self, random_key, train_state):\n",
    "        # Sample trajectories\n",
    "        random_keys = jax.random.split(random_key, self._config.no_agents+1)\n",
    "        start_time = time.time()\n",
    "        obs, action, logp, reward, _, mask = jax.vmap(self.sample_trajectory, in_axes=(0, None))(random_keys[:self._config.no_agents], train_state)\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        \n",
    "        # Compute standaerdized return\n",
    "        #print(f\"Reward before passing through the get_return_standardize{reward.shape}\")\n",
    "        return_standardized = self.get_return_standardize(reward, mask)\n",
    "        \n",
    "       # print(f\"Before flattening{obs.shape}\")\n",
    "        \n",
    "        obs_, action_, logp_, mask_, return_standardized_ = self.flatten_trajectory(obs, action, logp, mask, return_standardized)\n",
    "        \n",
    "        #print(f\"After flattening{obs_.shape}\")\n",
    "        #b_inds = random.permutation(random_keys[-1], self._config.batch_size)\n",
    "        \n",
    "        def _scan_epoch_train(carry, _):\n",
    "            (train_state,) = carry\n",
    "            \n",
    "            (train_state,), (losses,) = self.epoch_train(random_keys[-1], train_state, obs_, action_, logp_, mask_, return_standardized_)\n",
    "            \n",
    "            return (train_state,), (losses,)\n",
    "        \n",
    "        print(\"Before _scan_epoch_train\")\n",
    "        \n",
    "        print(type(train_state))\n",
    "        final_train_state, losses = jax.lax.scan(\n",
    "            _scan_epoch_train,\n",
    "            (train_state,),\n",
    "            None,\n",
    "            length=self._config.no_epochs,\n",
    "            )\n",
    "        \n",
    "        print(\"After _scan_epoch_train\")\n",
    "        print(type(final_train_state))\n",
    "        metrics = {\n",
    "            \"loss\" : losses,\n",
    "            \"reward\" : reward * mask,\n",
    "            \"mask\" : mask\n",
    "        }\n",
    "        \n",
    "        return (final_train_state,), (metrics,)\n",
    "    \n",
    "    '''\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def epoch_train(self, random_key, train_state, obs, action, logp, mask, return_standardized):\n",
    "        b_inds = random.permutation(random_key, self._config.batch_size)\n",
    "        \n",
    "        \n",
    "        def _scan_mini_train(carry, _):\n",
    "            (train_state, counter) = carry\n",
    "        \n",
    "            idx = b_inds[counter * self._config.mini_batch_size : (counter+1) * self._config.mini_batch_size]\n",
    "            loss, grad = jax.value_and_grad(self.loss_ppo)(train_state.params, obs[idx], action[idx], logp[idx], mask[idx], return_standardized[idx])\n",
    "            new_train_state = train_state.apply_gradients(grads=grad)  \n",
    "            return (new_train_state, counter+1), loss\n",
    "        \n",
    "        final_train_state, losses = jax.lax.scan(\n",
    "            _scan_mini_train,\n",
    "            (train_state, 0),\n",
    "            None,\n",
    "            length=self._config.batch_size // self._config.mini_batch_size,\n",
    "            )\n",
    "        \n",
    "        return (final_train_state,), (losses,)\n",
    "    '''\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def epoch_train(self, random_key, train_state, obs, action, logp, mask, return_standardized):\n",
    "        total_size = self._config.batch_size\n",
    "        \n",
    "        shuffled_indices = jax.random.permutation(random_key, total_size)\n",
    "        \n",
    "        num_batches = self._config.batch_size // self._config.mini_batch_size\n",
    "        batch_indices = jnp.array([shuffled_indices[i * self._config.mini_batch_size:(i + 1) * self._config.mini_batch_size] for i in range(num_batches)])\n",
    "                \n",
    "        def _scan_mini_train(carry, idx):\n",
    "            (train_state, counter) = carry\n",
    "        \n",
    "            loss, grad = jax.value_and_grad(self.loss_ppo)(train_state.params, obs[idx], action[idx], logp[idx], mask[idx], return_standardized[idx])\n",
    "            new_train_state = train_state.apply_gradients(grads=grad)  \n",
    "            return (new_train_state, counter+1), loss\n",
    "        \n",
    "        print('Before _scan_mini_train')\n",
    "        final_train_state, losses = jax.lax.scan(\n",
    "            _scan_mini_train,\n",
    "            (train_state, 0),\n",
    "            batch_indices,\n",
    "            length=self._config.batch_size // self._config.mini_batch_size,\n",
    "            )\n",
    "        \n",
    "        print('After _scan_mini_train')\n",
    "        \n",
    "        return (final_train_state[0],), (losses,)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\", \"no_steps\"))\n",
    "    def train(self, random_key, train_state, no_steps):\n",
    "        \"\"\"Trains the policy for a number of steps.\"\"\"\n",
    "        \n",
    "        random_keys = jax.random.split(random_key, no_steps)\n",
    "    \n",
    "\n",
    "        def _scan_train_step(carry, x):\n",
    "            (train_state,) = carry\n",
    "            (random_key,) = x\n",
    "            \n",
    "            (train_state,), (metrics,) = self.train_step(random_key, train_state)\n",
    "            \n",
    "            return (train_state,), (metrics,)\n",
    "        \n",
    "        print('Before  _scan_train_step')\n",
    "        \n",
    "        (train_state,), (metrics,) = jax.lax.scan(\n",
    "            _scan_train_step,\n",
    "            (train_state,),\n",
    "            (random_keys,),\n",
    "            length=no_steps,\n",
    "            )\n",
    "        \n",
    "        print('After  _scan_train_step')\n",
    "        \n",
    "        return (train_state,), (metrics,)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def evaluate(self, random_key, train_state):\n",
    "        \"\"\"Evaluates the policy in the environment.\"\"\"\n",
    "        random_keys = jax.random.split(random_key, 20)\n",
    "        \n",
    "        def _scan_evaluate(carry, _):\n",
    "            (train_state,) = carry\n",
    "            \n",
    "            obs, _, _, reward, _, mask = jax.vmap(self.sample_trajectory, in_axes=(0, None))(random_keys, train_state, True)\n",
    "            return (train_state,), (reward * mask)\n",
    "        \n",
    "        (train_state,), (reward,) = jax.lax.scan(\n",
    "            _scan_evaluate,\n",
    "            (train_state,),\n",
    "            None,\n",
    "            length=20,\n",
    "            )\n",
    "        \n",
    "        return jnp.mean(jnp.sum(reward, axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before  _scan_train_step\n",
      "Before _scan_epoch_train\n",
      "<class 'flax.training.train_state.TrainState'>\n",
      "Before _scan_mini_train\n",
      "After _scan_mini_train\n",
      "After _scan_epoch_train\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Scanned function carry input and carry output must have the same pytree structure, but they differ:\n  * the input carry component carry[0] is a <class 'flax.training.train_state.TrainState'> but the corresponding component of the carry output is a <class 'tuple'>, so their Python types differ\n\nRevise the scanned function so that its output is a pair where the first element has the same pytree structure as the first argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m no_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     29\u001b[0m random_key, random_subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(random_key)\n\u001b[0;32m---> 30\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_steps\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[49], line 258\u001b[0m, in \u001b[0;36mMCPG.train\u001b[0;34m(self, random_key, train_state, no_steps)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (train_state,), (metrics,)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBefore  _scan_train_step\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 258\u001b[0m (train_state,), (metrics,) \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_scan_train_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAfter  _scan_train_step\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (train_state,), (metrics,)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py:310\u001b[0m, in \u001b[0;36m_check_scan_carry_type\u001b[0;34m(body_fun, in_carry, out_carry_tree, out_avals)\u001b[0m\n\u001b[1;32m    304\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     differences \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  * \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomponent(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthing1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but the corresponding component \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mof the carry output is a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthing2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, so \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexplanation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m path, thing1, thing2, explanation\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;129;01min\u001b[39;00m equality_errors(in_carry, out_carry))\n\u001b[0;32m--> 310\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    311\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScanned function carry input and carry output must have the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytree structure, but they differ:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdifferences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevise the scanned function so that its output is a pair where the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst element has the same pytree structure as the first argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    316\u001b[0m   )\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_map(core\u001b[38;5;241m.\u001b[39mtypematch, in_avals, out_avals)):\n\u001b[1;32m    318\u001b[0m   differences \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    319\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  * \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomponent(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_aval\u001b[38;5;241m.\u001b[39mstr_short()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    320\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but the corresponding output carry component has type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    321\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_aval\u001b[38;5;241m.\u001b[39mstr_short()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_aval_mismatch_extra(in_aval,\u001b[38;5;250m \u001b[39mout_aval)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    322\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m path, in_aval, out_aval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(paths, in_avals, out_avals)\n\u001b[1;32m    323\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mtypematch(in_aval, out_aval))\n",
      "\u001b[0;31mTypeError\u001b[0m: Scanned function carry input and carry output must have the same pytree structure, but they differ:\n  * the input carry component carry[0] is a <class 'flax.training.train_state.TrainState'> but the corresponding component of the carry output is a <class 'tuple'>, so their Python types differ\n\nRevise the scanned function so that its output is a pair where the first element has the same pytree structure as the first argument."
     ]
    }
   ],
   "source": [
    "env = get_env(\"ant_uni\")\n",
    "config = Config(\n",
    "    no_agents=2,\n",
    "    batch_size=2048,\n",
    "    mini_batch_size=64,\n",
    "    no_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    discount_rate=0.99,\n",
    "    clip_param=0.2,\n",
    "    grad_steps=10\n",
    ")\n",
    "\n",
    "policy_hidden_layers = [128, 128]\n",
    "\n",
    "policy = MLP(\n",
    "    hidden_layers_size=policy_hidden_layers,\n",
    "    action_size=env.action_size,\n",
    "    activation=nn.tanh,\n",
    "    hidden_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "    mean_init=jax.nn.initializers.orthogonal(scale=0.01),\n",
    ")\n",
    "\n",
    "agent = MCPG(config, policy, env)\n",
    "\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "train_state = agent.init(random_key)\n",
    "\n",
    "no_steps = 1000\n",
    "random_key, random_subkey = jax.random.split(random_key)\n",
    "agent.train(random_key, train_state, no_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

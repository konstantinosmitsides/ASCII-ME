{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  2 18:28:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A30                     Off | 00000000:01:00.0 Off |                   On |\n",
      "| N/A   31C    P0              44W / 165W |     50MiB / 24576MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                          |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                |        ECC|                       |\n",
      "|==================+================================+===========+=======================|\n",
      "|  0    1   0   0  |              25MiB / 11968MiB  | 28      0 |  2   0    2    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from math import floor \n",
    "from typing import Callable, Tuple, Any\n",
    "\n",
    "import jax\n",
    "from jax import debug\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from chex import ArrayTree\n",
    "from qdax.core.containers.repertoire import Repertoire\n",
    "from qdax.types import Descriptor, ExtraScores, Fitness, Genotype, RNGKey\n",
    "from qdax.environments.base_wrappers import QDEnv\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.buffers.trajectory_buffer import TrajectoryBuffer\n",
    "from rein_related import *\n",
    "\n",
    "from qdax.core.emitters.emitter import Emitter, EmitterState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MCPGConfig:\n",
    "    \"\"\"Configuration for the REINaive emitter.\n",
    "    \n",
    "    Args:\n",
    "        rollout_number: num of rollouts for gradient estimate\n",
    "        sample_sigma: std to sample the samples for gradient estimate  (IS THIS PARAMETER SPACE EXPLORATION?)\n",
    "        sample_mirror: if True, use mirroring sampling\n",
    "        sample_rank_norm: if True, use normalisation\n",
    "        \n",
    "        num_generations_sample: frequency of archive-sampling\n",
    "        \n",
    "        adam_optimizer: if True, use ADAM, if False, use SGD\n",
    "        learning_rate: obvious\n",
    "        l2_coefficient: coefficient for regularisation\n",
    "        \n",
    "        novelty_nearest_neighbors: num of nearest neigbors for novelty computation\n",
    "        use_novelty_archive: if True, use novelty archive for novelty (default is to use the content of the reperoire)\n",
    "        use_novelty_fifo: if True, use fifo archive for novelty (default is to use the content of the repertoire)\n",
    "        fifo_size: size of the novelty fifo bugger if used\n",
    "        \n",
    "        proprtion_explore: proportion of explore\n",
    "    \"\"\"\n",
    "    no_agents: int = 256\n",
    "    batch_size: int = 1000*256\n",
    "    mini_batch_size: int = 1000*256\n",
    "    no_epochs: int = 16\n",
    "    learning_rate: float = 3e-4\n",
    "    discount_rate: float = 0.99\n",
    "    adam_optimizer: bool = True\n",
    "    buffer_size: int = 256000\n",
    "    clip_param: float = 0.2\n",
    "    \n",
    "class MCPGEmitterState(EmitterState):\n",
    "    \"\"\"Containes the trajectory buffer.\n",
    "    \"\"\"\n",
    "    buffer: TrajectoryBuffer\n",
    "    random_key: RNGKey\n",
    "    \n",
    "class MCPGEmitter(Emitter):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: MCPGConfig,\n",
    "        policy_net: nn.Module,\n",
    "        env: QDEnv,\n",
    "    ) -> None:\n",
    "        \n",
    "        self._config = config\n",
    "        self._policy = policy_net\n",
    "        self._env = env\n",
    "        \n",
    "        self._policy_opt = optax.adam(\n",
    "            learning_rate=self._config.learning_rate\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def batch_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: the batch size emitted by the emitter.\n",
    "        \"\"\"\n",
    "        return self._config.no_agents\n",
    "    \n",
    "    @property\n",
    "    def use_all_data(self) -> bool:\n",
    "        \"\"\"Whther to use all data or not when used along other emitters.\n",
    "        \"\"\"\n",
    "        return True\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def init(\n",
    "        self,\n",
    "        random_key: RNGKey,\n",
    "        repertoire: Repertoire,\n",
    "        genotypes: Genotype,\n",
    "        fitnesses: Fitness,\n",
    "        descriptors: Descriptor,\n",
    "        extra_scores: ExtraScores,\n",
    "    ) -> Tuple[MCPGEmitterState, RNGKey]:\n",
    "        \"\"\"Initializes the emitter state.\n",
    "        \"\"\"\n",
    "        obs_size = self._env.observation_size\n",
    "        action_size = self._env.action_size\n",
    "        descriptor_size = self._env.state_descriptor_length\n",
    "        \n",
    "        # Init trajectory buffer\n",
    "        dummy_transition = QDTransition.init_dummy(\n",
    "            observation_dim=obs_size,\n",
    "            action_dim=action_size,\n",
    "            descriptor_dim=descriptor_size,\n",
    "        )\n",
    "        \n",
    "        buffer = TrajectoryBuffer.init(\n",
    "            buffer_size=self._config.buffer_size,\n",
    "            transition=dummy_transition,\n",
    "            env_batch_size=self._config.no_agents*2,\n",
    "            episode_length=self._env.episode_length,\n",
    "        )\n",
    "        \n",
    "        random_key, subkey = jax.random.split(random_key)\n",
    "        emitter_state = MCPGEmitterState(\n",
    "            buffer=buffer,\n",
    "            random_key=subkey,\n",
    "        )\n",
    "        \n",
    "        return emitter_state, random_key\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def emit(\n",
    "        self,\n",
    "        repertoire: Repertoire,\n",
    "        emitter_state: MCPGEmitterState,\n",
    "        random_key: RNGKey,\n",
    "    ) -> Tuple[Genotype, RNGKey]:\n",
    "        \"\"\"Do a step of MCPG emission.\n",
    "        \"\"\"\n",
    "        \n",
    "        no_agents = self._config.no_agents\n",
    "        \n",
    "        # sample parents\n",
    "        parents, random_key = repertoire.sample(\n",
    "            random_key=random_key,\n",
    "            num_samples=no_agents,\n",
    "        )\n",
    "        \n",
    "        offsprings_mcpg = self.emit_mcpg(emitter_state, parents)\n",
    "        \n",
    "        return offsprings_mcpg, {}, random_key\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def emit_mcpg(\n",
    "        self,\n",
    "        emitter_state: MCPGEmitterState,\n",
    "        parents: Genotype,\n",
    "    ) -> Genotype:\n",
    "        \"\"\"Emit the offsprings generated through MCPG mutation.\n",
    "        \"\"\"\n",
    "        \n",
    "        mutation_fn = partial(\n",
    "            self._mutation_function_mcpg,\n",
    "            emitter_state=emitter_state,\n",
    "        )\n",
    "        \n",
    "        offsprings = jax.vmap(mutation_fn)(parents)\n",
    "        \n",
    "        return offsprings\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def state_update(\n",
    "        self,\n",
    "        emitter_state: MCPGEmitterState,\n",
    "        repertoire: Optional[Repertoire],\n",
    "        genotypes: Optional[Genotype],\n",
    "        fitnesses: Optional[Fitness],\n",
    "        descriptors: Optional[Descriptor],\n",
    "        extra_scores: ExtraScores,\n",
    "    ) -> MCPGEmitterState:\n",
    "        \"\"\"Update the emitter state.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert \"transitions\" in extra_scores.keys(), \"Missing transtitions or wrong key\"\n",
    "        transitions = extra_scores[\"transitions\"]\n",
    "        \n",
    "        # update the buffer\n",
    "        replay_buffer = emitter_state.buffer.insert(transitions)\n",
    "        emitter_state = emitter_state.replace(buffer=replay_buffer)\n",
    "        \n",
    "        return emitter_state\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def compute_mask(\n",
    "        self,\n",
    "        done,\n",
    "    ):\n",
    "        return 1. - jnp.clip(jnp.cumsum(done), a_min=0., a_max=1.)\n",
    "    '''\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def compute_logps(\n",
    "        self,\n",
    "        policy_params,\n",
    "        obs,\n",
    "        actions,\n",
    "    ):\n",
    "        \"\"\"Compute the log probabilities of the actions.\n",
    "        \"\"\"\n",
    "        compute_logp = partial(\n",
    "            self._policy.apply,\n",
    "            params=policy_params,\n",
    "            method=self._policy.logp,\n",
    "        )\n",
    "        \n",
    "        return jax.vmap(compute_logp)(obs, actions)\n",
    "    '''\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def compute_logps(self, policy_params, obs, actions):\n",
    "        def compute_logp(single_obs, single_action):\n",
    "            # Correctly handle operations on single_obs and single_action\n",
    "            # Ensure no inappropriate method calls like .items() are made\n",
    "            return self._policy.apply(policy_params, single_obs, single_action, method=self._policy.logp)\n",
    "\n",
    "        # Use jax.vmap to apply compute_logp across batches of obs and actions\n",
    "        return jax.vmap(compute_logp, in_axes=(0, 0))(obs, actions)\n",
    "       \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return(\n",
    "        self,\n",
    "        rewards,\n",
    "    ):\n",
    "        def _body(carry, x):\n",
    "            (next_return,) = carry\n",
    "            (rewards,) = x\n",
    "\n",
    "            current_return = rewards + self._config.discount_rate * next_return\n",
    "            return (current_return,), (current_return,)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #jax.debug.print(\"rewards\", rewards.shape)\n",
    "        \n",
    "        _, (return_,) = jax.lax.scan(\n",
    "            _body,\n",
    "            (jnp.array(0.),),\n",
    "            (rewards,),\n",
    "            length=self._env.episode_length,\n",
    "            reverse=True,\n",
    "        )\n",
    "        \n",
    "        return return_\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return(self, rewards):\n",
    "        def _body(carry, reward):\n",
    "            next_return = carry  # carry should be unpacked directly if it's a single element\n",
    "            current_return = reward + self._config.discount_rate * next_return\n",
    "            return current_return, current_return  # Maintain the same shape and type\n",
    "\n",
    "        initial_return = jnp.array(0.0)  # Ensure initial_return is correctly shaped as a scalar\n",
    "        _, return_ = jax.lax.scan(\n",
    "            _body,\n",
    "            initial_return,\n",
    "            rewards,  # Pass rewards directly without extra tuple wrapping\n",
    "            length=int(self._env.episode_length),\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        return return_\n",
    "    '''\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def standardize(\n",
    "        self,\n",
    "        return_,\n",
    "    ):\n",
    "        return jax.nn.standardize(return_, axis=0, variance=1, epsilon=EPS)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_standardized_return(\n",
    "        self,\n",
    "        rewards,\n",
    "        mask,\n",
    "    ):\n",
    "        mask = jnp.expand_dims(mask, axis=-1)\n",
    "        valid_rewards = (rewards * mask).squeeze(axis=-1)\n",
    "        #jax.debug.print(\"mask: {}\", mask.shape)\n",
    "        #jax.debug.print(\"rewards*mask: {}\", (rewards * mask).shape)\n",
    "        return_ = jax.vmap(self.get_return)(valid_rewards)\n",
    "        return self.standardize(return_)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _mutation_function_mcpg(\n",
    "        self,\n",
    "        policy_params,\n",
    "        emitter_state: MCPGEmitterState,\n",
    "    ) -> Genotype:\n",
    "        \"\"\"Mutation function for MCPG.\n",
    "        \"\"\"\n",
    "        \n",
    "        buffer = emitter_state.buffer\n",
    "        \n",
    "        policy_opt_state = self._policy_opt.init(policy_params)\n",
    "        \n",
    "        random_key = emitter_state.random_key\n",
    "        \n",
    "        #random_key, subkey = jax.random.split(emitter_state.random_key)\n",
    "        sample_size = int(self._config.batch_size) // int(self._env.episode_length)\n",
    "        #print(f\"episodic_data_size: {int(buffer.current_episodic_data_size)}\")\n",
    "        #episodic_data_size = buffer.current_episodic_data_size.item()\n",
    "        \n",
    "        trans, random_key = buffer.sample(\n",
    "            random_key=random_key,\n",
    "            sample_size=sample_size,\n",
    "            episodic_data_size=64,\n",
    "            sample_traj=True,\n",
    "        )\n",
    "        new_emitter_state = emitter_state.replace(random_key=random_key)\n",
    "        # trans has shape (episde_length*sample_size, transition_dim)\n",
    "        \n",
    "        obs = trans.obs.reshape(sample_size, self._env.episode_length, -1)\n",
    "        actions = trans.actions.reshape(sample_size, self._env.episode_length, -1)\n",
    "        rewards = trans.rewards.reshape(sample_size, self._env.episode_length, -1)\n",
    "        #jax.debug.print(\"rewards shape: {}\", rewards.shape)\n",
    "        #print(f\"rewards shape: {rewards.shape}\")\n",
    "        dones = trans.dones.reshape(sample_size, self._env.episode_length, -1)\n",
    "        \n",
    "        mask = jax.vmap(self.compute_mask, in_axes=0)(dones)\n",
    "        logps = jax.vmap(self.compute_logps, in_axes=(None, 0, 0))(policy_params, obs, actions)\n",
    "        \n",
    "        standardized_returns = self.get_standardized_return(rewards, mask)\n",
    "        \n",
    "        def scan_train_policy(\n",
    "            carry: Tuple[MCPGEmitterState, Genotype, optax.OptState],\n",
    "            unused: Any,\n",
    "        ) -> Tuple[Tuple[MCPGEmitterState, Genotype, optax.OptState], Any]:\n",
    "            \n",
    "            policy_params, policy_opt_state = carry\n",
    "            \n",
    "            (\n",
    "                new_policy_params,\n",
    "                new_policy_opt_state,\n",
    "            ) = self._train_policy_(\n",
    "                policy_params,\n",
    "                policy_opt_state,\n",
    "                obs,\n",
    "                actions,\n",
    "                standardized_returns,\n",
    "                mask,\n",
    "                logps,\n",
    "            )\n",
    "            \n",
    "            return (\n",
    "                new_policy_params,\n",
    "                new_policy_opt_state,\n",
    "            ), None\n",
    "\n",
    "        (policy_params, policy_opt_state), _ = jax.lax.scan(\n",
    "            scan_train_policy,\n",
    "            (policy_params, policy_opt_state),\n",
    "            None,\n",
    "            length=self._config.no_epochs,\n",
    "        )\n",
    "        \n",
    "        return policy_params\n",
    "    \n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _train_policy_(\n",
    "        self,\n",
    "        policy_params,\n",
    "        policy_opt_state,\n",
    "        obs,\n",
    "        actions,\n",
    "        standardized_returns,\n",
    "        mask,\n",
    "        logps,\n",
    "    ):\n",
    "        \"\"\"Train the policy.\n",
    "        \"\"\"\n",
    "        \n",
    "        def _scan_update(carry, _):\n",
    "            policy_params, policy_opt_state = carry\n",
    "            grads = jax.grad(self.loss_ppo)(policy_params, obs, actions, logps, mask, standardized_returns)\n",
    "            updates, new_policy_opt_state = self._policy_opt.update(grads, policy_opt_state)\n",
    "            new_policy_params = optax.apply_updates(policy_params, updates)\n",
    "            return (new_policy_params, new_policy_opt_state), None\n",
    "        \n",
    "        (final_policy_params, final_policy_opt_state), _ = jax.lax.scan(\n",
    "            _scan_update,\n",
    "            (policy_params, policy_opt_state),\n",
    "            None,\n",
    "            length=1,\n",
    "        )\n",
    "\n",
    "        return final_policy_params, final_policy_opt_state\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def loss_ppo(\n",
    "        self,\n",
    "        params,\n",
    "        obs,\n",
    "        actions,\n",
    "        logps,\n",
    "        mask,\n",
    "        standardized_returns,\n",
    "    ):\n",
    "        \n",
    "        logps_ = self._policy.apply(\n",
    "            params,\n",
    "            jax.lax.stop_gradient(obs),\n",
    "            jax.lax.stop_gradient(actions),\n",
    "            method=self._policy.logp,\n",
    "        )\n",
    "        ratio = jnp.exp(logps_ - jax.lax.stop_gradient(logps))\n",
    "        \n",
    "        pg_loss_1 = jnp.multiply(ratio * mask, jax.lax.stop_gradient(standardized_returns))\n",
    "        pg_loss_2 = jax.lax.stop_gradient(standardized_returns) * jax.lax.clamp(1. - self._config.clip_param, ratio, 1. + self._config.clip_param) * mask\n",
    "        \n",
    "        return -jnp.mean(jnp.minimum(pg_loss_1, pg_loss_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MPLCONFIGDIR'] = '/tmp/matplotlib'\n",
    "os.environ['WANDB_CACHE_DIR'] = '/tmp/wandb_cache'\n",
    "os.environ['JAX_LOG_COMPILATION'] = '1'\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from math import floor\n",
    "from typing import Any, Dict, Tuple, List, Callable\n",
    "import pickle\n",
    "from flax import serialization\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "import hydra\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from hydra.core.config_store import ConfigStore\n",
    "from qdax.core.map_elites import MAPElites\n",
    "from qdax.types import RNGKey, Genotype\n",
    "from qdax.utils.sampling import sampling \n",
    "from qdax.core.containers.mapelites_repertoire import compute_cvt_centroids, MapElitesRepertoire\n",
    "from qdax.core.neuroevolution.networks.networks import MLPMCPG\n",
    "from qdax.core.emitters.me_mcpg_emitter import MEMCPGConfig, MEMCPGEmitter\n",
    "#from qdax.core.emitters.rein_emitter_advanced import REINaiveConfig, REINaiveEmitter\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.environments import behavior_descriptor_extractor\n",
    "from qdax.tasks.brax_envs import reset_based_scoring_function_brax_envs as scoring_function\n",
    "from utils import Config, get_env\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "import wandb\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "from qdax.utils.plotting import plot_map_elites_results, plot_2d_map_elites_repertoire\n",
    "import matplotlib.pyplot as plt\n",
    "from set_up_brax import get_reward_offset_brax\n",
    "from qdax import environments_v1, environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_name):\n",
    "    if env_name == \"hopper_uni\":\n",
    "        episode_length = 1000\n",
    "        \n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "    elif env_name == \"halfcheetah_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "        \n",
    "    elif env_name == \"walker2d_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\t\n",
    "    elif env_name == \"ant_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=True)\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, exclude_current_positions_from_observation=True)\t\n",
    "    '''\n",
    "    elif env_name == \"ant_omni\":\n",
    "        episode_length = 250\n",
    "        max_bd = 30.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=False)\t\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "        max_bd = 1.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length)\t\n",
    "    else:\n",
    "        ValueError(f\"Environment {env_name} not supported.\")\n",
    "    '''\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration from this experiment script\n",
    "    \"\"\"\n",
    "    # Env config\n",
    "    #alg_name: str\n",
    "    seed: int\n",
    "    env_name: str\n",
    "    episode_length: int\n",
    "    policy_hidden_layer_sizes: Tuple[int, ...]   \n",
    "    # ME config\n",
    "    num_evaluations: int\n",
    "    num_iterations: int\n",
    "    no_agents: int\n",
    "    num_samples: int\n",
    "    fixed_init_state: bool\n",
    "    discard_dead: bool\n",
    "    # Emitter config\n",
    "    iso_sigma: float\n",
    "    line_sigma: float\n",
    "    #crossover_percentage: float\n",
    "    # Grid config \n",
    "    grid_shape: Tuple[int, ...]\n",
    "    num_init_cvt_samples: int\n",
    "    num_centroids: int\n",
    "    # Log config\n",
    "    log_period: int\n",
    "    store_repertoire: bool\n",
    "    store_repertoire_log_period: int\n",
    "    \n",
    "    # REINFORCE Parameters\n",
    "    proportion_mutation_ga : float\n",
    "    batch_size: int\n",
    "    mini_batch_size: int\n",
    "    adam_optimizer: bool\n",
    "    learning_rate: float\n",
    "    discount_rate: float\n",
    "    buffer_size: int\n",
    "    clip_param: float\n",
    "    no_epochs: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    seed=0,\n",
    "    env_name='ant_uni',\n",
    "    episode_length=1000,\n",
    "    policy_hidden_layer_sizes=[128, 128],\n",
    "    num_evaluations=0,\n",
    "    num_iterations=4000,\n",
    "    num_samples=16,\n",
    "    no_agents=64,\n",
    "    fixed_init_state=False,\n",
    "    discard_dead=False,\n",
    "    grid_shape=[50, 50],\n",
    "    num_init_cvt_samples=50000,\n",
    "    num_centroids=1296,\n",
    "    log_period=400,\n",
    "    store_repertoire=True,\n",
    "    store_repertoire_log_period=800,\n",
    "    iso_sigma=0.005,\n",
    "    line_sigma=0.05,\n",
    "    proportion_mutation_ga=0.5,\n",
    "    batch_size=64000,\n",
    "    mini_batch_size=64000,\n",
    "    no_epochs=16,\n",
    "    buffer_size=64000,\n",
    "    adam_optimizer=True,\n",
    "    learning_rate=3e-4,\n",
    "    discount_rate=0.99,\n",
    "    clip_param=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 128]\n",
      "Number of parameters in policy_network:  21264\n",
      "64000\n"
     ]
    }
   ],
   "source": [
    "# Init a random key\n",
    "random_key = jax.random.PRNGKey(config.seed)\n",
    "\n",
    "# Init environment\n",
    "env = get_env('ant_uni')\n",
    "reset_fn = jax.jit(env.reset)\n",
    "\n",
    "# Compute the centroids\n",
    "centroids, random_key = compute_cvt_centroids(\n",
    "    num_descriptors=env.behavior_descriptor_length,\n",
    "    num_init_cvt_samples=config.num_init_cvt_samples,\n",
    "    num_centroids=config.num_centroids,\n",
    "    minval=0,\n",
    "    maxval=1,\n",
    "    random_key=random_key,\n",
    ")\n",
    "# Init policy network\n",
    "policy_layer_sizes = config.policy_hidden_layer_sizes #+ (env.action_size,)\n",
    "print(policy_layer_sizes)\n",
    "\n",
    "'''\n",
    "policy_network = MLPRein(\n",
    "    action_size=env.action_size,\n",
    "    layer_sizes=policy_layer_sizes,\n",
    "    kernel_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "    kernel_init_final=jax.nn.initializers.orthogonal(scale=0.01),\n",
    ")\n",
    "'''\n",
    "policy_network = MLPMCPG(\n",
    "    hidden_layers_size=policy_layer_sizes,\n",
    "    action_size=env.action_size,\n",
    "    activation=jax.nn.tanh,\n",
    "    hidden_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "    mean_init=jax.nn.initializers.orthogonal(scale=0.01),\n",
    ")\n",
    "\n",
    "# Init population of controllers\n",
    "\n",
    "# maybe consider adding two random keys for each policy\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, num=config.no_agents)\n",
    "#split_keys = jax.vmap(lambda k: jax.random.split(k, 2))(keys)\n",
    "#keys1, keys2 = split_keys[:, 0], split_keys[:, 1]\n",
    "fake_batch_obs = jnp.zeros(shape=(config.no_agents, env.observation_size))\n",
    "init_params = jax.vmap(policy_network.init)(keys, fake_batch_obs)\n",
    "\n",
    "param_count = sum(x[0].size for x in jax.tree_util.tree_leaves(init_params))\n",
    "print(\"Number of parameters in policy_network: \", param_count)\n",
    "\n",
    "# Define the fonction to play a step with the policy in the environment\n",
    "def play_step_fn(env_state, policy_params, random_key):\n",
    "    #random_key, subkey = jax.random.split(random_key)\n",
    "    actions = policy_network.apply(policy_params, env_state.obs)\n",
    "    state_desc = env_state.info[\"state_descriptor\"]\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = QDTransition(\n",
    "        obs=env_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        actions=actions,\n",
    "        state_desc=state_desc,\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "        #desc=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "        #desc_prime=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "    )\n",
    "\n",
    "    return next_state, policy_params, random_key, transition\n",
    "\n",
    "# Prepare the scoring function\n",
    "bd_extraction_fn = behavior_descriptor_extractor['ant_uni']\n",
    "scoring_fn = partial(\n",
    "    scoring_function,\n",
    "    episode_length=env.episode_length,\n",
    "    play_reset_fn=reset_fn,\n",
    "    play_step_fn=play_step_fn,\n",
    "    behavior_descriptor_extractor=bd_extraction_fn,\n",
    ")\n",
    "#reward_offset = get_reward_offset_brax(env, config.env_name)\n",
    "#print(f\"Reward offset: {reward_offset}\")\n",
    "\n",
    "me_scoring_fn = partial(\n",
    "sampling,\n",
    "scoring_fn=scoring_fn,\n",
    "num_samples=config.num_samples,\n",
    ")\n",
    "\n",
    "reward_offset = 0\n",
    "\n",
    "metrics_function = partial(\n",
    "    default_qd_metrics,\n",
    "    qd_offset=reward_offset * env.episode_length,\n",
    ")\n",
    "\n",
    "# Define the PG-emitter config\n",
    "\n",
    "me_mcpg_config = MEMCPGConfig(\n",
    "    proportion_mutation_ga=config.proportion_mutation_ga,\n",
    "    no_agents=config.no_agents,\n",
    "    batch_size=config.batch_size,\n",
    "    mini_batch_size=config.mini_batch_size,\n",
    "    no_epochs=config.no_epochs,\n",
    "    buffer_size=config.buffer_size,\n",
    "    learning_rate=config.learning_rate,\n",
    "    adam_optimizer=config.adam_optimizer,\n",
    "    clip_param=config.clip_param,\n",
    ")\n",
    "\n",
    "variation_fn = partial(\n",
    "    isoline_variation, iso_sigma=config.iso_sigma, line_sigma=config.line_sigma\n",
    ")\n",
    "\n",
    "me_mcpg_emitter = MEMCPGEmitter(\n",
    "    config=me_mcpg_config,\n",
    "    policy_network=policy_network,\n",
    "    env=env,\n",
    "    variation_fn=variation_fn,\n",
    "    )\n",
    "\n",
    "'''\n",
    "rein_emitter = REINaiveEmitter(\n",
    "    config=rein_emitter_config,\n",
    "    policy_network=policy_network,\n",
    "    env=env,\n",
    "    )\n",
    "'''\n",
    "'''\n",
    "me_scoring_fn = partial(\n",
    "    sampling,\n",
    "    scoring_fn=scoring_fn,\n",
    "    num_samples=config.num_samples,\n",
    ")\n",
    "'''\n",
    "\n",
    "# Instantiate MAP Elites\n",
    "map_elites = MAPElites(\n",
    "    scoring_function=scoring_fn,\n",
    "    emitter=me_mcpg_emitter,\n",
    "    metrics_function=metrics_function,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitnesses, descriptors, extra_scores, random_key = scoring_fn(\n",
    "    init_params, random_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3055538/3082900524.py:1: UserWarning: This type of repertoire does not store the extra scores computed by the scoring function\n",
      "  repertoire = MapElitesRepertoire.init(\n"
     ]
    }
   ],
   "source": [
    "repertoire = MapElitesRepertoire.init(\n",
    "    genotypes=init_params,\n",
    "    fitnesses=fitnesses,\n",
    "    descriptors=descriptors,\n",
    "    centroids=centroids,\n",
    "    extra_scores=extra_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emitter_state, random_key = me_mcpg_emitter.init(\n",
    "    random_key=random_key,\n",
    "    repertoire=repertoire,\n",
    "    genotypes=init_params,\n",
    "    fitnesses=fitnesses,\n",
    "    descriptors=descriptors,\n",
    "    extra_scores=extra_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened transitions shape pre: (64, 20, 75)\n",
      "Flattened transitions shape post: (20, 64, 75)\n"
     ]
    }
   ],
   "source": [
    "emitter_state = me_mcpg_emitter.state_update(\n",
    "    emitter_state=emitter_state,\n",
    "    repertoire=repertoire,\n",
    "    genotypes=init_params,\n",
    "    fitnesses=fitnesses,\n",
    "    descriptors=descriptors,\n",
    "    extra_scores={**extra_scores}#, **extra_info},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDTransition(obs=Traced<ShapedArray(float32[64,20,28])>with<DynamicJaxprTrace(level=1/0)>, next_obs=Traced<ShapedArray(float32[64,20,28])>with<DynamicJaxprTrace(level=1/0)>, rewards=Traced<ShapedArray(float32[64,20])>with<DynamicJaxprTrace(level=1/0)>, dones=Traced<ShapedArray(float32[64,20], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>, truncations=Traced<ShapedArray(float32[64,20], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>, actions=Traced<ShapedArray(float32[64,20,8])>with<DynamicJaxprTrace(level=1/0)>, state_desc=Traced<ShapedArray(float32[64,20,4])>with<DynamicJaxprTrace(level=1/0)>, next_state_desc=Traced<ShapedArray(float32[64,20,4])>with<DynamicJaxprTrace(level=1/0)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/qdax/core/map_elites.py:82: UserWarning: This type of repertoire does not store the extra scores computed by the scoring function\n",
      "  repertoire = MapElitesRepertoire.init(\n"
     ]
    }
   ],
   "source": [
    "repertoire, emitter_state, random_key = map_elites.init(init_params, centroids, random_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = emitter_state.emitter_states[0].buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1280, dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer.current_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1280, dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer.current_position"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 29 15:37:42 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off | 00000000:2D:00.0  On |                  N/A |\n",
      "|  0%   41C    P5              13W / 320W |     86MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    705392      G   /usr/lib/xorg/Xorg                           78MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MPLCONFIGDIR'] = '/tmp/matplotlib'\n",
    "os.environ['WANDB_CACHE_DIR'] = '/tmp/wandb_cache'\n",
    "os.environ['JAX_LOG_COMPILATION'] = '1'\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from math import floor\n",
    "from typing import Any, Dict, Tuple, List, Callable\n",
    "import pickle\n",
    "from flax import serialization\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "import hydra\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from hydra.core.config_store import ConfigStore\n",
    "from qdax.core.map_elites import MAPElites\n",
    "from qdax.types import RNGKey, Genotype\n",
    "from qdax.utils.sampling import sampling \n",
    "from qdax.core.containers.mapelites_repertoire import compute_cvt_centroids, MapElitesRepertoire\n",
    "from qdax.core.neuroevolution.networks.networks import MLP, MLPRein\n",
    "from qdax.core.emitters.rein_var import REINConfig, REINEmitter\n",
    "#from qdax.core.emitters.rein_emitter_advanced import REINaiveConfig, REINaiveEmitter\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.environments import behavior_descriptor_extractor\n",
    "from qdax.tasks.brax_envs import reset_based_scoring_function_brax_envs as scoring_function\n",
    "from utils import Config, get_env\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "import wandb\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "from qdax.utils.plotting import plot_map_elites_results, plot_2d_map_elites_repertoire\n",
    "import matplotlib.pyplot as plt\n",
    "from set_up_brax import get_reward_offset_brax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration from this experiment script\n",
    "    \"\"\"\n",
    "    # Env config\n",
    "    alg_name: str\n",
    "    seed: int\n",
    "    env_name: str\n",
    "    episode_length: int\n",
    "    policy_hidden_layer_sizes: Tuple[int, ...]   \n",
    "    # ME config\n",
    "    num_evaluations: int\n",
    "    num_iterations: int\n",
    "    batch_size: int\n",
    "    num_samples: int\n",
    "    fixed_init_state: bool\n",
    "    discard_dead: bool\n",
    "    # Emitter config\n",
    "    is_sigma: float\n",
    "    line_sigma: float\n",
    "    crossover_percentage: float\n",
    "    # Grid config \n",
    "    grid_shape: Tuple[int, ...]\n",
    "    num_init_cvt_samples: int\n",
    "    num_centroids: int\n",
    "    # Log config\n",
    "    log_period: int\n",
    "    store_repertoire: bool\n",
    "    store_reperoire_log_period: int\n",
    "    \n",
    "    # REINFORCE Parameters\n",
    "    sample_number: int\n",
    "    num_in_optimizer_steps: int\n",
    "    adam_optimizer: bool\n",
    "    learning_rate: float\n",
    "    l2_coefficient: float\n",
    "    scan_batch_size: int\n",
    "    \n",
    "\n",
    "\n",
    "@hydra.main(version_base=\"1.2\", config_path=\"configs\", config_name=\"rein-me\")\n",
    "def main(config: Config) -> None:\n",
    "    # Init a random key\n",
    "    random_key = jax.random.PRNGKey(config.seed)\n",
    "\n",
    "    # Init environment\n",
    "    env = get_env(config)\n",
    "    reset_fn = jax.jit(env.reset)\n",
    "\n",
    "    # Compute the centroids\n",
    "    centroids, random_key = compute_cvt_centroids(\n",
    "        num_descriptors=env.behavior_descriptor_length,\n",
    "        num_init_cvt_samples=config.num_init_cvt_samples,\n",
    "        num_centroids=config.num_centroids,\n",
    "        minval=config.env.min_bd,\n",
    "        maxval=config.env.max_bd,\n",
    "        random_key=random_key,\n",
    "    )\n",
    "    # Init policy network\n",
    "    policy_layer_sizes = config.policy_hidden_layer_sizes #+ (env.action_size,)\n",
    "    print(policy_layer_sizes)\n",
    "    \n",
    "    '''\n",
    "    policy_network = MLPRein(\n",
    "        action_size=env.action_size,\n",
    "        layer_sizes=policy_layer_sizes,\n",
    "        kernel_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "        kernel_init_final=jax.nn.initializers.orthogonal(scale=0.01),\n",
    "    )\n",
    "    '''\n",
    "    policy_network = MLPRein(\n",
    "        action_size=env.action_size,\n",
    "        layer_sizes=policy_layer_sizes,\n",
    "        kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "        kernel_init_final=jax.nn.initializers.lecun_uniform(),\n",
    "    )\n",
    "\n",
    "\n",
    "    # Init population of controllers\n",
    "    \n",
    "    # maybe consider adding two random keys for each policy\n",
    "    random_key, subkey = jax.random.split(random_key)\n",
    "    keys = jax.random.split(subkey, num=config.batch_size)\n",
    "    #split_keys = jax.vmap(lambda k: jax.random.split(k, 2))(keys)\n",
    "    #keys1, keys2 = split_keys[:, 0], split_keys[:, 1]\n",
    "    fake_batch_obs = jnp.zeros(shape=(config.batch_size, env.observation_size))\n",
    "    init_params = jax.vmap(policy_network.init)(keys, fake_batch_obs)\n",
    "\n",
    "    param_count = sum(x[0].size for x in jax.tree_util.tree_leaves(init_params))\n",
    "    print(\"Number of parameters in policy_network: \", param_count)\n",
    "\n",
    "    # Define the fonction to play a step with the policy in the environment\n",
    "    def play_step_fn(env_state, policy_params, random_key):\n",
    "        #random_key, subkey = jax.random.split(random_key)\n",
    "        actions = policy_network.apply(policy_params, env_state.obs)\n",
    "        state_desc = env_state.info[\"state_descriptor\"]\n",
    "        next_state = env.step(env_state, actions)\n",
    "\n",
    "        transition = QDTransition(\n",
    "            obs=env_state.obs,\n",
    "            next_obs=next_state.obs,\n",
    "            rewards=next_state.reward,\n",
    "            dones=next_state.done,\n",
    "            truncations=next_state.info[\"truncation\"],\n",
    "            actions=actions,\n",
    "            state_desc=state_desc,\n",
    "            next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "            #desc=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "            #desc_prime=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "        )\n",
    "\n",
    "        return next_state, policy_params, random_key, transition\n",
    "\n",
    "    # Prepare the scoring function\n",
    "    bd_extraction_fn = behavior_descriptor_extractor[config.env.name]\n",
    "    scoring_fn = partial(\n",
    "        scoring_function,\n",
    "        episode_length=config.env.episode_length,\n",
    "        play_reset_fn=reset_fn,\n",
    "        play_step_fn=play_step_fn,\n",
    "        behavior_descriptor_extractor=bd_extraction_fn,\n",
    "    )\n",
    "    #reward_offset = get_reward_offset_brax(env, config.env_name)\n",
    "    #print(f\"Reward offset: {reward_offset}\")\n",
    "    \n",
    "    me_scoring_fn = partial(\n",
    "    sampling,\n",
    "    scoring_fn=scoring_fn,\n",
    "    num_samples=config.num_samples,\n",
    ")\n",
    "\n",
    "    \n",
    "    \n",
    "    reward_offset = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    # Get minimum reward value to make sure qd_score are positive\n",
    "    \n",
    "\n",
    "    # Define a metrics function\n",
    "    metrics_function = partial(\n",
    "        default_qd_metrics,\n",
    "        qd_offset=reward_offset * config.env.episode_length,\n",
    "    )\n",
    "\n",
    "    # Define the PG-emitter config\n",
    "    \n",
    "    rein_emitter_config = REINConfig(\n",
    "        proportion_mutation_ga=config.proportion_mutation_ga,\n",
    "        batch_size=config.batch_size,\n",
    "        num_rein_training_steps=config.num_rein_training_steps,\n",
    "        buffer_size=config.buffer_size,\n",
    "        rollout_number=config.rollout_number,\n",
    "        discount_rate=config.discount_rate,\n",
    "        adam_optimizer=config.adam_optimizer,\n",
    "        learning_rate=config.learning_rate,\n",
    "    )\n",
    "    \n",
    "\n",
    "    variation_fn = partial(\n",
    "        isoline_variation, iso_sigma=config.iso_sigma, line_sigma=config.line_sigma\n",
    "    )\n",
    "    \n",
    "    rein_emitter = REINEmitter(\n",
    "        config=rein_emitter_config,\n",
    "        policy_network=policy_network,\n",
    "        env=env,\n",
    "        variation_fn=variation_fn,\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "    # Instantiate MAP Elites\n",
    "    map_elites = MAPElites(\n",
    "        scoring_function=scoring_fn,\n",
    "        emitter=rein_emitter,\n",
    "        metrics_function=metrics_function,\n",
    "    )\n",
    "\n",
    "    # compute initial repertoire\n",
    "    repertoire, emitter_state, random_key = map_elites.init(init_params, centroids, random_key)\n",
    "\n",
    "    log_period = 1\n",
    "    num_loops = int(config.num_iterations / log_period)\n",
    "\n",
    "\n",
    "    # Main loop\n",
    "    map_elites_scan_update = map_elites.scan_update\n",
    "    eval_num = int(config.proportion_mutation_ga * (config.batch_size * config.rollout_number * config.num_rein_training_steps)) + config.batch_size\n",
    "    print(f\"Number of evaluations per iteration: {eval_num}\")\n",
    "    for i in range(num_loops):\n",
    "        print(f\"Loop {i+1}/{num_loops}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        (repertoire, emitter_state, random_key,), current_metrics = jax.lax.scan(\n",
    "            map_elites_scan_update,\n",
    "            (repertoire, emitter_state, random_key),\n",
    "            (),\n",
    "            length=log_period,\n",
    "        )\n",
    "        timelapse = time.time() - start_time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

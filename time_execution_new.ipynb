{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul  5 20:43:50 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off | 00000000:2D:00.0 Off |                  N/A |\n",
      "|  0%   40C    P0              53W / 320W |     89MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1852      G   /usr/lib/xorg/Xorg                           81MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from math import floor \n",
    "from typing import Callable, Tuple, Any\n",
    "\n",
    "import jax\n",
    "from jax import debug\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from chex import ArrayTree\n",
    "from qdax.core.containers.repertoire import Repertoire\n",
    "from qdax.types import Descriptor, ExtraScores, Fitness, Genotype, RNGKey\n",
    "from qdax.environments.base_wrappers import QDEnv\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition, QDMCTransition\n",
    "#from qdax.core.neuroevolution.buffers.trajectory_buffer import TrajectoryBuffer\n",
    "import flashbax as fbx\n",
    "import chex\n",
    "from rein_related import *\n",
    "\n",
    "from qdax.core.emitters.emitter import Emitter, EmitterState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MCPGConfig:\n",
    "    \"\"\"Configuration for the REINaive emitter.\n",
    "    \"\"\"\n",
    "    no_agents: int = 256\n",
    "    batch_size: int = 1000*256\n",
    "    mini_batch_size: int = 1000*256\n",
    "    no_epochs: int = 16\n",
    "    learning_rate: float = 3e-4\n",
    "    discount_rate: float = 0.99\n",
    "    adam_optimizer: bool = True\n",
    "    buffer_size: int = 256000\n",
    "    clip_param: float = 0.2\n",
    "    \n",
    "class MCPGEmitterState(EmitterState):\n",
    "    \"\"\"Contains the trajectory buffer.\n",
    "    \"\"\"\n",
    "    #buffer: Any\n",
    "    buffer_state: Any\n",
    "    random_key: RNGKey\n",
    "    \n",
    "class MCPGEmitter(Emitter):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: MCPGConfig,\n",
    "        policy_net: nn.Module,\n",
    "        env: QDEnv,\n",
    "    ) -> None:\n",
    "        \n",
    "        self._config = config\n",
    "        self._policy = policy_net\n",
    "        self._env = env\n",
    "        \n",
    "        self._policy_opt = optax.adam(\n",
    "            learning_rate=self._config.learning_rate\n",
    "        )\n",
    "        buffer = fbx.make_trajectory_buffer(\n",
    "            max_length_time_axis=self._env.episode_length,\n",
    "            min_length_time_axis=self._env.episode_length,\n",
    "            sample_batch_size=int(0.125*self._config.no_agents),\n",
    "            add_batch_size=2*self._config.no_agents,\n",
    "            sample_sequence_length=self._env.episode_length,\n",
    "            period=self._env.episode_length,\n",
    "        )\n",
    "        self._buffer = buffer\n",
    "        \n",
    "    @property\n",
    "    def batch_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: the batch size emitted by the emitter.\n",
    "        \"\"\"\n",
    "        return self._config.no_agents\n",
    "    \n",
    "    @property\n",
    "    def use_all_data(self) -> bool:\n",
    "        \"\"\"Whther to use all data or not when used along other emitters.\n",
    "        \"\"\"\n",
    "        return True\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def init(\n",
    "        self,\n",
    "        random_key: RNGKey,\n",
    "        repertoire: Repertoire,\n",
    "        genotypes: Genotype,\n",
    "        fitnesses: Fitness,\n",
    "        descriptors: Descriptor,\n",
    "        extra_scores: ExtraScores,\n",
    "    ) -> Tuple[MCPGEmitterState, RNGKey]:\n",
    "        \"\"\"Initializes the emitter state.\n",
    "        \"\"\"\n",
    "        obs_size = self._env.observation_size\n",
    "        action_size = self._env.action_size\n",
    "        descriptor_size = self._env.state_descriptor_length\n",
    "        \n",
    "        # Init trajectory buffer\n",
    "\n",
    "        dummy_transition = QDMCTransition.init_dummy(\n",
    "            observation_dim=obs_size,\n",
    "            action_dim=action_size,\n",
    "            descriptor_dim=descriptor_size,\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        buffer = fbx.make_trajectory_buffer(\n",
    "            max_length_time_axis=self._env.episode_length,\n",
    "            min_length_time_axis=self._env.episode_length,\n",
    "            sample_batch_size=self._config.no_agents,\n",
    "            add_batch_size=self._config.no_agents,\n",
    "            sample_sequence_length=self._env.episode_length,\n",
    "            period=self._env.episode_length,\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        buffer_state = self._buffer.init(dummy_transition)\n",
    "        \n",
    "        '''\n",
    "        buffer = TrajectoryBuffer.init(\n",
    "            buffer_size=self._config.buffer_size,\n",
    "            transition=dummy_transition,\n",
    "            env_batch_size=self._config.no_agents*2,\n",
    "            episode_length=self._env.episode_length,\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        random_key, subkey = jax.random.split(random_key)\n",
    "        emitter_state = MCPGEmitterState(\n",
    "            #buffer=buffer,\n",
    "            buffer_state=buffer_state,\n",
    "            random_key=subkey,\n",
    "        )\n",
    "        #ÃŸprint(emitter_state)\n",
    "        \n",
    "        return emitter_state, random_key\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def emit(\n",
    "        self,\n",
    "        repertoire: Repertoire,\n",
    "        emitter_state: MCPGEmitterState,\n",
    "        random_key: RNGKey,\n",
    "    ) -> Tuple[Genotype, RNGKey]:\n",
    "        \"\"\"Do a step of MCPG emission.\n",
    "        \"\"\"\n",
    "        \n",
    "        no_agents = self._config.no_agents\n",
    "        \n",
    "        # sample parents\n",
    "        parents, random_key = repertoire.sample(\n",
    "            random_key=random_key,\n",
    "            num_samples=no_agents,\n",
    "        )\n",
    "        \n",
    "        offsprings_mcpg = self.emit_mcpg(emitter_state, parents)\n",
    "        \n",
    "        return offsprings_mcpg, {}, random_key\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def emit_mcpg(\n",
    "        self,\n",
    "        emitter_state: MCPGEmitterState,\n",
    "        parents: Genotype,\n",
    "    ) -> Genotype:\n",
    "        \"\"\"Emit the offsprings generated through MCPG mutation.\n",
    "        \"\"\"\n",
    "        \n",
    "        mutation_fn = partial(\n",
    "            self._mutation_function_mcpg,\n",
    "            emitter_state=emitter_state,\n",
    "        )\n",
    "        \n",
    "        offsprings = jax.vmap(mutation_fn)(parents)\n",
    "        \n",
    "        return offsprings\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def state_update(\n",
    "        self,\n",
    "        emitter_state: MCPGEmitterState,\n",
    "        repertoire: Optional[Repertoire],\n",
    "        genotypes: Optional[Genotype],\n",
    "        fitnesses: Optional[Fitness],\n",
    "        descriptors: Optional[Descriptor],\n",
    "        extra_scores: ExtraScores,\n",
    "    ) -> MCPGEmitterState:\n",
    "        \"\"\"Update the emitter state.\n",
    "        \"\"\"\n",
    "        \n",
    "        random_key, _ = jax.random.split(emitter_state.random_key)\n",
    "        \n",
    "        assert \"transitions\" in extra_scores.keys(), \"Missing transtitions or wrong key\"\n",
    "        transitions = extra_scores[\"transitions\"]\n",
    "        new_buffer_state = self._buffer.add(emitter_state.buffer_state, transitions)\n",
    "        new_emitter_state = emitter_state.replace(random_key=random_key, buffer_state=new_buffer_state)\n",
    "        \n",
    "        return new_emitter_state\n",
    "        \n",
    "        # update the buffer\n",
    "        '''\n",
    "        replay_buffer = emitter_state.buffer.insert(transitions)\n",
    "        emitter_state = emitter_state.replace(buffer=replay_buffer)\n",
    "        \n",
    "        return emitter_state\n",
    "        '''\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def compute_mask(\n",
    "        self,\n",
    "        done,\n",
    "    ):\n",
    "        return 1. - jnp.clip(jnp.cumsum(done), a_min=0., a_max=1.)\n",
    "    '''\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def compute_logps(\n",
    "        self,\n",
    "        policy_params,\n",
    "        obs,\n",
    "        actions,\n",
    "    ):\n",
    "        \"\"\"Compute the log probabilities of the actions.\n",
    "        \"\"\"\n",
    "        compute_logp = partial(\n",
    "            self._policy.apply,\n",
    "            params=policy_params,\n",
    "            method=self._policy.logp,\n",
    "        )\n",
    "        \n",
    "        return jax.vmap(compute_logp)(obs, actions)\n",
    "    '''\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def compute_logps(self, policy_params, obs, actions):\n",
    "        def compute_logp(single_obs, single_action):\n",
    "            # Correctly handle operations on single_obs and single_action\n",
    "            # Ensure no inappropriate method calls like .items() are made\n",
    "            return self._policy.apply(policy_params, single_obs, single_action, method=self._policy.logp)\n",
    "\n",
    "        # Use jax.vmap to apply compute_logp across batches of obs and actions\n",
    "        return jax.vmap(compute_logp, in_axes=(0, 0))(obs, actions)\n",
    "       \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return(\n",
    "        self,\n",
    "        rewards,\n",
    "    ):\n",
    "        def _body(carry, x):\n",
    "            (next_return,) = carry\n",
    "            (rewards,) = x\n",
    "\n",
    "            current_return = rewards + self._config.discount_rate * next_return\n",
    "            return (current_return,), (current_return,)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #jax.debug.print(\"rewards\", rewards.shape)\n",
    "        \n",
    "        _, (return_,) = jax.lax.scan(\n",
    "            _body,\n",
    "            (jnp.array(0.),),\n",
    "            (rewards,),\n",
    "            length=self._env.episode_length,\n",
    "            reverse=True,\n",
    "        )\n",
    "        \n",
    "        return return_\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return(self, rewards):\n",
    "        def _body(carry, reward):\n",
    "            next_return = carry  # carry should be unpacked directly if it's a single element\n",
    "            current_return = reward + self._config.discount_rate * next_return\n",
    "            return current_return, current_return  # Maintain the same shape and type\n",
    "\n",
    "        initial_return = jnp.array(0.0)  # Ensure initial_return is correctly shaped as a scalar\n",
    "        _, return_ = jax.lax.scan(\n",
    "            _body,\n",
    "            initial_return,\n",
    "            rewards,  # Pass rewards directly without extra tuple wrapping\n",
    "            length=int(self._env.episode_length),\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        return return_\n",
    "    '''\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def standardize(\n",
    "        self,\n",
    "        return_,\n",
    "    ):\n",
    "        return jax.nn.standardize(return_, axis=0, variance=1, epsilon=EPS)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_standardized_return(\n",
    "        self,\n",
    "        rewards,\n",
    "        mask,\n",
    "    ):\n",
    "        #mask = jnp.expand_dims(mask, axis=-1)\n",
    "        #valid_rewards = (rewards * mask)#.squeeze(axis=-1)\n",
    "        #jax.debug.print(\"mask: {}\", mask.shape)\n",
    "        #jax.debug.print(\"rewards*mask: {}\", (rewards * mask).shape)\n",
    "        return_ = jax.vmap(self.get_return)(rewards * mask)\n",
    "        return self.standardize(return_)\n",
    "    \n",
    "    '''\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _mutation_function_mcpg(\n",
    "        self,\n",
    "        policy_params,\n",
    "        emitter_state: MCPGEmitterState,\n",
    "    ) -> Genotype:\n",
    "        \"\"\"Mutation function for MCPG.\n",
    "        \"\"\"\n",
    "        \n",
    "        policy_opt_state = self._policy_opt.init(policy_params)\n",
    "        \n",
    "        random_key = emitter_state.random_key\n",
    "        buffer_state = emitter_state.buffer_state\n",
    "        \n",
    "        # NOW YOU DONT CARE BUT AT SOME POINT YOU MIGH NEED DIFFERENT RANDOM KEY FRO SAMPLING FOR EACH GENOTYPE\n",
    "        batch = self._buffer.sample(buffer_state, random_key)\n",
    "        \n",
    "        trans = batch.experience\n",
    "\n",
    "        \n",
    "        obs = trans.obs\n",
    "        actions = trans.actions\n",
    "        rewards = trans.rewards\n",
    "        dones = trans.dones\n",
    "        \n",
    "        mask = jax.vmap(self.compute_mask, in_axes=0)(dones)\n",
    "        logps = trans.logp  \n",
    "         \n",
    "        standardized_returns = self.get_standardized_return(rewards, mask)\n",
    "        \n",
    "        def scan_train_policy(\n",
    "            carry: Tuple[MCPGEmitterState, Genotype, optax.OptState],\n",
    "            unused: Any,\n",
    "        ) -> Tuple[Tuple[MCPGEmitterState, Genotype, optax.OptState], Any]:\n",
    "            \n",
    "            policy_params, policy_opt_state = carry\n",
    "            \n",
    "            (\n",
    "                new_policy_params,\n",
    "                new_policy_opt_state,\n",
    "            ) = self._train_policy_(\n",
    "                policy_params,\n",
    "                policy_opt_state,\n",
    "                obs,\n",
    "                actions,\n",
    "                standardized_returns,\n",
    "                logps,\n",
    "                mask\n",
    "            )\n",
    "            return (\n",
    "                new_policy_params,\n",
    "                new_policy_opt_state,\n",
    "            ), None\n",
    "            \n",
    "        (policy_params, policy_opt_state), _ = jax.lax.scan(\n",
    "            scan_train_policy,\n",
    "            (policy_params, policy_opt_state),\n",
    "            None,\n",
    "            length=self._config.no_epochs,\n",
    "        )\n",
    "        \n",
    "        return policy_params\n",
    "        '''\n",
    "        \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _mutation_function_mcpg(\n",
    "        self,\n",
    "        policy_params,\n",
    "        emitter_state: MCPGEmitterState,\n",
    "    ) -> Genotype:\n",
    "        \"\"\"Mutation function for MCPG.\"\"\"\n",
    "\n",
    "        policy_opt_state = self._policy_opt.init(policy_params)\n",
    "        \n",
    "        # Directly sample batch and use necessary components\n",
    "        batch = self._buffer.sample(emitter_state.buffer_state, emitter_state.random_key)\n",
    "        trans = batch.experience\n",
    "        mask = jax.vmap(self.compute_mask, in_axes=0)(trans.dones)\n",
    "        standardized_returns = self.get_standardized_return(trans.rewards, mask)\n",
    "        \n",
    "        def scan_train_policy(\n",
    "            carry: Tuple[MCPGEmitterState, Genotype, optax.OptState],\n",
    "            unused: Any,\n",
    "        ) -> Tuple[Tuple[MCPGEmitterState, Genotype, optax.OptState], Any]:\n",
    "            \n",
    "            policy_params, policy_opt_state = carry\n",
    "            \n",
    "            # Train policy with directly used transaction fields\n",
    "            new_policy_params, new_policy_opt_state = self._train_policy_(\n",
    "                policy_params,\n",
    "                policy_opt_state,\n",
    "                trans.obs,\n",
    "                trans.actions,\n",
    "                standardized_returns,\n",
    "                trans.logp,\n",
    "                mask\n",
    "            )\n",
    "            return (new_policy_params, new_policy_opt_state), None\n",
    "            \n",
    "        (policy_params, policy_opt_state), _ = jax.lax.scan(\n",
    "            scan_train_policy,\n",
    "            (policy_params, policy_opt_state),\n",
    "            None,\n",
    "            length=self._config.no_epochs,\n",
    "        )\n",
    "        \n",
    "        return policy_params\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _mutation_function_mcpg(\n",
    "        self,\n",
    "        policy_params,\n",
    "        emitter_state: MCPGEmitterState,\n",
    "    ) -> Genotype:\n",
    "        \"\"\"Mutation function for MCPG.\n",
    "        \"\"\"\n",
    "        \n",
    "        buffer = emitter_state.buffer\n",
    "        \n",
    "        policy_opt_state = self._policy_opt.init(policy_params)\n",
    "        \n",
    "        random_key = emitter_state.random_key\n",
    "        \n",
    "        #random_key, subkey = jax.random.split(emitter_state.random_key)\n",
    "        sample_size = int(self._config.batch_size) // int(self._env.episode_length)\n",
    "        #print(f\"episodic_data_size: {int(buffer.current_episodic_data_size)}\")\n",
    "        #episodic_data_size = buffer.current_episodic_data_size.item()\n",
    "        \n",
    "        trans, random_key = buffer.sample(\n",
    "            random_key=random_key,\n",
    "            sample_size=sample_size,\n",
    "            episodic_data_size=64,\n",
    "            sample_traj=True,\n",
    "        )\n",
    "        new_emitter_state = emitter_state.replace(random_key=random_key)\n",
    "        # trans has shape (episde_length*sample_size, transition_dim)\n",
    "        \n",
    "        obs = trans.obs.reshape(sample_size, self._env.episode_length, -1)\n",
    "        actions = trans.actions.reshape(sample_size, self._env.episode_length, -1)\n",
    "        rewards = trans.rewards.reshape(sample_size, self._env.episode_length, -1)\n",
    "        #jax.debug.print(\"rewards shape: {}\", rewards.shape)\n",
    "        #print(f\"rewards shape: {rewards.shape}\")\n",
    "        dones = trans.dones.reshape(sample_size, self._env.episode_length, -1)\n",
    "        \n",
    "        mask = jax.vmap(self.compute_mask, in_axes=0)(dones)\n",
    "        logps = jax.vmap(self.compute_logps, in_axes=(None, 0, 0))(policy_params, obs, actions)\n",
    "        \n",
    "        standardized_returns = self.get_standardized_return(rewards, mask)\n",
    "        \n",
    "        def scan_train_policy(\n",
    "            carry: Tuple[MCPGEmitterState, Genotype, optax.OptState],\n",
    "            unused: Any,\n",
    "        ) -> Tuple[Tuple[MCPGEmitterState, Genotype, optax.OptState], Any]:\n",
    "            \n",
    "            policy_params, policy_opt_state = carry\n",
    "            \n",
    "            (\n",
    "                new_policy_params,\n",
    "                new_policy_opt_state,\n",
    "            ) = self._train_policy_(\n",
    "                policy_params,\n",
    "                policy_opt_state,\n",
    "                obs,\n",
    "                actions,\n",
    "                standardized_returns,\n",
    "                mask,\n",
    "                logps,\n",
    "            )\n",
    "            \n",
    "            return (\n",
    "                new_policy_params,\n",
    "                new_policy_opt_state,\n",
    "            ), None\n",
    "\n",
    "        (policy_params, policy_opt_state), _ = jax.lax.scan(\n",
    "            scan_train_policy,\n",
    "            (policy_params, policy_opt_state),\n",
    "            None,\n",
    "            length=self._config.no_epochs,\n",
    "        )\n",
    "        \n",
    "        return policy_params\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _train_policy_(\n",
    "        self,\n",
    "        #emitter_state: MCPGEmitterState,\n",
    "        policy_params,\n",
    "        policy_opt_state: optax.OptState,\n",
    "        obs,\n",
    "        actions,\n",
    "        standardized_returns,\n",
    "        logps,\n",
    "        mask\n",
    "    ) -> Tuple[MCPGEmitterState, Genotype, optax.OptState]:\n",
    "        \"\"\"Train the policy.\n",
    "        \"\"\"\n",
    "        '''\n",
    "        random_key, subkey = jax.random.split(emitter_state.random_key)\n",
    "        buffer_state = emitter_state.buffer_state\n",
    "        \n",
    "        batch = self._buffer.sample(buffer_state, subkey)\n",
    "        \n",
    "        #sample_size = int(self._config.batch_size) // int(self._env.episode_length)\n",
    "        \n",
    "        trans = batch.experience\n",
    "        \n",
    "        #obs = trans.obs.reshape(self._config.no_agents, self._env.episode_length, -1)\n",
    "        #actions = trans.actions.reshape(self._config.no_agents, self._env.episode_length, -1)\n",
    "        #rewards = trans.rewards.reshape(self._config.no_agents, self._env.episode_length, -1)\n",
    "        #jax.debug.print(\"rewards shape: {}\", rewards.shape)\n",
    "        #print(f\"rewards shape: {rewards.shape}\")\n",
    "        #dones = trans.dones.reshape(self._config.no_agents, self._env.episode_length, -1)\n",
    "        \n",
    "        obs = trans.obs\n",
    "        actions = trans.actions\n",
    "        rewards = trans.rewards\n",
    "        dones = trans.dones\n",
    "        \n",
    "        mask = jax.vmap(self.compute_mask, in_axes=0)(dones)\n",
    "        logps = jax.vmap(self.compute_logps, in_axes=(None, 0, 0))(policy_params, obs, actions)\n",
    "        \n",
    "        standardized_returns = self.get_standardized_return(rewards, mask)\n",
    "        '''\n",
    "        \n",
    "        def scan_update(carry, _):\n",
    "            policy_params, policy_opt_state = carry\n",
    "            grads = jax.grad(self.loss_ppo)(policy_params, obs, actions, logps, mask, standardized_returns)\n",
    "            updates, new_policy_opt_state = self._policy_opt.update(grads, policy_opt_state)\n",
    "            new_policy_params = optax.apply_updates(policy_params, updates)\n",
    "            return (new_policy_params, new_policy_opt_state), None\n",
    "        \n",
    "        (final_policy_params, final_policy_opt_state), _ = jax.lax.scan(\n",
    "            scan_update,\n",
    "            (policy_params, policy_opt_state),\n",
    "            None,\n",
    "            length=1,\n",
    "        )\n",
    "        \n",
    "        #new_emitter_state = emitter_state.replace(random_key=random_key)\n",
    "        \n",
    "        return final_policy_params, final_policy_opt_state\n",
    "    \n",
    "    '''\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _train_policy_(\n",
    "        self,\n",
    "        policy_params,\n",
    "        policy_opt_state,\n",
    "        obs,\n",
    "        actions,\n",
    "        standardized_returns,\n",
    "        mask,\n",
    "        logps,\n",
    "    ):\n",
    "        \"\"\"Train the policy.\n",
    "        \"\"\"\n",
    "        \n",
    "        def _scan_update(carry, _):\n",
    "            policy_params, policy_opt_state = carry\n",
    "            grads = jax.grad(self.loss_ppo)(policy_params, obs, actions, logps, mask, standardized_returns)\n",
    "            updates, new_policy_opt_state = self._policy_opt.update(grads, policy_opt_state)\n",
    "            new_policy_params = optax.apply_updates(policy_params, updates)\n",
    "            return (new_policy_params, new_policy_opt_state), None\n",
    "        \n",
    "        (final_policy_params, final_policy_opt_state), _ = jax.lax.scan(\n",
    "            _scan_update,\n",
    "            (policy_params, policy_opt_state),\n",
    "            None,\n",
    "            length=1,\n",
    "        )\n",
    "\n",
    "        return final_policy_params, final_policy_opt_state\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def loss_ppo(\n",
    "        self,\n",
    "        params,\n",
    "        obs,\n",
    "        actions,\n",
    "        logps,\n",
    "        mask,\n",
    "        standardized_returns,\n",
    "    ):\n",
    "        \n",
    "        logps_ = self._policy.apply(\n",
    "            params,\n",
    "            jax.lax.stop_gradient(obs),\n",
    "            jax.lax.stop_gradient(actions),\n",
    "            method=self._policy.logp,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        ratio = jnp.exp(logps_ - jax.lax.stop_gradient(logps))\n",
    "        \n",
    "        pg_loss_1 = jnp.multiply(ratio * mask, jax.lax.stop_gradient(standardized_returns))\n",
    "        pg_loss_2 = jax.lax.stop_gradient(standardized_returns) * jax.lax.clamp(1. - self._config.clip_param, ratio, 1. + self._config.clip_param) * mask\n",
    "        \n",
    "        #return -jnp.mean(jnp.minimum(pg_loss_1, pg_loss_2))\n",
    "        return (-jnp.sum(jnp.minimum(pg_loss_1, pg_loss_2))) / jnp.sum(ratio * mask)\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MPLCONFIGDIR'] = '/tmp/matplotlib'\n",
    "os.environ['WANDB_CACHE_DIR'] = '/tmp/wandb_cache'\n",
    "os.environ['JAX_LOG_COMPILATION'] = '1'\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from math import floor\n",
    "from typing import Any, Dict, Tuple, List, Callable\n",
    "import pickle\n",
    "from flax import serialization\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "import hydra\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from hydra.core.config_store import ConfigStore\n",
    "from qdax.core.map_elites import MAPElites\n",
    "from qdax.types import RNGKey, Genotype\n",
    "from qdax.utils.sampling import sampling \n",
    "from qdax.core.containers.mapelites_repertoire import compute_cvt_centroids, MapElitesRepertoire\n",
    "from qdax.core.neuroevolution.networks.networks import MLPMCPG\n",
    "from qdax.core.emitters.me_mcpg_emitter import MEMCPGConfig, MEMCPGEmitter\n",
    "#from qdax.core.emitters.rein_emitter_advanced import REINaiveConfig, REINaiveEmitter\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.environments import behavior_descriptor_extractor\n",
    "from qdax.tasks.brax_envs import reset_based_scoring_function_brax_envs as scoring_function\n",
    "from utils import Config, get_env\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "import wandb\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "from qdax.utils.plotting import plot_map_elites_results, plot_2d_map_elites_repertoire\n",
    "import matplotlib.pyplot as plt\n",
    "from set_up_brax import get_reward_offset_brax\n",
    "from qdax import environments_v1, environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_name):\n",
    "    if env_name == \"hopper_uni\":\n",
    "        episode_length = 1000\n",
    "        \n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "    elif env_name == \"halfcheetah_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "        \n",
    "    elif env_name == \"walker2d_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\t\n",
    "    elif env_name == \"ant_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=True)\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, exclude_current_positions_from_observation=True)\t\n",
    "    '''\n",
    "    elif env_name == \"ant_omni\":\n",
    "        episode_length = 250\n",
    "        max_bd = 30.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=False)\t\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "        max_bd = 1.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length)\t\n",
    "    else:\n",
    "        ValueError(f\"Environment {env_name} not supported.\")\n",
    "    '''\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration from this experiment script\n",
    "    \"\"\"\n",
    "    # Env config\n",
    "    #alg_name: str\n",
    "    seed: int\n",
    "    env_name: str\n",
    "    episode_length: int\n",
    "    policy_hidden_layer_sizes: Tuple[int, ...]   \n",
    "    # ME config\n",
    "    num_evaluations: int\n",
    "    num_iterations: int\n",
    "    no_agents: int\n",
    "    num_samples: int\n",
    "    fixed_init_state: bool\n",
    "    discard_dead: bool\n",
    "    # Emitter config\n",
    "    iso_sigma: float\n",
    "    line_sigma: float\n",
    "    #crossover_percentage: float\n",
    "    # Grid config \n",
    "    grid_shape: Tuple[int, ...]\n",
    "    num_init_cvt_samples: int\n",
    "    num_centroids: int\n",
    "    # Log config\n",
    "    log_period: int\n",
    "    store_repertoire: bool\n",
    "    store_repertoire_log_period: int\n",
    "    \n",
    "    # REINFORCE Parameters\n",
    "    proportion_mutation_ga : float\n",
    "    batch_size: int\n",
    "    mini_batch_size: int\n",
    "    adam_optimizer: bool\n",
    "    learning_rate: float\n",
    "    discount_rate: float\n",
    "    buffer_size: int\n",
    "    clip_param: float\n",
    "    no_epochs: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    seed=0,\n",
    "    env_name='ant_uni',\n",
    "    episode_length=1000,\n",
    "    policy_hidden_layer_sizes=[128, 128],\n",
    "    num_evaluations=0,\n",
    "    num_iterations=4000,\n",
    "    num_samples=16,\n",
    "    no_agents=256,\n",
    "    fixed_init_state=False,\n",
    "    discard_dead=False,\n",
    "    grid_shape=[50, 50],\n",
    "    num_init_cvt_samples=50000,\n",
    "    num_centroids=1296,\n",
    "    log_period=400,\n",
    "    store_repertoire=True,\n",
    "    store_repertoire_log_period=800,\n",
    "    iso_sigma=0.005,\n",
    "    line_sigma=0.05,\n",
    "    proportion_mutation_ga=0.5,\n",
    "    batch_size=64000,\n",
    "    mini_batch_size=64000,\n",
    "    no_epochs=16,\n",
    "    buffer_size=64000,\n",
    "    adam_optimizer=True,\n",
    "    learning_rate=3e-4,\n",
    "    discount_rate=0.99,\n",
    "    clip_param=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 128]\n",
      "Number of parameters in policy_network:  21264\n"
     ]
    }
   ],
   "source": [
    "random_key = jax.random.PRNGKey(config.seed)\n",
    "\n",
    "# Init environment\n",
    "env = get_env('ant_uni')\n",
    "reset_fn = jax.jit(env.reset)\n",
    "\n",
    "# Compute the centroids\n",
    "centroids, random_key = compute_cvt_centroids(\n",
    "    num_descriptors=env.behavior_descriptor_length,\n",
    "    num_init_cvt_samples=config.num_init_cvt_samples,\n",
    "    num_centroids=config.num_centroids,\n",
    "    minval=0,\n",
    "    maxval=1,\n",
    "    random_key=random_key,\n",
    ")\n",
    "# Init policy network\n",
    "policy_layer_sizes = config.policy_hidden_layer_sizes #+ (env.action_size,)\n",
    "print(policy_layer_sizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "policy_network = MLPMCPG(\n",
    "    hidden_layers_size=policy_layer_sizes,\n",
    "    action_size=env.action_size,\n",
    "    activation=jax.nn.tanh,\n",
    "    hidden_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "    mean_init=jax.nn.initializers.orthogonal(scale=0.01),\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "'''\n",
    "policy_network = MLPMCPG(\n",
    "    hidden_layers_size=policy_layer_sizes,\n",
    "    action_size=env.action_size,\n",
    "    activation=jax.nn.tanh,\n",
    "    hidden_init=jax.nn.initializers.variance_scaling(scale=jnp.sqrt(2), mode='fan_in', distribution='uniform'),\n",
    "    mean_init=jax.nn.initializers.variance_scaling(scale=0.02*jnp.sqrt(2), mode='fan_in', distribution='uniform'),\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "policy_network = MLPMCPG(\n",
    "    hidden_layers_size=policy_layer_sizes,\n",
    "    action_size=env.action_size,\n",
    "    activation=jax.nn.tanh,\n",
    "    hidden_init=jax.nn.initializers.lecun_uniform(),\n",
    "    mean_init=jax.nn.initializers.lecun_uniform(),\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Init population of controllers\n",
    "\n",
    "# maybe consider adding two random keys for each policy\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, num=config.no_agents)\n",
    "#split_keys = jax.vmap(lambda k: jax.random.split(k, 2))(keys)\n",
    "#keys1, keys2 = split_keys[:, 0], split_keys[:, 1]\n",
    "fake_batch_obs = jnp.zeros(shape=(config.no_agents, env.observation_size))\n",
    "init_params = jax.vmap(policy_network.init)(keys, fake_batch_obs)\n",
    "\n",
    "param_count = sum(x[0].size for x in jax.tree_util.tree_leaves(init_params))\n",
    "print(\"Number of parameters in policy_network: \", param_count)\n",
    "\n",
    "# Define the fonction to play a step with the policy in the environment\n",
    "@jax.jit\n",
    "def play_step_fn(env_state, policy_params, random_key):\n",
    "    #random_key, subkey = jax.random.split(random_key)\n",
    "    actions, logp = policy_network.apply(policy_params, env_state.obs)\n",
    "    #logp = policy_network.apply(policy_params, env_state.obs, actions, method=policy_network.logp)\n",
    "    state_desc = env_state.info[\"state_descriptor\"]\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = QDMCTransition(\n",
    "        obs=env_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        actions=actions,\n",
    "        state_desc=state_desc,\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "        logp=logp,\n",
    "        #desc=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "        #desc_prime=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "    )\n",
    "\n",
    "    return next_state, policy_params, random_key, transition\n",
    "\n",
    "\n",
    "# Prepare the scoring function\n",
    "bd_extraction_fn = behavior_descriptor_extractor['ant_uni']\n",
    "scoring_fn = partial(\n",
    "    scoring_function,\n",
    "    episode_length=env.episode_length,\n",
    "    play_reset_fn=reset_fn,\n",
    "    play_step_fn=play_step_fn,\n",
    "    behavior_descriptor_extractor=bd_extraction_fn,\n",
    ")\n",
    "#reward_offset = get_reward_offset_brax(env, config.env_name)\n",
    "#print(f\"Reward offset: {reward_offset}\")\n",
    "\n",
    "me_scoring_fn = partial(\n",
    "sampling,\n",
    "scoring_fn=scoring_fn,\n",
    "num_samples=config.num_samples,\n",
    ")\n",
    "\n",
    "reward_offset = 0\n",
    "\n",
    "# Define a metrics function\n",
    "metrics_function = partial(\n",
    "    default_qd_metrics,\n",
    "    qd_offset=reward_offset * env.episode_length,\n",
    ")\n",
    "\n",
    "# Define the PG-emitter config\n",
    "\n",
    "me_mcpg_config = MEMCPGConfig(\n",
    "    proportion_mutation_ga=config.proportion_mutation_ga,\n",
    "    no_agents=config.no_agents,\n",
    "    batch_size=config.batch_size,\n",
    "    mini_batch_size=config.mini_batch_size,\n",
    "    no_epochs=config.no_epochs,\n",
    "    buffer_size=config.buffer_size,\n",
    "    learning_rate=config.learning_rate,\n",
    "    adam_optimizer=config.adam_optimizer,\n",
    "    clip_param=config.clip_param,\n",
    ")\n",
    "\n",
    "variation_fn = partial(\n",
    "    isoline_variation, iso_sigma=config.iso_sigma, line_sigma=config.line_sigma\n",
    ")\n",
    "\n",
    "me_mcpg_emitter = MEMCPGEmitter(\n",
    "    config=me_mcpg_config,\n",
    "    policy_network=policy_network,\n",
    "    env=env,\n",
    "    variation_fn=variation_fn,\n",
    "    )\n",
    "\n",
    "'''\n",
    "rein_emitter = REINaiveEmitter(\n",
    "    config=rein_emitter_config,\n",
    "    policy_network=policy_network,\n",
    "    env=env,\n",
    "    )\n",
    "'''\n",
    "'''\n",
    "me_scoring_fn = partial(\n",
    "    sampling,\n",
    "    scoring_fn=scoring_fn,\n",
    "    num_samples=config.num_samples,\n",
    ")\n",
    "'''\n",
    "\n",
    "# Instantiate MAP Elites\n",
    "map_elites = MAPElites(\n",
    "    scoring_function=scoring_fn,\n",
    "    emitter=me_mcpg_emitter,\n",
    "    metrics_function=metrics_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcpg_emitter_config = MCPGConfig(\n",
    "    no_agents=int(0.5*config.no_agents),\n",
    "    batch_size=config.batch_size,\n",
    "    mini_batch_size=config.mini_batch_size,\n",
    "    no_epochs=config.no_epochs,\n",
    "    learning_rate=config.learning_rate,\n",
    "    adam_optimizer=config.adam_optimizer,\n",
    "    buffer_size=config.buffer_size,\n",
    "    clip_param=config.clip_param,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcpg_emitter = MCPGEmitter(\n",
    "    config=mcpg_emitter_config,\n",
    "    policy_net=policy_network,\n",
    "    env=env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitnesses, descriptors, extra_scores, random_key = scoring_fn(\n",
    "    init_params, random_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 0.3811905301976367\n",
      "Standard deviation: 0.0003687136914491291\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "def score():\n",
    "    scoring_fn(\n",
    "        init_params, random_key\n",
    "    )\n",
    "\n",
    "score()\n",
    "\n",
    "timer = timeit.Timer(score)\n",
    "results = timer.repeat(repeat=10, number=1)  # Adjust 'repeat' and 'number' as needed\n",
    "\n",
    "# Calculate mean time and standard deviation\n",
    "mean_time = sum(results) / len(results)\n",
    "standard_deviation = (sum((x - mean_time) ** 2 for x in results) / len(results)) ** 0.5\n",
    "\n",
    "# Now you can use `mean_time` and `standard_deviation` as needed\n",
    "print(\"Mean time:\", mean_time)\n",
    "print(\"Standard deviation:\", standard_deviation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1025040/3082900524.py:1: UserWarning: This type of repertoire does not store the extra scores computed by the scoring function\n",
      "  repertoire = MapElitesRepertoire.init(\n"
     ]
    }
   ],
   "source": [
    "repertoire = MapElitesRepertoire.init(\n",
    "    genotypes=init_params,\n",
    "    fitnesses=fitnesses,\n",
    "    descriptors=descriptors,\n",
    "    centroids=centroids,\n",
    "    extra_scores=extra_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emitter_state, random_key = mcpg_emitter.init(\n",
    "    random_key=random_key,\n",
    "    repertoire=repertoire,\n",
    "    genotypes=init_params,\n",
    "    fitnesses=fitnesses,\n",
    "    descriptors=descriptors,\n",
    "    extra_scores=extra_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 0.00013925409875810147\n",
      "Standard deviation: 2.4144103243730508e-05\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "def update_state():\n",
    "    mcpg_emitter.state_update(emitter_state=emitter_state, extra_scores=extra_scores, repertoire=repertoire, genotypes=init_params, fitnesses=fitnesses, descriptors=descriptors)\n",
    "    \n",
    "    \n",
    "update_state()\n",
    "\n",
    "timer = timeit.Timer(update_state)\n",
    "results = timer.repeat(repeat=10, number=1)  # Adjust 'repeat' and 'number' as needed\n",
    "\n",
    "# Calculate mean time and standard deviation\n",
    "mean_time = sum(results) / len(results)\n",
    "standard_deviation = (sum((x - mean_time) ** 2 for x in results) / len(results)) ** 0.5\n",
    "\n",
    "# Now you can use `mean_time` and `standard_deviation` as needed\n",
    "print(\"Mean time:\", mean_time)\n",
    "print(\"Standard deviation:\", standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_genotype = jax.tree_map(lambda x: x[0], init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 0.00992647249950096\n",
      "Standard deviation: 0.00021561480170339742\n"
     ]
    }
   ],
   "source": [
    "def mutation_fn():\n",
    "    mcpg_emitter._mutation_function_mcpg(policy_params=first_genotype, emitter_state=emitter_state)\n",
    "    \n",
    "mutation_fn()\n",
    "\n",
    "timer = timeit.Timer(mutation_fn)\n",
    "results = timer.repeat(repeat=10, number=1)  # Adjust 'repeat' and 'number' as needed\n",
    "\n",
    "# Calculate mean time and standard deviation\n",
    "mean_time = sum(results) / len(results)\n",
    "standard_deviation = (sum((x - mean_time) ** 2 for x in results) / len(results)) ** 0.5\n",
    "\n",
    "# Now you can use `mean_time` and `standard_deviation` as needed\n",
    "print(\"Mean time:\", mean_time)\n",
    "print(\"Standard deviation:\", standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def emit():\n",
    "    mcpg_emitter.emit_mcpg(emitter_state=emitter_state, parents=init_params)\n",
    "    \n",
    "emit()\n",
    "\n",
    "timer = timeit.Timer(emit)\n",
    "\n",
    "results = timer.repeat(repeat=10, number=1)  # Adjust 'repeat' and 'number' as needed\n",
    "\n",
    "# Calculate mean time and standard deviation\n",
    "mean_time = sum(results) / len(results)\n",
    "standard_deviation = (sum((x - mean_time) ** 2 for x in results) / len(results)) ** 0.5\n",
    "\n",
    "# Now you can use `mean_time` and `standard_deviation` as needed\n",
    "print(\"Mean time:\", mean_time)\n",
    "print(\"Standard deviation:\", standard_deviation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 30 00:07:19 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A30                     Off | 00000000:01:00.0 Off |                   On |\n",
      "| N/A   29C    P0              26W / 165W |     50MiB / 24576MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                          |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                |        ECC|                       |\n",
      "|==================+================================+===========+=======================|\n",
      "|  0    1   0   0  |              25MiB / 11968MiB  | 28      0 |  2   0    2    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Union, Tuple, Callable, Optional\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "\n",
    "from qdax import environments, environments_v1\n",
    "from jax import random\n",
    "import wandb\n",
    "\n",
    "import pickle\n",
    "from optax import exponential_decay\n",
    "from IPython.display import HTML\n",
    "from brax.io import html\n",
    "import os\n",
    "import jax.debug\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 00:07:28.845321: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 00:07:28.875102: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 00:07:28.875173: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 00:07:30.484912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax \n",
    "from flax.linen.initializers import constant, orthogonal \n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "from wrappers import (\n",
    "    LogWrapper,\n",
    "    BraxGymnaxWrapper,\n",
    "    VecEnv,\n",
    "    NormalizeVecObservation,\n",
    "    NormalizeVecReward,\n",
    "    ClipAction,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "            \n",
    "        actor_mean = nn. Dense(\n",
    "            64, kernel_init = orthogonal(np.sqrt(2)), bias_init = constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_logstd = self.param(\"log_std\", lambda _, shape: jnp.log(0.5)*jnp.ones(shape), (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(loc=actor_mean, scale_diag=jnp.exp(actor_logstd))\n",
    "        \n",
    "        critic = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(critic)\n",
    "        \n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "    \n",
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    env, env_params = BraxGymnaxWrapper(config[\"ENV_NAME\"]), None\n",
    "    env = LogWrapper(env)\n",
    "    env = ClipAction(env)\n",
    "    env = VecEnv(env)\n",
    "    if config[\"NORMALIZE_ENV\"]:\n",
    "        env = NormalizeVecObservation(env)\n",
    "        env = NormalizeVecReward(env, config[\"GAMMA\"])\n",
    "    \n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "    \n",
    "    def train(rng):\n",
    "        # INIT NETWORK\n",
    "        network = ActorCritic(\n",
    "            env.action_space(env_params).shape[0], activation=config[\"ACTIVATION\"]\n",
    "        )\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(\n",
    "                    learning_rate=linear_schedule,\n",
    "                    eps=1e-5,\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(\n",
    "                    learning_rate=config[\"LR\"],\n",
    "                    eps=1e-5,\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "        \n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = env.reset(reset_rng, env_params)\n",
    "        \n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "                \n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi, value = network.apply(train_state.params, last_obs)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "                \n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = env.step(\n",
    "                    rng_step, env_state, action, env_params,\n",
    "                )    \n",
    "                transition = Transition(\n",
    "                    done, action, value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "            \n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, \n",
    "                runner_state, \n",
    "                None, \n",
    "                length=config[\"NUM_STEPS\"]\n",
    "            )\n",
    "            \n",
    "            # COMPUTE ADVANTAGES\n",
    "            train_state, env_state, last_obs, rng = runner_state\n",
    "            _, last_val = network.apply(train_state.params, last_obs)\n",
    "            \n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "                \n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                \n",
    "                return advantages, advantages + traj_batch.value\n",
    "            \n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "            \n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minibatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "                    \n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "                        \n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = 0.5 * jnp.mean(jnp.maximum(value_losses, value_losses_clipped))\n",
    "                        \n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                            ratio, \n",
    "                            1.0 - config[\"CLIP_EPS\"], \n",
    "                            1.0 + config[\"CLIP_EPS\"]\n",
    "                            ) \n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.mean(jnp.minimum(loss_actor1, loss_actor2))\n",
    "                        entropy = jnp.mean(pi.entropy())\n",
    "                        \n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "                    \n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grad = grad_fn(\n",
    "                        train_state.params, \n",
    "                        traj_batch, \n",
    "                        advantages, \n",
    "                        targets)\n",
    "                    train_state = train_state.apply_gradients(grads=grad)\n",
    "                    return train_state, total_loss\n",
    "                \n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of envs\"\n",
    "                \n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch   \n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                \n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minibatch,\n",
    "                    train_state,\n",
    "                    minibatches,\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "            \n",
    "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch,\n",
    "                update_state,\n",
    "                None,\n",
    "                length=config[\"UPDATE_EPOCHS\"], \n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "            if config.get(\"DEBUG\"):\n",
    "                \n",
    "                def callback(info):\n",
    "                    return_values = info[\"returned_episode_returns\"][\n",
    "                        info[\"returned_episode\"]\n",
    "                    ]\n",
    "                    timesteps = (\n",
    "                        info[\"timestep\"][info[\"returned_episode\"]] * config[\"NUM_ENVS\"]\n",
    "                    )\n",
    "                    for t in range(len(timesteps)):\n",
    "                        print(\n",
    "                            f\"global step={timesteps[t]}, episodic return={return_values[t]}\"\n",
    "                        )\n",
    "                        \n",
    "                jax.debug.callback(callback, metric)\n",
    "                \n",
    "            runner_state = (train_state, env_state, last_obs, rng)\n",
    "            return runner_state, metric\n",
    "        \n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step,\n",
    "            runner_state,\n",
    "            None,\n",
    "            length=config[\"NUM_UPDATES\"],\n",
    "        )\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "    \n",
    "    return train\n",
    "        \n",
    "            \n",
    "                \n",
    "                \n",
    "                    \n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Value ActorCritic(\n    # attributes\n    action_dim = 6\n    activation = 'tanh'\n) with type <class '__main__.ActorCritic'> is not a valid JAX type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m     21\u001b[0m train_jit \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mjit(make_train(config))\n\u001b[0;32m---> 22\u001b[0m out, network \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_jit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/core.py:1490\u001b[0m, in \u001b[0;36mconcrete_aval\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__jax_array__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1489\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete_aval(x\u001b[38;5;241m.\u001b[39m__jax_array__())\n\u001b[0;32m-> 1490\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid JAX \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1491\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Value ActorCritic(\n    # attributes\n    action_dim = 6\n    activation = 'tanh'\n) with type <class '__main__.ActorCritic'> is not a valid JAX type"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"LR\": 1e-3,\n",
    "    \"NUM_ENVS\": 2048,\n",
    "    \"NUM_STEPS\": 10,\n",
    "    \"TOTAL_TIMESTEPS\": 2e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 32,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.0,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"tanh\",\n",
    "    \"ENV_NAME\": \"walker2d\",\n",
    "    \"ANNEAL_LR\": False,\n",
    "    \"NORMALIZE_ENV\": True,\n",
    "    \"DEBUG\": True,\n",
    "}\n",
    "rng = jax.random.PRNGKey(30)\n",
    "train_jit = jax.jit(make_train(config))\n",
    "out = train_jit(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(env, network, train_state, num_steps=1000):\n",
    "    env_state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    rollout = []\n",
    "\n",
    "    while not done and len(rollout) < num_steps:\n",
    "        obs = jnp.array(env_state.observation)  # Make sure observation is correctly formatted\n",
    "        pi, _ = network.apply(train_state.params, obs)\n",
    "        action = pi.sample(seed=jax.random.PRNGKey(0))  # You might want to manage the RNG key better\n",
    "\n",
    "        env_state, reward, done, _ = env.step(action)\n",
    "        rollout.append(env_state)\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Total steps: {len(rollout)}, Total reward: {total_reward}\")\n",
    "    return rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TrainState' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     10\u001b[0m     rollout\u001b[38;5;241m.\u001b[39mappend(env_state)\n\u001b[0;32m---> 11\u001b[0m     pi, _ \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrunner_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m(out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunner_state\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m     12\u001b[0m     action \u001b[38;5;241m=\u001b[39m pi\u001b[38;5;241m.\u001b[39msample(seed\u001b[38;5;241m=\u001b[39m_rng)\n\u001b[1;32m     13\u001b[0m     rng, _rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainState' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "env = BraxGymnaxWrapper(config[\"ENV_NAME\"])\n",
    "\n",
    "rollout = []\n",
    "rewards = []\n",
    "\n",
    "rng, _rng = jax.random.split(rng)\n",
    "env_state = jax.jit(env.reset)(_rng)\n",
    "done = False\n",
    "while not done:\n",
    "    rollout.append(env_state)\n",
    "    pi, _ = out[\"runner_state\"][0].apply(out[\"runner_state\"][0].params)\n",
    "    action = pi.sample(seed=_rng)\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    env_state, _, reward, done, _ = env.step(_rng, env_state, action, None)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "a = html.render(env.sys, [s.qp for s in rollout])\n",
    "HTML(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_name):\n",
    "    if env_name == \"hopper_uni\":\n",
    "        episode_length = 1000\n",
    "        \n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "    elif env_name == \"halfcheetah_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "        \n",
    "    elif env_name == \"walker2d_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\t\n",
    "    elif env_name == \"ant_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=True)\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, exclude_current_positions_from_observation=True)\t\n",
    "    '''\n",
    "    elif env_name == \"ant_omni\":\n",
    "        episode_length = 250\n",
    "        max_bd = 30.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=False)\t\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "        max_bd = 1.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length)\t\n",
    "    else:\n",
    "        ValueError(f\"Environment {env_name} not supported.\")\n",
    "    '''\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_mitsides\u001b[0m (\u001b[33mmitsides\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/wandb/run-20240729_142058-r58x9nrr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mitsides/mcpg/runs/r58x9nrr' target=\"_blank\">PPOish</a></strong> to <a href='https://wandb.ai/mitsides/mcpg' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mitsides/mcpg' target=\"_blank\">https://wandb.ai/mitsides/mcpg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mitsides/mcpg/runs/r58x9nrr' target=\"_blank\">https://wandb.ai/mitsides/mcpg/runs/r58x9nrr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'MLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m policy_hidden_layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m]\n\u001b[1;32m     21\u001b[0m value_hidden_layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mMLP\u001b[49m(\n\u001b[1;32m     24\u001b[0m     hidden_layers_size\u001b[38;5;241m=\u001b[39mpolicy_hidden_layers,\n\u001b[1;32m     25\u001b[0m     action_size\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39maction_size,\n\u001b[1;32m     26\u001b[0m     activation\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mtanh,\n\u001b[1;32m     27\u001b[0m     hidden_init\u001b[38;5;241m=\u001b[39mjax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minitializers\u001b[38;5;241m.\u001b[39morthogonal(scale\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m)),\n\u001b[1;32m     28\u001b[0m     mean_init\u001b[38;5;241m=\u001b[39mjax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minitializers\u001b[38;5;241m.\u001b[39morthogonal(scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m),\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m value_net \u001b[38;5;241m=\u001b[39m ValueNet(\n\u001b[1;32m     32\u001b[0m     hidden_layers_size\u001b[38;5;241m=\u001b[39mvalue_hidden_layers,\n\u001b[1;32m     33\u001b[0m     hidden_init\u001b[38;5;241m=\u001b[39mjax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minitializers\u001b[38;5;241m.\u001b[39morthogonal(scale\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m)),\n\u001b[1;32m     34\u001b[0m     value_init\u001b[38;5;241m=\u001b[39mjax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minitializers\u001b[38;5;241m.\u001b[39morthogonal(scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m),\n\u001b[1;32m     35\u001b[0m     activation\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mtanh,\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m agent \u001b[38;5;241m=\u001b[39m MCPG(Config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mwandb\u001b[38;5;241m.\u001b[39mconfig), policy, value_net, env)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MLP' is not defined"
     ]
    }
   ],
   "source": [
    "config_dict = {\n",
    "    \"no_agents\": 32,\n",
    "    \"batch_size\": 32 * 1000,\n",
    "    \"mini_batch_size\": 32000,\n",
    "    \"no_epochs\": 10,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"discount_rate\": 0.99,\n",
    "    \"clip_param\": 0.2,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"env_name\": \"halfcheetah_uni\",\n",
    "}\n",
    "\n",
    "# Initialize wandb with the configuration dictionary\n",
    "wandb.init(project=\"mcpg\", name='PPOish', config=config_dict)\n",
    "\n",
    "env = get_env(config_dict[\"env_name\"])\n",
    "\n",
    "\n",
    "policy_hidden_layers = [64, 64]\n",
    "value_hidden_layers = [64, 64]\n",
    "\n",
    "policy = MLP(\n",
    "    hidden_layers_size=policy_hidden_layers,\n",
    "    action_size=env.action_size,\n",
    "    activation=nn.tanh,\n",
    "    hidden_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "    mean_init=jax.nn.initializers.orthogonal(scale=0.01),\n",
    ")\n",
    "\n",
    "value_net = ValueNet(\n",
    "    hidden_layers_size=value_hidden_layers,\n",
    "    hidden_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "    value_init=jax.nn.initializers.orthogonal(scale=1.),\n",
    "    activation=nn.tanh,\n",
    ")\n",
    "\n",
    "agent = MCPG(Config(**wandb.config), policy, value_net, env)\n",
    "\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "train_state_policy, train_state_value = agent.init(random_key)\n",
    "\n",
    "num_steps = 1000\n",
    "log_period = 10\n",
    "\n",
    "metrics_wandb = dict.fromkeys([\"mean loss\", \"mean reward\", \"mask\", \"evaluation\", 'time'], jnp.array([]))\n",
    "eval_num = config_dict[\"no_agents\"]\n",
    "print(f\"Number of evaluations per training step: {eval_num}\")\n",
    "start_time = time.time()\n",
    "for i in range(num_steps // log_period):\n",
    "    random_key, subkey = jax.random.split(random_key)\n",
    "    train_state_policy, train_state_value, current_metrics = agent.train(subkey, train_state_policy, train_state_value, log_period, eval=False)\n",
    "    timelapse = time.time() - start_time\n",
    "    print(f\"Step {(i+1) * log_period}, Time: {timelapse}\")\n",
    "    \n",
    "    current_metrics[\"evaluation\"] = jnp.arange(log_period*eval_num*(i+1), log_period*eval_num*(i+2), dtype=jnp.int32)\n",
    "    current_metrics[\"time\"] = jnp.repeat(timelapse, log_period)\n",
    "    current_metrics[\"mean loss\"] = jnp.repeat(jnp.mean(current_metrics[\"loss\"]), log_period)\n",
    "    current_metrics[\"mean reward\"] = jnp.repeat(jnp.mean(jnp.sum(current_metrics[\"reward\"], axis=-1)), log_period)\n",
    "    current_metrics[\"mask\"] = jnp.repeat(jnp.mean(current_metrics[\"mask\"]), log_period)\n",
    "    '''\n",
    "    metrics_wandb = jax.tree_util.tree_map(lambda metric, current_metric: jnp.concatenate([metric, current_metric], axis=0), metrics_wandb, current_metrics)\n",
    "    \n",
    "    log_metrics = jax.tree_util.tree_map(lambda metric: metric[-1], metrics_wandb)\n",
    "    \n",
    "    wandb.log(log_metrics)\n",
    "    '''\n",
    "    \n",
    "    def update_metrics(old_metrics, new_metrics):\n",
    "        updated_metrics = {}\n",
    "        for key in old_metrics:\n",
    "            if key in new_metrics:\n",
    "                # Check if old metrics for key is empty, and initialize properly if so\n",
    "                if old_metrics[key].size == 0:\n",
    "                    updated_metrics[key] = new_metrics[key]\n",
    "                else:\n",
    "                    updated_metrics[key] = jnp.concatenate([old_metrics[key], new_metrics[key]], axis=0)\n",
    "            else:\n",
    "                raise KeyError(f\"Key {key} not found in new metrics.\")\n",
    "        return updated_metrics\n",
    "\n",
    "    # In your training loop:\n",
    "    try:\n",
    "        metrics_wandb = update_metrics(metrics_wandb, current_metrics)\n",
    "        log_metrics = {k: v[-1] for k, v in metrics_wandb.items()}  # Assuming you want the latest entry\n",
    "        wandb.log(log_metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating metrics: {e}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

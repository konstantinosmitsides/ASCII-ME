{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 28 23:10:06 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A30                     Off | 00000000:01:00.0 Off |                   On |\n",
      "| N/A   30C    P0              25W / 165W |     50MiB / 24576MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                          |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                |        ECC|                       |\n",
      "|==================+================================+===========+=======================|\n",
      "|  0    1   0   0  |              25MiB / 11968MiB  | 28      0 |  2   0    2    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Union, Tuple, Callable, Optional\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "\n",
    "from qdax import environments, environments_v1\n",
    "from jax import random\n",
    "import wandb\n",
    "\n",
    "import pickle\n",
    "from optax import exponential_decay\n",
    "from IPython.display import HTML\n",
    "from brax.io import html\n",
    "import os\n",
    "import jax.debug\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax \n",
    "from flax.linen.initializers import constant, orthogonal \n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "from wrappers import (\n",
    "    LogWrapper,\n",
    "    BraxGymnaxWrapper,\n",
    "    VecEnv,\n",
    "    NormalizeVecObservation,\n",
    "    NormalizeVecReward,\n",
    "    ClipAction,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_name):\n",
    "    if env_name == \"hopper_uni\":\n",
    "        episode_length = 1000\n",
    "        \n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "    elif env_name == \"halfcheetah_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "        \n",
    "    elif env_name == \"walker2d_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\t\n",
    "    elif env_name == \"ant_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=True)\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, exclude_current_positions_from_observation=True)\t\n",
    "    '''\n",
    "    elif env_name == \"ant_omni\":\n",
    "        episode_length = 250\n",
    "        max_bd = 30.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=False)\t\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "        max_bd = 1.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length)\t\n",
    "    else:\n",
    "        ValueError(f\"Environment {env_name} not supported.\")\n",
    "    '''\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_mitsides\u001b[0m (\u001b[33mmitsides\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/wandb/run-20240728_231013-60bnpvhb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mitsides/mcpg/runs/60bnpvhb' target=\"_blank\">PPOish</a></strong> to <a href='https://wandb.ai/mitsides/mcpg' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mitsides/mcpg' target=\"_blank\">https://wandb.ai/mitsides/mcpg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mitsides/mcpg/runs/60bnpvhb' target=\"_blank\">https://wandb.ai/mitsides/mcpg/runs/60bnpvhb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluations per training step: 32\n",
      "Rewards shape: (1000,)\n",
      "Values shape: (1000,)\n",
      "Masks shape: (1000,)\n",
      "Next values shape: (1000,)\n",
      "Next masks shape: (1000,)\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=5/0)>\n",
      "<class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>\n",
      "<class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>\n",
      "Shape of obs: (32000, 19)\n",
      "Shape of action: (32000, 6)\n",
      "Shape of logp: (32000,)\n",
      "Shape of mask: (32000,)\n",
      "Shape of returns: (32000,)\n",
      "Shape of advantages: (32000,)\n",
      "Mean Loss: -0.3751281201839447\n",
      "Mean Reward: 227.6048583984375\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: -0.34584346413612366\n",
      "Mean Reward: 220.39080810546875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: -0.2665552794933319\n",
      "Mean Reward: 207.3115692138672\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: -0.09995461255311966\n",
      "Mean Reward: 208.86026000976562\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: -0.077603280544281\n",
      "Mean Reward: 194.46490478515625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: -0.022055653855204582\n",
      "Mean Reward: 233.114501953125\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: -0.06437928229570389\n",
      "Mean Reward: 215.60496520996094\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.047743894159793854\n",
      "Mean Reward: 208.30392456054688\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.07476633042097092\n",
      "Mean Reward: 222.58596801757812\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.0260646753013134\n",
      "Mean Reward: 214.0915069580078\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Step 10, Time: 32.865591049194336\n",
      "Mean Loss: 0.07876376062631607\n",
      "Mean Reward: 206.2412109375\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1214771717786789\n",
      "Mean Reward: 251.10751342773438\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.12060312926769257\n",
      "Mean Reward: 239.311767578125\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.08023425191640854\n",
      "Mean Reward: 239.06161499023438\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1109682023525238\n",
      "Mean Reward: 240.70773315429688\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1848255693912506\n",
      "Mean Reward: 244.6688232421875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.18940848112106323\n",
      "Mean Reward: 251.62136840820312\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.15878863632678986\n",
      "Mean Reward: 246.40478515625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.17910076677799225\n",
      "Mean Reward: 267.24505615234375\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.144684299826622\n",
      "Mean Reward: 265.54876708984375\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Step 20, Time: 17.260655641555786\n",
      "Mean Loss: 0.14903734624385834\n",
      "Mean Reward: 263.22808837890625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.24051031470298767\n",
      "Mean Reward: 283.5880126953125\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.17819581925868988\n",
      "Mean Reward: 280.0904846191406\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.16819578409194946\n",
      "Mean Reward: 277.17889404296875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.17942583560943604\n",
      "Mean Reward: 296.026123046875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.22617113590240479\n",
      "Mean Reward: 298.4344177246094\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.10987920314073563\n",
      "Mean Reward: 260.287841796875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1282748430967331\n",
      "Mean Reward: 302.81866455078125\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.17293959856033325\n",
      "Mean Reward: 296.5783386230469\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.14151443541049957\n",
      "Mean Reward: 296.6677551269531\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Step 30, Time: 17.255568027496338\n",
      "Mean Loss: 0.2167087197303772\n",
      "Mean Reward: 336.7820739746094\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.12913252413272858\n",
      "Mean Reward: 295.94647216796875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.19896838068962097\n",
      "Mean Reward: 305.31036376953125\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.15364743769168854\n",
      "Mean Reward: 318.013916015625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1792527586221695\n",
      "Mean Reward: 323.27734375\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1844256967306137\n",
      "Mean Reward: 333.8360595703125\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.20226134359836578\n",
      "Mean Reward: 329.4991760253906\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.15443862974643707\n",
      "Mean Reward: 326.71246337890625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1481863111257553\n",
      "Mean Reward: 342.75311279296875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.1720166653394699\n",
      "Mean Reward: 358.81939697265625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Step 40, Time: 17.244336128234863\n",
      "Mean Loss: 0.15675006806850433\n",
      "Mean Reward: 336.49078369140625\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.17616915702819824\n",
      "Mean Reward: 344.0032958984375\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.18327157199382782\n",
      "Mean Reward: 349.5532531738281\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n",
      "Mean Loss: 0.12332914769649506\n",
      "Mean Reward: 334.60614013671875\n",
      "Mean Mask: 1.0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "config_dict = {\n",
    "    \"no_agents\": 32,\n",
    "    \"batch_size\": 32 * 1000,\n",
    "    \"mini_batch_size\": 32000,\n",
    "    \"no_epochs\": 10,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"discount_rate\": 0.99,\n",
    "    \"clip_param\": 0.2,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"env_name\": \"halfcheetah_uni\",\n",
    "}\n",
    "\n",
    "# Initialize wandb with the configuration dictionary\n",
    "wandb.init(project=\"mcpg\", name='PPOish', config=config_dict)\n",
    "\n",
    "env = get_env(config_dict[\"env_name\"])\n",
    "\n",
    "\n",
    "policy_hidden_layers = [64, 64]\n",
    "value_hidden_layers = [64, 64]\n",
    "\n",
    "policy = MLP(\n",
    "    hidden_layers_size=policy_hidden_layers,\n",
    "    action_size=env.action_size,\n",
    "    activation=nn.tanh,\n",
    "    hidden_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "    mean_init=jax.nn.initializers.orthogonal(scale=0.01),\n",
    ")\n",
    "\n",
    "value_net = ValueNet(\n",
    "    hidden_layers_size=value_hidden_layers,\n",
    "    hidden_init=jax.nn.initializers.orthogonal(scale=jnp.sqrt(2)),\n",
    "    value_init=jax.nn.initializers.orthogonal(scale=1.),\n",
    "    activation=nn.tanh,\n",
    ")\n",
    "\n",
    "agent = MCPG(Config(**wandb.config), policy, value_net, env)\n",
    "\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "train_state_policy, train_state_value = agent.init(random_key)\n",
    "\n",
    "num_steps = 1000\n",
    "log_period = 10\n",
    "\n",
    "metrics_wandb = dict.fromkeys([\"mean loss\", \"mean reward\", \"mask\", \"evaluation\", 'time'], jnp.array([]))\n",
    "eval_num = config_dict[\"no_agents\"]\n",
    "print(f\"Number of evaluations per training step: {eval_num}\")\n",
    "start_time = time.time()\n",
    "for i in range(num_steps // log_period):\n",
    "    random_key, subkey = jax.random.split(random_key)\n",
    "    train_state_policy, train_state_value, current_metrics = agent.train(subkey, train_state_policy, train_state_value, log_period, eval=False)\n",
    "    timelapse = time.time() - start_time\n",
    "    print(f\"Step {(i+1) * log_period}, Time: {timelapse}\")\n",
    "    \n",
    "    current_metrics[\"evaluation\"] = jnp.arange(log_period*eval_num*(i+1), log_period*eval_num*(i+2), dtype=jnp.int32)\n",
    "    current_metrics[\"time\"] = jnp.repeat(timelapse, log_period)\n",
    "    current_metrics[\"mean loss\"] = jnp.repeat(jnp.mean(current_metrics[\"loss\"]), log_period)\n",
    "    current_metrics[\"mean reward\"] = jnp.repeat(jnp.mean(jnp.sum(current_metrics[\"reward\"], axis=-1)), log_period)\n",
    "    current_metrics[\"mask\"] = jnp.repeat(jnp.mean(current_metrics[\"mask\"]), log_period)\n",
    "    '''\n",
    "    metrics_wandb = jax.tree_util.tree_map(lambda metric, current_metric: jnp.concatenate([metric, current_metric], axis=0), metrics_wandb, current_metrics)\n",
    "    \n",
    "    log_metrics = jax.tree_util.tree_map(lambda metric: metric[-1], metrics_wandb)\n",
    "    \n",
    "    wandb.log(log_metrics)\n",
    "    '''\n",
    "    \n",
    "    def update_metrics(old_metrics, new_metrics):\n",
    "        updated_metrics = {}\n",
    "        for key in old_metrics:\n",
    "            if key in new_metrics:\n",
    "                # Check if old metrics for key is empty, and initialize properly if so\n",
    "                if old_metrics[key].size == 0:\n",
    "                    updated_metrics[key] = new_metrics[key]\n",
    "                else:\n",
    "                    updated_metrics[key] = jnp.concatenate([old_metrics[key], new_metrics[key]], axis=0)\n",
    "            else:\n",
    "                raise KeyError(f\"Key {key} not found in new metrics.\")\n",
    "        return updated_metrics\n",
    "\n",
    "    # In your training loop:\n",
    "    try:\n",
    "        metrics_wandb = update_metrics(metrics_wandb, current_metrics)\n",
    "        log_metrics = {k: v[-1] for k, v in metrics_wandb.items()}  # Assuming you want the latest entry\n",
    "        wandb.log(log_metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating metrics: {e}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from math import floor \n",
    "from typing import Callable, Tuple, Any\n",
    "\n",
    "import jax\n",
    "from jax import debug\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from chex import ArrayTree\n",
    "from qdax.core.containers.repertoire import Repertoire\n",
    "from qdax.types import Descriptor, ExtraScores, Fitness, Genotype, RNGKey\n",
    "from qdax.environments.base_wrappers import QDEnv\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.buffers.trajectory_buffer import TrajectoryBuffer\n",
    "from rein_related import *\n",
    "from qdax import environments_v1, environments\n",
    "\n",
    "from qdax.core.emitters.emitter import Emitter, EmitterState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_name):\n",
    "    if env_name == \"hopper_uni\":\n",
    "        episode_length = 1000\n",
    "        \n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "    elif env_name == \"halfcheetah_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "        \n",
    "    elif env_name == \"walker2d_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\t\n",
    "    elif env_name == \"ant_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=True)\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, exclude_current_positions_from_observation=True)\t\n",
    "    '''\n",
    "    elif env_name == \"ant_omni\":\n",
    "        episode_length = 250\n",
    "        max_bd = 30.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=False)\t\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "        max_bd = 1.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length)\t\n",
    "    else:\n",
    "        ValueError(f\"Environment {env_name} not supported.\")\n",
    "    '''\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration from this experiment script\n",
    "    \"\"\"\n",
    "    # Env config\n",
    "    #alg_name: str\n",
    "    seed: int\n",
    "    env_name: str\n",
    "    episode_length: int\n",
    "    policy_hidden_layer_sizes: Tuple[int, ...]   \n",
    "    # ME config\n",
    "    num_evaluations: int\n",
    "    num_iterations: int\n",
    "    batch_size: int\n",
    "    num_samples: int\n",
    "    fixed_init_state: bool\n",
    "    discard_dead: bool\n",
    "    # Emitter config\n",
    "    iso_sigma: float\n",
    "    line_sigma: float\n",
    "    #crossover_percentage: float\n",
    "    # Grid config \n",
    "    grid_shape: Tuple[int, ...]\n",
    "    num_init_cvt_samples: int\n",
    "    num_centroids: int\n",
    "    # Log config\n",
    "    log_period: int\n",
    "    store_repertoire: bool\n",
    "    store_repertoire_log_period: int\n",
    "    \n",
    "    # REINFORCE Parameters\n",
    "    proportion_mutation_ga : float\n",
    "    rollout_number: int\n",
    "    num_rein_training_steps: int\n",
    "    adam_optimizer: bool\n",
    "    learning_rate: float\n",
    "    discount_rate: float\n",
    "    temperature: int\n",
    "    buffer_size: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    seed=0,\n",
    "    env_name='ant_uni',\n",
    "    episode_length=1000,\n",
    "    policy_hidden_layer_sizes=[128, 128],\n",
    "    num_evaluations=0,\n",
    "    num_iterations=200,\n",
    "    num_samples=32,\n",
    "    batch_size=64,\n",
    "    fixed_init_state=False,\n",
    "    discard_dead=False,\n",
    "    grid_shape=[50, 50],\n",
    "    num_init_cvt_samples=50000,\n",
    "    num_centroids=1296,\n",
    "    log_period=400,\n",
    "    store_repertoire=True,\n",
    "    store_repertoire_log_period=800,\n",
    "    iso_sigma=0.005,\n",
    "    line_sigma=0.05,\n",
    "    proportion_mutation_ga=0.5,\n",
    "    rollout_number=32, # Num of episodes used for gradient estimate\n",
    "    num_rein_training_steps=5, # Num gradient steps per generation\n",
    "    buffer_size=102400, # Size of the replay buffer\n",
    "    adam_optimizer=True,\n",
    "    learning_rate=1e-3,\n",
    "    discount_rate=0.99,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class REINaiveConfig:\n",
    "    \"\"\"Configuration for the REINaive emitter.\n",
    "    \n",
    "    Args:\n",
    "        rollout_number: num of rollouts for gradient estimate\n",
    "        sample_sigma: std to sample the samples for gradient estimate  (IS THIS PARAMETER SPACE EXPLORATION?)\n",
    "        sample_mirror: if True, use mirroring sampling\n",
    "        sample_rank_norm: if True, use normalisation\n",
    "        \n",
    "        num_generations_sample: frequency of archive-sampling\n",
    "        \n",
    "        adam_optimizer: if True, use ADAM, if False, use SGD\n",
    "        learning_rate: obvious\n",
    "        l2_coefficient: coefficient for regularisation\n",
    "        \n",
    "        novelty_nearest_neighbors: num of nearest neigbors for novelty computation\n",
    "        use_novelty_archive: if True, use novelty archive for novelty (default is to use the content of the reperoire)\n",
    "        use_novelty_fifo: if True, use fifo archive for novelty (default is to use the content of the repertoire)\n",
    "        fifo_size: size of the novelty fifo bugger if used\n",
    "        \n",
    "        proprtion_explore: proportion of explore\n",
    "    \"\"\"\n",
    "    batch_size: int = 32\n",
    "    num_rein_training_steps: int = 10\n",
    "    buffer_size: int = 320000\n",
    "    rollout_number: int = 100\n",
    "    discount_rate: float = 0.99\n",
    "    adam_optimizer: bool = True\n",
    "    learning_rate: float = 1e-3\n",
    "    temperature: float = 0.\n",
    "\n",
    "\n",
    "\n",
    "class REINaiveEmitterState(EmitterState):\n",
    "    \"\"\"Contains replay buffer.\n",
    "    \"\"\"\n",
    "    trajectory_buffer: TrajectoryBuffer\n",
    "    random_key: RNGKey\n",
    "    \n",
    "    \n",
    "class REINaiveEmitter(Emitter):\n",
    "    \"\"\"\n",
    "    An emitter that uses gradients approximated through rollouts.\n",
    "    It dedicates part of the process on REINFORCE for fitness gradients and part\n",
    "    to exploration gradients.\n",
    "    \n",
    "    This scan version scans through parents isntead of performing all REINFORCE\n",
    "    operations in parallel, to avoid memory overload issue.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: REINaiveConfig,\n",
    "        policy_network: nn.Module,\n",
    "        env: QDEnv,\n",
    "    ) -> None:\n",
    "        self._config = config\n",
    "        self._policy = policy_network\n",
    "        self._env = env\n",
    "            \n",
    "            \n",
    "        # SET UP THE LOSSES\n",
    "        \n",
    "        # Init optimizers\n",
    "        \n",
    "        self._policies_optimizer = optax.adam(\n",
    "            learning_rate=self._config.learning_rate\n",
    "            )\n",
    "        \n",
    "    @property\n",
    "    def batch_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: the batch size emitted by the emitter.\n",
    "        \"\"\"\n",
    "        return self._config.batch_size\n",
    "    \n",
    "    @property \n",
    "    def use_all_data(self) -> bool:\n",
    "        \"\"\"Whether to use all data or not when used along other emitters.\n",
    "        \"\"\"\n",
    "        return True\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def init(\n",
    "        self,\n",
    "        random_key: RNGKey,\n",
    "        repertoire: Repertoire,\n",
    "        genotypes: Genotype,\n",
    "        fitnesses: Fitness,\n",
    "        descriptors: Descriptor,\n",
    "        extra_scores: ExtraScores,\n",
    "    ) -> Tuple[REINaiveEmitterState, RNGKey]:\n",
    "        \"\"\"Initializes the emitter.\n",
    "\n",
    "        Args:\n",
    "            init_genotypes: The initial population.\n",
    "            random_key: A random key.\n",
    "\n",
    "        Returns:\n",
    "            The initial state of the REINaiveEmitter, a new random key.\n",
    "        \"\"\"\n",
    "\n",
    "        observation_size = self._env.observation_size\n",
    "        action_size = self._env.action_size\n",
    "        descriptor_size = self._env.state_descriptor_length\n",
    "        \n",
    "        # Init trajectory buffer\n",
    "        dummy_transition = QDTransition.init_dummy(\n",
    "            observation_dim=observation_size,\n",
    "            action_dim=action_size,\n",
    "            descriptor_dim=descriptor_size,\n",
    "        )\n",
    "        \n",
    "        trajectory_buffer = TrajectoryBuffer.init(\n",
    "            buffer_size=self._config.buffer_size,\n",
    "            transition=dummy_transition,\n",
    "            env_batch_size=self._config.batch_size,\n",
    "            episode_length=self._env.episode_length,\n",
    "        )\n",
    "        \n",
    "        random_key, subkey = jax.random.split(random_key)\n",
    "        emitter_state = REINaiveEmitterState(\n",
    "            trajectory_buffer=trajectory_buffer,\n",
    "            random_key=subkey,\n",
    "        )\n",
    "        \n",
    "        return emitter_state, random_key\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def emit(\n",
    "        self,\n",
    "        repertoire: Repertoire,\n",
    "        emitter_state: REINaiveEmitterState,\n",
    "        random_key: RNGKey,\n",
    "    ) -> Tuple[Genotype, RNGKey]:\n",
    "        \"\"\"Do a step of REINFORCE emission.\n",
    "\n",
    "        Args:\n",
    "            repertoire: the current repertoire of genotypes.\n",
    "            emitter_state: the state of the emitter used\n",
    "            random_key: random key\n",
    "\n",
    "        Returns:\n",
    "            A batch of offspring, the new emitter state and a new key.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = self._config.batch_size\n",
    "        \n",
    "        # sample parents\n",
    "        parents, random_key = repertoire.sample(random_key, batch_size)\n",
    "        \n",
    "        offsprings_rein = self.emit_rein(emitter_state, parents)\n",
    "        \n",
    "        genotypes = offsprings_rein\n",
    "        \n",
    "        return genotypes, {}, random_key\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def emit_rein(\n",
    "        self,\n",
    "        emitter_state: REINaiveEmitterState,\n",
    "        parents: Genotype,\n",
    "    ) -> Genotype:\n",
    "        \"\"\"Emit the offsprings generated through REINFORCE mutation.\n",
    "\n",
    "        Args:\n",
    "            emitter_state: the state of the emitter used, contains\n",
    "            the trahectory buffer.\n",
    "            parents: the parents selected to be applied gradients \n",
    "            to mutate towards better performance.\n",
    "\n",
    "        Returns:\n",
    "            A new set of offspring.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Do a step of REINFORCE emission\n",
    "        mutation_fn = partial(\n",
    "            self._mutation_function_rein,\n",
    "            emitter_state=emitter_state,\n",
    "        )\n",
    "        offsprings = jax.vmap(mutation_fn)(parents)\n",
    "        \n",
    "        return offsprings\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _mutation_function_rein(\n",
    "        self,\n",
    "        policy_params: Genotype,\n",
    "        emitter_state: REINaiveEmitterState,\n",
    "    ) -> Genotype:\n",
    "        \"\"\"Apply REINFORCE mutation to a policy via multiple steps of gradient descent.\n",
    "\n",
    "        Args:\n",
    "            policy_params: a policy, supposed to be a differentiable neuaral network.\n",
    "            emitter_state: the current state of the emitter, containing among others,\n",
    "            the trajectory buffer.\n",
    "\n",
    "        Returns:\n",
    "            The updated parameters of the neural network.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define new policy optimizer state\n",
    "        policy_optimizer_state = self._policies_optimizer.init(policy_params)\n",
    "        \n",
    "        def scan_train_policy(\n",
    "            carry: Tuple[REINaiveEmitterState, Genotype, optax.OptState],\n",
    "            unused: Any,\n",
    "        ) -> Tuple[Tuple[REINaiveEmitterState, Genotype, optax.OptState], Any]:\n",
    "            \"\"\"Scans through the parents and applies REINFORCE training.\n",
    "            \"\"\"\n",
    "            \n",
    "            emitter_state, policy_params, policy_optimizer_state = carry\n",
    "            \n",
    "            (\n",
    "                new_emitter_state,\n",
    "                new_policy_params,\n",
    "                new_policy_optimizer_state,\n",
    "            ) = self._train_policy_(\n",
    "                emitter_state,\n",
    "                policy_params,\n",
    "                policy_optimizer_state,\n",
    "            )\n",
    "            return (\n",
    "                new_emitter_state,\n",
    "                new_policy_params,\n",
    "                new_policy_optimizer_state,\n",
    "            ), ()\n",
    "            \n",
    "        (emitter_state, policy_params, policy_optimizer_state,), _ = jax.lax.scan(\n",
    "            scan_train_policy,\n",
    "            (emitter_state, policy_params, policy_optimizer_state),\n",
    "            (),\n",
    "            length=self._config.num_rein_training_steps,\n",
    "        )\n",
    "        \n",
    "        return policy_params\n",
    "        \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _train_policy_(\n",
    "        self,\n",
    "        emitter_state: REINaiveEmitterState,\n",
    "        policy_params: Genotype,\n",
    "        policy_optimizer_state: optax.OptState,\n",
    "    ) -> Tuple[REINaiveEmitterState, Genotype, optax.OptState]:\n",
    "        \"\"\"Apply one gradient step to a policy (called policy_params).\n",
    "\n",
    "        Args:\n",
    "            emitter_state: the current state of the emitter.\n",
    "            policy_params: the current parameters of the policy network.\n",
    "            policy_optimizer_state: the current state of the optimizer.\n",
    "\n",
    "        Returns:\n",
    "            The updated state of the emitter, the updated policy parameters\n",
    "            and the updated optimizer state.\n",
    "        \"\"\"\n",
    "\n",
    "        random_keys = jax.random.split(emitter_state.random_key, self._config.rollout_number+1)\n",
    "        obs, action, logp, reward, _, mask = jax.vmap(\n",
    "            self._sample_trajectory, in_axes=(0, None))(random_keys[:-1], policy_params)\n",
    "        \n",
    "        #debug.print(\"obs.shape: {}\", obs.shape)\n",
    "        \n",
    "        # Add entropy term to reward\n",
    "        reward += self._config.temperature * (-logp)\n",
    "        \n",
    "        # Compute standardized return\n",
    "        return_standardized = self.get_return_standardized(reward, mask)\n",
    "        \n",
    "        # update policy\n",
    "        policy_optimizer_state, policy_params = self._update_policy(\n",
    "            policy_params=policy_params,\n",
    "            policy_optimizer_state=policy_optimizer_state,\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            logp=logp,\n",
    "            mask=mask,\n",
    "            return_standardized=return_standardized,\n",
    "        )\n",
    "        \n",
    "        new_emitter_state = emitter_state.replace(\n",
    "            random_key=random_keys[-1]\n",
    "        )\n",
    "        #print(reward * mask)\n",
    "        #print('-'*50)\n",
    "        \n",
    "        #average_reward = jnp.mean(jnp.sum(reward * mask, axis=-1))\n",
    "        #av_mask = jnp.mean(jnp.sum(mask, axis=-1))\n",
    "        #debug.print(\"Average Reward: {}\", average_reward)\n",
    "        #debug.print('-'*50)      \n",
    "        #debug.print(\"Average mask: {}\", av_mask)  \n",
    "        return new_emitter_state, policy_params, policy_optimizer_state\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _sample_trajectory(\n",
    "        self,\n",
    "        random_key: RNGKey,\n",
    "        policy_params: Genotype,\n",
    "    ):\n",
    "        \"\"\"Samples a full trajectory using the environment and policy.\n",
    "        Args:\n",
    "            random_key: a random key.\n",
    "            policy_params: the current parameters of the policy network.\n",
    "        Returns:\n",
    "            A tuple of observation, action, log-probability, reward, state descriptor, and mask arrays.\n",
    "        \"\"\"\n",
    "        random_keys = jax.random.split(random_key, self._env.episode_length + 1)\n",
    "        env_state_init = self._env.reset(random_keys[-1])\n",
    "        #debug.print(\"env_state_init: {}\", env_state_init)\n",
    "        #debug.print('-'*50)        \n",
    "        \n",
    "        def _scan_sample_step(carry, x):\n",
    "            (policy_params, env_state,) = carry\n",
    "            (random_key,) = x\n",
    "            \n",
    "            next_env_state, action, action_logp = self.sample_step(\n",
    "                random_key, policy_params, env_state\n",
    "            )\n",
    "            return (policy_params, next_env_state), (\n",
    "                env_state.obs,\n",
    "                action,\n",
    "                action_logp,\n",
    "                next_env_state.reward,\n",
    "                env_state.done,\n",
    "                env_state.info[\"state_descriptor\"],\n",
    "            )\n",
    "        print(f\"Length : {self._env.episode_length}\")\n",
    "        _, (obs, action, action_logp, reward, done, state_desc) = jax.lax.scan(\n",
    "            _scan_sample_step,\n",
    "            (policy_params, env_state_init),\n",
    "            (random_keys[:self._env.episode_length],),\n",
    "            length=self._env.episode_length,\n",
    "        )\n",
    "        \n",
    "        # compute a mask to indicate the valid steps\n",
    "        mask = 1. - jnp.clip(jnp.cumsum(done), a_min=0., a_max=1.)        \n",
    "        return obs, action, action_logp, reward, state_desc, mask\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def sample_step(\n",
    "        self,\n",
    "        random_key: RNGKey,\n",
    "        policy_params: Genotype,\n",
    "        env_state: Any,\n",
    "    ) -> Tuple[Any, Any, Any]:\n",
    "        \"\"\"Samples a step using the environment and policy.\n",
    "\n",
    "        Args:\n",
    "            random_key: a random key.\n",
    "            policy_params: the current parameters of the policy network.\n",
    "            env_state: the current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of the next environment state, the action, and log-probability of the action.\n",
    "        \"\"\"\n",
    "        #print(f\"policy_params type: {type(policy_params)}\")\n",
    "        #print(f\"policy_params: {policy_params}\")\n",
    "        #print(f\"env_state.obs: {env_state.obs}\")\n",
    "        #print(f\"env_state.obs type: {type(env_state.obs)}\")\n",
    "        '''\n",
    "        action, action_logp = self._policy.sample(\n",
    "            policy_params, random_key, env_state.obs\n",
    "        )\n",
    "        '''\n",
    "        action, action_logp = self._policy.apply(\n",
    "            policy_params, random_key, env_state.obs, method=self._policy.sample\n",
    "        )\n",
    "\n",
    "        next_env_state = self._env.step(env_state, action)\n",
    "        \n",
    "        return next_env_state, action, action_logp\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return_standardized(self, reward: jnp.ndarray, mask: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Compute the standardized return.\n",
    "\n",
    "        Args:\n",
    "            reward: the reward obtained.\n",
    "            mask: the mask to indicate the valid steps.\n",
    "\n",
    "        Returns:\n",
    "            The standardized return.\n",
    "        \"\"\"\n",
    "        # compute the return\n",
    "        return_ = jax.vmap(self.get_return)(reward * mask)\n",
    "        return self.standardize(return_)\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def get_return(self, reward):\n",
    "        \"\"\"Computes the discounted return for each step in the trajectory.\n",
    "        Args:\n",
    "            reward: the reward array.\n",
    "        Returns:\n",
    "            The discounted return array.\n",
    "        \"\"\"\n",
    "        def _body(carry, x):\n",
    "            (next_return,) = carry\n",
    "            (reward,) = x\n",
    "            current_return = reward + self._config.discount_rate * next_return\n",
    "            return (current_return,), (current_return,)\n",
    "        \n",
    "        _, (return_,) = jax.lax.scan(\n",
    "            _body,\n",
    "            (jnp.array(0.),),\n",
    "            (reward,),\n",
    "            length=self._env.episode_length,\n",
    "            reverse=True,\n",
    "        )\n",
    "        return return_\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def standardize(self, return_):\n",
    "        \"\"\"Standardizes the return values.\n",
    "        Args:\n",
    "            return_: the return array.\n",
    "        Returns:\n",
    "            The standardized return array.\n",
    "        \"\"\"\n",
    "        #return (return_ - return_.mean()) / (return_.std() + 1e-8)\n",
    "        return jax.nn.standardize(return_, axis=0, variance=1., epsilon=EPS)\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _update_policy(\n",
    "        self,\n",
    "        policy_params: Genotype,\n",
    "        policy_optimizer_state: optax.OptState,\n",
    "        obs,\n",
    "        action,\n",
    "        logp,\n",
    "        mask,\n",
    "        return_standardized\n",
    "    ):\n",
    "        \"\"\"Updates the policy parameters using the optimizer.\n",
    "        Args:\n",
    "            policy_params: the current parameters of the policy network.\n",
    "            policy_optimizer_state: the current state of the optimizer.\n",
    "            obs: observations from the environment.\n",
    "            action: actions taken in the environment.\n",
    "            logp: log-probabilities of the actions.\n",
    "            mask: the mask array indicating valid steps.\n",
    "            return_standardized: the standardized return values.\n",
    "        Returns:\n",
    "            The updated optimizer state and policy parameters.\n",
    "        \"\"\"\n",
    "        def loss_fn(params):\n",
    "            #logp_ = self._policy.logp(params, jax.lax.stop_gradient(obs), jax.lax.stop_gradient(action))\n",
    "            logp_ = self._policy.apply(params, jax.lax.stop_gradient(obs), jax.lax.stop_gradient(action), method=self._policy.logp)\n",
    "            #return -jnp.mean(logp_ * mask * return_standardized)\n",
    "            return -jnp.mean(jnp.multiply(logp_ * mask, jax.lax.stop_gradient(return_standardized)))\n",
    "\n",
    "        grads = jax.grad(loss_fn)(policy_params)\n",
    "        updates, new_optimizer_state = self._policies_optimizer.update(grads, policy_optimizer_state)\n",
    "        new_policy_params = optax.apply_updates(policy_params, updates)\n",
    "        return new_optimizer_state, new_policy_params\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def replace_state(self, emitter_state, random_key):\n",
    "        return emitter_state.replace(random_key=random_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MPLCONFIGDIR'] = '/tmp/matplotlib'\n",
    "os.environ['WANDB_CACHE_DIR'] = '/tmp/wandb_cache'\n",
    "os.environ['JAX_LOG_COMPILATION'] = '1'\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from math import floor\n",
    "from typing import Any, Dict, Tuple, List, Callable\n",
    "import pickle\n",
    "from flax import serialization\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "import hydra\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from hydra.core.config_store import ConfigStore\n",
    "from qdax.core.map_elites import MAPElites\n",
    "from qdax.types import RNGKey, Genotype\n",
    "from qdax.utils.sampling import sampling \n",
    "from qdax.core.containers.mapelites_repertoire import compute_cvt_centroids, MapElitesRepertoire\n",
    "from qdax.core.neuroevolution.networks.networks import MLP, MLPRein\n",
    "#from qdax.core.emitters.rein_var import REINConfig, REINEmitter\n",
    "#from qdax.core.emitters.rein_emitter import REINaiveConfig, REINaiveEmitter\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.environments import behavior_descriptor_extractor\n",
    "from qdax.tasks.brax_envs import reset_based_scoring_function_brax_envs as scoring_function\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "import wandb\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "from qdax.utils.plotting import plot_map_elites_results, plot_2d_map_elites_repertoire\n",
    "import matplotlib.pyplot as plt\n",
    "from set_up_brax import get_reward_offset_brax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 128]\n",
      "Number of parameters in policy_network:  21264\n"
     ]
    }
   ],
   "source": [
    "random_key = jax.random.PRNGKey(config.seed)\n",
    "\n",
    "# Init environment\n",
    "env = get_env(\"ant_uni\")\n",
    "reset_fn = jax.jit(env.reset)\n",
    "\n",
    "# Compute the centroids\n",
    "centroids, random_key = compute_cvt_centroids(\n",
    "    num_descriptors=env.behavior_descriptor_length,\n",
    "    num_init_cvt_samples=config.num_init_cvt_samples,\n",
    "    num_centroids=config.num_centroids,\n",
    "    minval=0,\n",
    "    maxval=1,\n",
    "    random_key=random_key,\n",
    ")\n",
    "# Init policy network\n",
    "policy_layer_sizes = config.policy_hidden_layer_sizes #+ (env.action_size,)\n",
    "print(policy_layer_sizes)\n",
    "policy_network = MLPRein(\n",
    "    action_size=env.action_size,\n",
    "    layer_sizes=policy_layer_sizes,\n",
    "    kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "    final_activation=jnp.tanh,\n",
    ")\n",
    "\n",
    "# Init population of controllers\n",
    "\n",
    "# maybe consider adding two random keys for each policy\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, num=config.batch_size)\n",
    "#split_keys = jax.vmap(lambda k: jax.random.split(k, 2))(keys)\n",
    "#keys1, keys2 = split_keys[:, 0], split_keys[:, 1]\n",
    "fake_batch_obs = jnp.zeros(shape=(config.batch_size, env.observation_size))\n",
    "init_params = jax.vmap(policy_network.init)(keys, fake_batch_obs)\n",
    "\n",
    "param_count = sum(x[0].size for x in jax.tree_util.tree_leaves(init_params))\n",
    "print(\"Number of parameters in policy_network: \", param_count)\n",
    "\n",
    "# Define the fonction to play a step with the policy in the environment\n",
    "def play_step_fn(env_state, policy_params, random_key):\n",
    "    #random_key, subkey = jax.random.split(random_key)\n",
    "    actions = policy_network.apply(policy_params, env_state.obs)\n",
    "    state_desc = env_state.info[\"state_descriptor\"]\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = QDTransition(\n",
    "        obs=env_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        actions=actions,\n",
    "        state_desc=state_desc,\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "        #desc=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "        #desc_prime=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "    )\n",
    "\n",
    "    return next_state, policy_params, random_key, transition\n",
    "\n",
    "# Prepare the scoring function\n",
    "bd_extraction_fn = behavior_descriptor_extractor[config.env_name]\n",
    "scoring_fn = partial(\n",
    "    scoring_function,\n",
    "    episode_length=env.episode_length,\n",
    "    play_reset_fn=reset_fn,\n",
    "    play_step_fn=play_step_fn,\n",
    "    behavior_descriptor_extractor=bd_extraction_fn,\n",
    ")\n",
    "#reward_offset = get_reward_offset_brax(env, config.env_name)\n",
    "#print(f\"Reward offset: {reward_offset}\")\n",
    "\n",
    "me_scoring_fn = partial(\n",
    "sampling,\n",
    "scoring_fn=scoring_fn,\n",
    "num_samples=config.num_samples,\n",
    ")\n",
    "\n",
    "reward_offset = 0\n",
    "metrics_function = partial(\n",
    "    default_qd_metrics,\n",
    "    qd_offset=reward_offset * env.episode_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rein_emitter_config = REINaiveConfig(\n",
    "    batch_size=config.batch_size,\n",
    "    num_rein_training_steps=config.num_rein_training_steps,\n",
    "    buffer_size=config.buffer_size,\n",
    "    rollout_number=config.rollout_number,\n",
    "    discount_rate=config.discount_rate,\n",
    "    adam_optimizer=config.adam_optimizer,\n",
    "    learning_rate=config.learning_rate,\n",
    ")\n",
    "\n",
    "\n",
    "rein_emitter = REINaiveEmitter(\n",
    "    config=rein_emitter_config,\n",
    "    policy_network=policy_network,\n",
    "    env=env,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_optimizer = optax.adam(config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emitter_state, random_key = rein_emitter.init(random_key, None, None, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "genotype = jax.tree_util.tree_map(lambda x: x[0], init_params)\n",
    "policy_opt_state = policy_optimizer.init(genotype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample one trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length : 1000\n",
      "548 ms ± 2.55 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rein_emitter._sample_trajectory(random_key, genotype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample rollout_number trajectories in parallel, update, replace emitter_state for one genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604 ms ± 350 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rein_emitter._train_policy_(emitter_state, genotype, policy_opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the above for num_rein_training_steps gradient steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.03 s ± 826 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rein_emitter._mutation_function_rein(genotype, emitter_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do all the above for half batch_size in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.88 s ± 3.51 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rein_emitter.emit_rein(emitter_state, init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length : 1000\n"
     ]
    }
   ],
   "source": [
    "random_keys = jax.random.split(emitter_state.random_key, config.rollout_number+1)\n",
    "obs, action, logp, reward, _, mask = jax.vmap(rein_emitter._sample_trajectory, in_axes=(0, None))(random_keys[:-1], genotype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_standardized = rein_emitter.get_return_standardized(reward, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do one update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 µs ± 46.5 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rein_emitter._update_policy(genotype, policy_opt_state, obs, action, logp, mask, return_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace emitter_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.65 ms ± 110 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rein_emitter.replace_state(emitter_state, random_keys[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter, PercentFormatter\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines\n",
    "from scipy.stats import wilcoxon, ranksums, mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "\n",
    "from utils import get_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGO_LIST = [\n",
    "    \"mcpg_me\",\n",
    "    \"dcg_me\",\n",
    "    \"pga_me\",\n",
    "    \"me\",\n",
    "    \"memes\",\n",
    "    \"ppga\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(df_row):\n",
    "    if df_row[\"algo\"] == \"pga_me\":\n",
    "        if df_row[\"batch_size\"] != 1024:\n",
    "            return \n",
    "\n",
    "    if df_row[\"algo\"] == \"me\":\n",
    "        if df_row[\"batch_size\"] != 8192:\n",
    "            return \n",
    "        \n",
    "    if df_row[\"algo\"] == \"ppga\":\n",
    "        if df_row[\"batch_size\"] != 6000:\n",
    "            return \n",
    "        \n",
    "    if df_row[\"algo\"] == \"memes\":\n",
    "        if df_row[\"batch_size\"] != 8192:\n",
    "            return \n",
    "        \n",
    "    if df_row[\"algo\"] == \"dcg_me\":\n",
    "        if df_row[\"batch_size\"] != 2048:\n",
    "            return \n",
    "        \n",
    "    if df_row[\"algo\"] == \"mcpg_me\":\n",
    "        if df_row[\"batch_size\"] != 4096:\n",
    "            return \n",
    "        \n",
    "\n",
    "        \n",
    "    if df_row[\"algo\"] == \"mcpg_me\":\n",
    "        if df_row[\"proportion_mutation_ga\"] == 0 and df_row[\"greedy\"] == 0:\n",
    "            return \"mcpg_only\"\n",
    "    if df_row[\"algo\"] == \"mcpg_me\":\n",
    "        if df_row[\"proportion_mutation_ga\"] == 0 and df_row[\"greedy\"] == 0.5:\n",
    "            return \"mcpg_only_05\"\n",
    "        \n",
    "    if df_row[\"algo\"] == \"mcpg_me\":\n",
    "        if df_row[\"proportion_mutation_ga\"] == 0 and df_row[\"greedy\"] == 1:\n",
    "            return \"mcpg_only_1\"\n",
    "        \n",
    "\n",
    "    if df_row[\"algo\"] == \"mcpg_me\":\n",
    "        if df_row[\"proportion_mutation_ga\"] == 0.5 and df_row[\"greedy\"] == 0.5:\n",
    "            return \"mcpg_me_05\"\n",
    "        \n",
    "    if df_row[\"algo\"] == \"mcpg_me\":\n",
    "        if df_row[\"proportion_mutation_ga\"] == 0.5 and df_row[\"greedy\"] == 1:\n",
    "            return \"mcpg_me_1\"\n",
    "        \n",
    "\n",
    "    return df_row[\"algo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ant_omni_250\n",
      "ant_uni_250\n",
      "anttrap_omni_250\n",
      "hopper_uni_250\n",
      "walker2d_uni_250\n"
     ]
    }
   ],
   "source": [
    "results_dir = Path(\"fig1/output/\")\n",
    "EPISODE_LENGTH = 250\n",
    "df = get_df(results_dir, EPISODE_LENGTH)\n",
    "df['algo'] = df.apply(filter, axis=1)\n",
    "df = df[df[\"algo\"].isin(ALGO_LIST)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compare p-values wrt samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_wrt_samples(df, algo_list, env_list, metric_list, sample_reference=1_008_000):\n",
    "    p_value_list = [[metric, env, algo1, algo2] for metric in metric_list for env in env_list for idx, algo1 in enumerate(algo_list) for algo2 in algo_list[idx+1:]]\n",
    "    df = df[df[\"algo\"].isin(algo_list)]\n",
    "    df = df[df[\"env\"].isin(env_list)]\n",
    "    df = df[df[\"num_evaluations\"] <= sample_reference]\n",
    "\n",
    "    idx = df.groupby([\"env\", \"algo\", \"run\"])[\"iteration\"].idxmax()\n",
    "    df = df.loc[idx]\n",
    "\n",
    "    # Calculate percentage performance difference\n",
    "    performance_diff = pd.DataFrame(columns=[\"metric\", \"env\", \"algo_1\", \"algo_2\", \"percentage_diff\"])\n",
    "    for metric in metric_list:\n",
    "        for env in env_list:\n",
    "            means = df[df[\"env\"] == env].groupby(\"algo\")[metric].median()\n",
    "            for idx, algo_1 in enumerate(algo_list):\n",
    "                for algo_2 in algo_list[idx+1:]:\n",
    "                    mean_1 = means[algo_1]\n",
    "                    mean_2 = means[algo_2]\n",
    "                    if mean_1 > mean_2:\n",
    "                        percentage_diff = ((mean_1 - mean_2) / mean_2) * 100\n",
    "                        performance_diff.loc[len(performance_diff)] = {\n",
    "                            \"metric\": metric,\n",
    "                            \"env\": env,\n",
    "                            \"algo_1\": algo_1,\n",
    "                            \"algo_2\": algo_2,\n",
    "                            \"percentage_diff\": percentage_diff\n",
    "                        }\n",
    "                    elif mean_2 > mean_1:\n",
    "                        percentage_diff = ((mean_2 - mean_1) / mean_1) * 100\n",
    "                        performance_diff.loc[len(performance_diff)] = {\n",
    "                            \"metric\": metric,\n",
    "                            \"env\": env,\n",
    "                            \"algo_1\": algo_2,\n",
    "                            \"algo_2\": algo_1,\n",
    "                            \"percentage_diff\": percentage_diff\n",
    "                        }\n",
    "\n",
    "    # Calculate mean percentage difference across all environments\n",
    "    mean_performance_diff = performance_diff.groupby([\"metric\", \"algo_1\", \"algo_2\"])[\"percentage_diff\"].mean().reset_index()\n",
    "    mean_performance_diff.rename(columns={\"percentage_diff\": \"mean_percentage_diff\"}, inplace=True)\n",
    "# Compute p-values\n",
    "    p_value_df = pd.DataFrame(columns=[\"metric\", \"env\", \"algo_1\", \"algo_2\", \"p_value\"])\n",
    "    for metric in metric_list:\n",
    "        for env in env_list:\n",
    "            for algo_1 in algo_list:\n",
    "                for algo_2 in algo_list:\n",
    "                    stat = mannwhitneyu(\n",
    "                        df[(df[\"env\"] == env) & (df[\"algo\"] == algo_1)][metric],\n",
    "                        df[(df[\"env\"] == env) & (df[\"algo\"] == algo_2)][metric],\n",
    "                    )\n",
    "                    p_value_df.loc[len(p_value_df)] = {\"metric\": metric, \"env\": env, \"algo_1\": algo_1, \"algo_2\": algo_2, \"p_value\": stat.pvalue}\n",
    "\n",
    "    # Filter p-values\n",
    "    p_value_df.set_index([\"metric\", \"env\", \"algo_1\", \"algo_2\"], inplace=True)\n",
    "    p_value_df = p_value_df.loc[p_value_list]\n",
    "\n",
    "    # Correct p-values\n",
    "    p_value_df.reset_index(inplace=True)\n",
    "    p_value_df[\"p_value_corrected\"] = multipletests(p_value_df[\"p_value\"], method=\"holm\")[1]\n",
    "    p_value_df = p_value_df.pivot(index=[\"env\", \"algo_1\", \"algo_2\"], columns=\"metric\", values=\"p_value_corrected\")\n",
    "    p_value_df.columns.name = None\n",
    "    p_value_df = p_value_df.rename(columns={metric: \"p-value\" for metric in metric_list})\n",
    "\n",
    "    #p_value_df.to_csv(\"p_value_results.csv\")\n",
    "\n",
    "\n",
    "    return p_value_df, performance_diff, mean_performance_diff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      p-value\n",
      "env              algo_1  algo_2              \n",
      "hopper_uni_250   mcpg_me dcg_me  3.302298e-08\n",
      "walker2d_uni_250 mcpg_me dcg_me  3.302298e-08\n",
      "  metric               env  algo_1   algo_2  percentage_diff\n",
      "0   time    hopper_uni_250  dcg_me  mcpg_me       427.593184\n",
      "1   time  walker2d_uni_250  dcg_me  mcpg_me       385.124350\n",
      "  metric  algo_1   algo_2  mean_percentage_diff\n",
      "0   time  dcg_me  mcpg_me            406.358767\n"
     ]
    }
   ],
   "source": [
    "metric_list = [\"time\"]\n",
    "env_list = [\"hopper_uni_250\", \"walker2d_uni_250\"]\n",
    "algo_list = [\"mcpg_me\", \"dcg_me\"]\n",
    "p_value_df, performance_diff, mean_performance_diff = compare_wrt_samples(df, algo_list, env_list, metric_list)\n",
    "print(p_value_df)\n",
    "print(performance_diff)\n",
    "print(mean_performance_diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compare p-values wrt runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_wrt_runtime(df, algo_list, env_list, metric_list, ratio=1):\n",
    "    p_value_list = [[metric, env, algo1, algo2] for metric in metric_list for env in env_list for idx, algo1 in enumerate(algo_list) for algo2 in algo_list[idx+1:]]\n",
    "    env_one_mil = df[df[\"num_evaluations\"] <= 1_008_000]\n",
    "    max_time_per_env = env_one_mil.groupby('env')[\"time\"].max()\n",
    "    df = df[df.apply(lambda row: row['time'] <= max_time_per_env[row['env']] * ratio, axis=1)]\n",
    "    df = df[df[\"algo\"].isin(algo_list)]\n",
    "    df = df[df[\"env\"].isin(env_list)]\n",
    "    idx = df.groupby([\"env\", \"algo\", \"run\"])[\"time\"].idxmax()\n",
    "    df = df.loc[idx]\n",
    "\n",
    "    performance_diff = pd.DataFrame(columns=[\"metric\", \"env\", \"algo_1\", \"algo_2\", \"percentage_diff\"])\n",
    "    for metric in metric_list:\n",
    "        for env in env_list:\n",
    "            means = df[df[\"env\"] == env].groupby(\"algo\")[metric].median()\n",
    "            for idx, algo_1 in enumerate(algo_list):\n",
    "                for algo_2 in algo_list[idx+1:]:\n",
    "                    mean_1 = means[algo_1]\n",
    "                    mean_2 = means[algo_2]\n",
    "                    if mean_1 > mean_2:\n",
    "                        percentage_diff = ((mean_1 - mean_2) / mean_2) * 100\n",
    "                        performance_diff.loc[len(performance_diff)] = {\n",
    "                            \"metric\": metric,\n",
    "                            \"env\": env,\n",
    "                            \"algo_1\": algo_1,\n",
    "                            \"algo_2\": algo_2,\n",
    "                            \"percentage_diff\": percentage_diff\n",
    "                        }\n",
    "                    elif mean_2 > mean_1:\n",
    "                        percentage_diff = ((mean_2 - mean_1) / mean_1) * 100\n",
    "                        performance_diff.loc[len(performance_diff)] = {\n",
    "                            \"metric\": metric,\n",
    "                            \"env\": env,\n",
    "                            \"algo_1\": algo_2,\n",
    "                            \"algo_2\": algo_1,\n",
    "                            \"percentage_diff\": percentage_diff\n",
    "                        }\n",
    "\n",
    "    # Calculate mean percentage difference across all environments\n",
    "    mean_performance_diff = performance_diff.groupby([\"metric\", \"algo_1\", \"algo_2\"])[\"percentage_diff\"].mean().reset_index()\n",
    "    mean_performance_diff.rename(columns={\"percentage_diff\": \"mean_percentage_diff\"}, inplace=True)\n",
    "    # Compute p-values\n",
    "    p_value_df = pd.DataFrame(columns=[\"metric\", \"env\", \"algo_1\", \"algo_2\", \"p_value\"])\n",
    "    for metric in metric_list:\n",
    "        for env in env_list:\n",
    "            for algo_1 in algo_list:\n",
    "                for algo_2 in algo_list:\n",
    "                    stat = mannwhitneyu(\n",
    "                        df[(df[\"env\"] == env) & (df[\"algo\"] == algo_1)][metric],\n",
    "                        df[(df[\"env\"] == env) & (df[\"algo\"] == algo_2)][metric],\n",
    "                    )\n",
    "                    p_value_df.loc[len(p_value_df)] = {\"metric\": metric, \"env\": env, \"algo_1\": algo_1, \"algo_2\": algo_2, \"p_value\": stat.pvalue}\n",
    "\n",
    "    # Filter p-values\n",
    "    p_value_df.set_index([\"metric\", \"env\", \"algo_1\", \"algo_2\"], inplace=True)\n",
    "    p_value_df = p_value_df.loc[p_value_list]\n",
    "\n",
    "    # Correct p-values\n",
    "    p_value_df.reset_index(inplace=True)\n",
    "    p_value_df[\"p_value_corrected\"] = multipletests(p_value_df[\"p_value\"], method=\"holm\")[1]\n",
    "    p_value_df = p_value_df.pivot(index=[\"env\", \"algo_1\", \"algo_2\"], columns=\"metric\", values=\"p_value_corrected\")\n",
    "    p_value_df.columns.name = None\n",
    "    p_value_df = p_value_df.rename(columns={metric: \"p-value\" for metric in metric_list})\n",
    "\n",
    "    return p_value_df, performance_diff, mean_performance_diff\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      p-value\n",
      "env              algo_1  algo_2              \n",
      "ant_omni_250     dcg_me  pga_me  1.825329e-06\n",
      "                 mcpg_me dcg_me  7.231514e-08\n",
      "                         pga_me  7.231514e-08\n",
      "anttrap_omni_250 dcg_me  pga_me  2.355659e-06\n",
      "                 mcpg_me dcg_me  7.231514e-08\n",
      "                         pga_me  7.231514e-08\n",
      "     metric               env   algo_1  algo_2  percentage_diff\n",
      "0  qd_score      ant_omni_250  mcpg_me  dcg_me       415.616035\n",
      "1  qd_score      ant_omni_250  mcpg_me  pga_me       491.209650\n",
      "2  qd_score      ant_omni_250   dcg_me  pga_me        14.660837\n",
      "3  qd_score  anttrap_omni_250  mcpg_me  dcg_me       299.752862\n",
      "4  qd_score  anttrap_omni_250  mcpg_me  pga_me       355.304934\n",
      "5  qd_score  anttrap_omni_250   dcg_me  pga_me        13.896604\n",
      "     metric   algo_1  algo_2  mean_percentage_diff\n",
      "0  qd_score   dcg_me  pga_me             14.278720\n",
      "1  qd_score  mcpg_me  dcg_me            357.684449\n",
      "2  qd_score  mcpg_me  pga_me            423.257292\n"
     ]
    }
   ],
   "source": [
    "metric_list = [\"qd_score\"]\n",
    "env_list = [\"ant_omni_250\", \"anttrap_omni_250\"]\n",
    "algo_list = [\"mcpg_me\", \"dcg_me\",\"pga_me\"]\n",
    "p_value_df, performance_diff, mean_performance_diff = compare_wrt_runtime(df, algo_list, env_list, metric_list, ratio=0.05)\n",
    "print(p_value_df)\n",
    "print(performance_diff)\n",
    "print(mean_performance_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

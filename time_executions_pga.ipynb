{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  3 23:41:53 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A30                     Off | 00000000:01:00.0 Off |                   On |\n",
      "| N/A   33C    P0              30W / 165W |     50MiB / 24576MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                          |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                |        ECC|                       |\n",
      "|==================+================================+===========+=======================|\n",
      "|  0    1   0   0  |              25MiB / 11968MiB  | 28      0 |  2   0    2    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MPLCONFIGDIR'] = '/tmp/matplotlib'\n",
    "os.environ['WANDB_CACHE_DIR'] = '/tmp/wandb_cache'\n",
    "os.environ['JAX_LOG_COMPILATION'] = '1'\n",
    "#os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false'\n",
    "#os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import optax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "from qdax.core.containers.repertoire import Repertoire\n",
    "from qdax.core.emitters.emitter import Emitter, EmitterState\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition, ReplayBuffer\n",
    "from qdax.core.neuroevolution.losses.td3_loss import make_td3_loss_fn\n",
    "from qdax.core.neuroevolution.networks.networks import QModule\n",
    "from qdax.environments.base_wrappers import QDEnv\n",
    "from qdax.types import Descriptor, ExtraScores, Fitness, Genotype, Params, RNGKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from dataclasses import dataclass\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import serialization\n",
    "\n",
    "from qdax.core.containers.mapelites_repertoire import compute_cvt_centroids, MapElitesRepertoire\n",
    "from qdax.tasks.brax_envs import reset_based_scoring_function_brax_envs as scoring_function\n",
    "from qdax.environments import behavior_descriptor_extractor\n",
    "from qdax.core.map_elites_pga import MAPElites\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.core.emitters.pga_me_emitter import PGAMEConfig, PGAMEEmitter\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "from qdax.utils.plotting import plot_map_elites_results, plot_2d_map_elites_repertoire\n",
    "from set_up_brax import get_reward_offset_brax\n",
    "from qdax.utils.sampling import sampling \n",
    "from qdax.types import RNGKey, Genotype\n",
    "from typing import Any, Dict, Tuple, List, Callable\n",
    "import matplotlib.pyplot as plt\n",
    "from qdax import environments_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_name):\n",
    "    if env_name == \"hopper_uni\":\n",
    "        episode_length = 1000\n",
    "        \n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "    elif env_name == \"halfcheetah_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "        \n",
    "    elif env_name == \"walker2d_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\t\n",
    "    elif env_name == \"ant_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=True)\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, exclude_current_positions_from_observation=True)\t\n",
    "    '''\n",
    "    elif env_name == \"ant_omni\":\n",
    "        episode_length = 250\n",
    "        max_bd = 30.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=False)\t\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "        max_bd = 1.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length)\t\n",
    "    else:\n",
    "        ValueError(f\"Environment {env_name} not supported.\")\n",
    "    '''\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QualityPGConfig:\n",
    "    \"\"\"Configuration for QualityPG Emitter\"\"\"\n",
    "\n",
    "    env_batch_size: int = 256\n",
    "    num_critic_training_steps: int = 300\n",
    "    num_pg_training_steps: int = 100\n",
    "\n",
    "    # TD3 params\n",
    "    replay_buffer_size: int = 1000000\n",
    "    critic_hidden_layer_size: Tuple[int, ...] = (256, 256)\n",
    "    critic_learning_rate: float = 3e-4\n",
    "    actor_learning_rate: float = 3e-4\n",
    "    policy_learning_rate: float = 1e-3\n",
    "    noise_clip: float = 0.5\n",
    "    policy_noise: float = 0.2\n",
    "    discount: float = 0.99\n",
    "    reward_scaling: float = 1.0\n",
    "    batch_size: int = 100\n",
    "    soft_tau_update: float = 0.005\n",
    "    policy_delay: int = 2\n",
    "\n",
    "\n",
    "class QualityPGEmitterState(EmitterState):\n",
    "    \"\"\"Contains training state for the learner.\"\"\"\n",
    "\n",
    "    critic_params: Params\n",
    "    critic_optimizer_state: optax.OptState\n",
    "    actor_params: Params\n",
    "    actor_opt_state: optax.OptState\n",
    "    target_critic_params: Params\n",
    "    target_actor_params: Params\n",
    "    replay_buffer: ReplayBuffer\n",
    "    random_key: RNGKey\n",
    "    steps: jnp.ndarray\n",
    "\n",
    "\n",
    "class QualityPGEmitter(Emitter):\n",
    "    \"\"\"\n",
    "    A policy gradient emitter used to implement the Policy Gradient Assisted MAP-Elites\n",
    "    (PGA-Map-Elites) algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: QualityPGConfig,\n",
    "        policy_network: nn.Module,\n",
    "        env: QDEnv,\n",
    "    ) -> None:\n",
    "        self._config = config\n",
    "        self._env = env\n",
    "        self._policy_network = policy_network\n",
    "\n",
    "        # Init Critics\n",
    "        critic_network = QModule(\n",
    "            n_critics=2, hidden_layer_sizes=self._config.critic_hidden_layer_size\n",
    "        )\n",
    "        self._critic_network = critic_network\n",
    "\n",
    "        # Set up the losses and optimizers - return the opt states\n",
    "        self._policy_loss_fn, self._critic_loss_fn = make_td3_loss_fn(\n",
    "            policy_fn=policy_network.apply,\n",
    "            critic_fn=critic_network.apply,\n",
    "            reward_scaling=self._config.reward_scaling,\n",
    "            discount=self._config.discount,\n",
    "            noise_clip=self._config.noise_clip,\n",
    "            policy_noise=self._config.policy_noise,\n",
    "        )\n",
    "\n",
    "        # Init optimizers\n",
    "        self._actor_optimizer = optax.adam(\n",
    "            learning_rate=self._config.actor_learning_rate\n",
    "        )\n",
    "        self._critic_optimizer = optax.adam(\n",
    "            learning_rate=self._config.critic_learning_rate\n",
    "        )\n",
    "        self._policies_optimizer = optax.adam(\n",
    "            learning_rate=self._config.policy_learning_rate\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def batch_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            the batch size emitted by the emitter.\n",
    "        \"\"\"\n",
    "        return self._config.env_batch_size\n",
    "\n",
    "    @property\n",
    "    def use_all_data(self) -> bool:\n",
    "        \"\"\"Whether to use all data or not when used along other emitters.\n",
    "\n",
    "        QualityPGEmitter uses the transitions from the genotypes that were generated\n",
    "        by other emitters.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def init(\n",
    "        self,\n",
    "        random_key: RNGKey,\n",
    "        repertoire: Repertoire,\n",
    "        genotypes: Genotype,\n",
    "        fitnesses: Fitness,\n",
    "        descriptors: Descriptor,\n",
    "        extra_scores: ExtraScores,\n",
    "    ) -> Tuple[QualityPGEmitterState, RNGKey]:\n",
    "        \"\"\"Initializes the emitter state.\n",
    "\n",
    "        Args:\n",
    "            genotypes: The initial population.\n",
    "            random_key: A random key.\n",
    "\n",
    "        Returns:\n",
    "            The initial state of the PGAMEEmitter, a new random key.\n",
    "        \"\"\"\n",
    "\n",
    "        observation_size = self._env.observation_size\n",
    "        action_size = self._env.action_size\n",
    "        descriptor_size = self._env.state_descriptor_length\n",
    "\n",
    "        # Initialise critic, greedy actor and population\n",
    "        random_key, subkey = jax.random.split(random_key)\n",
    "        fake_obs = jnp.zeros(shape=(observation_size,))\n",
    "        fake_action = jnp.zeros(shape=(action_size,))\n",
    "        critic_params = self._critic_network.init(\n",
    "            subkey, obs=fake_obs, actions=fake_action\n",
    "        )\n",
    "        target_critic_params = jax.tree_util.tree_map(lambda x: x, critic_params)\n",
    "\n",
    "        actor_params = jax.tree_util.tree_map(lambda x: x[0], genotypes)\n",
    "        target_actor_params = jax.tree_util.tree_map(lambda x: x[0], genotypes)\n",
    "\n",
    "        # Prepare init optimizer states\n",
    "        critic_optimizer_state = self._critic_optimizer.init(critic_params)\n",
    "        actor_optimizer_state = self._actor_optimizer.init(actor_params)\n",
    "\n",
    "        # Initialize replay buffer\n",
    "        dummy_transition = QDTransition.init_dummy(\n",
    "            observation_dim=observation_size,\n",
    "            action_dim=action_size,\n",
    "            descriptor_dim=descriptor_size,\n",
    "        )\n",
    "\n",
    "        replay_buffer = ReplayBuffer.init(\n",
    "            buffer_size=self._config.replay_buffer_size, transition=dummy_transition\n",
    "        )\n",
    "\n",
    "        # get the transitions out of the dictionary\n",
    "        assert \"transitions\" in extra_scores.keys(), \"Missing transitions or wrong key\"\n",
    "        transitions = extra_scores[\"transitions\"]\n",
    "\n",
    "        # add transitions in the replay buffer\n",
    "        replay_buffer = replay_buffer.insert(transitions)\n",
    "\n",
    "        # Initial training state\n",
    "        random_key, subkey = jax.random.split(random_key)\n",
    "        emitter_state = QualityPGEmitterState(\n",
    "            critic_params=critic_params,\n",
    "            critic_optimizer_state=critic_optimizer_state,\n",
    "            actor_params=actor_params,\n",
    "            actor_opt_state=actor_optimizer_state,\n",
    "            target_critic_params=target_critic_params,\n",
    "            target_actor_params=target_actor_params,\n",
    "            replay_buffer=replay_buffer,\n",
    "            random_key=subkey,\n",
    "            steps=jnp.array(0),\n",
    "        )\n",
    "\n",
    "        return emitter_state, random_key\n",
    "\n",
    "    @partial(\n",
    "        jax.jit,\n",
    "        static_argnames=(\"self\",),\n",
    "    )\n",
    "    def emit(\n",
    "        self,\n",
    "        repertoire: Repertoire,\n",
    "        emitter_state: QualityPGEmitterState,\n",
    "        random_key: RNGKey,\n",
    "    ) -> Tuple[Genotype, ExtraScores, RNGKey]:\n",
    "        \"\"\"Do a step of PG emission.\n",
    "\n",
    "        Args:\n",
    "            repertoire: the current repertoire of genotypes\n",
    "            emitter_state: the state of the emitter used\n",
    "            random_key: a random key\n",
    "\n",
    "        Returns:\n",
    "            A batch of offspring, the new emitter state and a new key.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = self._config.env_batch_size\n",
    "\n",
    "        # sample parents\n",
    "        mutation_pg_batch_size = int(batch_size - 1)\n",
    "        print(type(repertoire))\n",
    "        parents, random_key = repertoire.sample(random_key, mutation_pg_batch_size)\n",
    "\n",
    "        # apply the pg mutation\n",
    "        offsprings_pg = self.emit_pg(emitter_state, parents)\n",
    "\n",
    "        # get the actor (greedy actor)\n",
    "        offspring_actor = self.emit_actor(emitter_state)\n",
    "\n",
    "        # add dimension for concatenation\n",
    "        offspring_actor = jax.tree_util.tree_map(\n",
    "            lambda x: jnp.expand_dims(x, axis=0), offspring_actor\n",
    "        )\n",
    "\n",
    "        # gather offspring\n",
    "        genotypes = jax.tree_util.tree_map(\n",
    "            lambda x, y: jnp.concatenate([x, y], axis=0),\n",
    "            offsprings_pg,\n",
    "            offspring_actor,\n",
    "        )\n",
    "\n",
    "        return genotypes, {}, random_key\n",
    "\n",
    "    @partial(\n",
    "        jax.jit,\n",
    "        static_argnames=(\"self\",),\n",
    "    )\n",
    "    def emit_pg(\n",
    "        self, emitter_state: QualityPGEmitterState, parents: Genotype\n",
    "    ) -> Genotype:\n",
    "        \"\"\"Emit the offsprings generated through pg mutation.\n",
    "\n",
    "        Args:\n",
    "            emitter_state: current emitter state, contains critic and\n",
    "                replay buffer.\n",
    "            parents: the parents selected to be applied gradients in order\n",
    "                to mutate towards better performance.\n",
    "\n",
    "        Returns:\n",
    "            A new set of offsprings.\n",
    "        \"\"\"\n",
    "        mutation_fn = partial(\n",
    "            self._mutation_function_pg,\n",
    "            emitter_state=emitter_state,\n",
    "        )\n",
    "        offsprings = jax.vmap(mutation_fn)(parents)\n",
    "\n",
    "        return offsprings\n",
    "\n",
    "    @partial(\n",
    "        jax.jit,\n",
    "        static_argnames=(\"self\",),\n",
    "    )\n",
    "    def emit_actor(self, emitter_state: QualityPGEmitterState) -> Genotype:\n",
    "        \"\"\"Emit the greedy actor.\n",
    "\n",
    "        Simply needs to be retrieved from the emitter state.\n",
    "\n",
    "        Args:\n",
    "            emitter_state: the current emitter state, it stores the\n",
    "                greedy actor.\n",
    "\n",
    "        Returns:\n",
    "            The parameters of the actor.\n",
    "        \"\"\"\n",
    "        return emitter_state.actor_params\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def state_update(\n",
    "        self,\n",
    "        emitter_state: QualityPGEmitterState,\n",
    "        repertoire: Optional[Repertoire],\n",
    "        genotypes: Optional[Genotype],\n",
    "        fitnesses: Optional[Fitness],\n",
    "        descriptors: Optional[Descriptor],\n",
    "        extra_scores: ExtraScores,\n",
    "    ) -> QualityPGEmitterState:\n",
    "        \"\"\"This function gives an opportunity to update the emitter state\n",
    "        after the genotypes have been scored.\n",
    "\n",
    "        Here it is used to fill the Replay Buffer with the transitions\n",
    "        from the scoring of the genotypes, and then the training of the\n",
    "        critic/actor happens. Hence the params of critic/actor are updated,\n",
    "        as well as their optimizer states.\n",
    "\n",
    "        Args:\n",
    "            emitter_state: current emitter state.\n",
    "            repertoire: the current genotypes repertoire\n",
    "            genotypes: unused here - but compulsory in the signature.\n",
    "            fitnesses: unused here - but compulsory in the signature.\n",
    "            descriptors: unused here - but compulsory in the signature.\n",
    "            extra_scores: extra information coming from the scoring function,\n",
    "                this contains the transitions added to the replay buffer.\n",
    "\n",
    "        Returns:\n",
    "            New emitter state where the replay buffer has been filled with\n",
    "            the new experienced transitions.\n",
    "        \"\"\"\n",
    "        # get the transitions out of the dictionary\n",
    "        assert \"transitions\" in extra_scores.keys(), \"Missing transitions or wrong key\"\n",
    "        transitions = extra_scores[\"transitions\"]\n",
    "\n",
    "        # add transitions in the replay buffer\n",
    "        replay_buffer = emitter_state.replay_buffer.insert(transitions)\n",
    "        emitter_state = emitter_state.replace(replay_buffer=replay_buffer)\n",
    "\n",
    "        def scan_train_critics(\n",
    "            carry: QualityPGEmitterState, unused: Any\n",
    "        ) -> Tuple[QualityPGEmitterState, Any]:\n",
    "            emitter_state = carry\n",
    "            new_emitter_state = self._train_critics(emitter_state)\n",
    "            return new_emitter_state, ()\n",
    "\n",
    "        # Train critics and greedy actor\n",
    "        emitter_state, _ = jax.lax.scan(\n",
    "            scan_train_critics,\n",
    "            emitter_state,\n",
    "            (),\n",
    "            length=self._config.num_critic_training_steps,\n",
    "        )\n",
    "\n",
    "        return emitter_state  # type: ignore\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _train_critics(\n",
    "        self, emitter_state: QualityPGEmitterState\n",
    "    ) -> QualityPGEmitterState:\n",
    "        \"\"\"Apply one gradient step to critics and to the greedy actor\n",
    "        (contained in carry in training_state), then soft update target critics\n",
    "        and target actor.\n",
    "\n",
    "        Those updates are very similar to those made in TD3.\n",
    "\n",
    "        Args:\n",
    "            emitter_state: actual emitter state\n",
    "\n",
    "        Returns:\n",
    "            New emitter state where the critic and the greedy actor have been\n",
    "            updated. Optimizer states have also been updated in the process.\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample a batch of transitions in the buffer\n",
    "        random_key = emitter_state.random_key\n",
    "        replay_buffer = emitter_state.replay_buffer\n",
    "        transitions, random_key = replay_buffer.sample(\n",
    "            random_key, sample_size=self._config.batch_size\n",
    "        )\n",
    "\n",
    "        # Update Critic\n",
    "        (\n",
    "            critic_optimizer_state,\n",
    "            critic_params,\n",
    "            target_critic_params,\n",
    "            random_key,\n",
    "        ) = self._update_critic(\n",
    "            critic_params=emitter_state.critic_params,\n",
    "            target_critic_params=emitter_state.target_critic_params,\n",
    "            target_actor_params=emitter_state.target_actor_params,\n",
    "            critic_optimizer_state=emitter_state.critic_optimizer_state,\n",
    "            transitions=transitions,\n",
    "            random_key=random_key,\n",
    "        )\n",
    "\n",
    "        # Update greedy actor\n",
    "        (actor_optimizer_state, actor_params, target_actor_params,) = jax.lax.cond(\n",
    "            emitter_state.steps % self._config.policy_delay == 0,\n",
    "            lambda x: self._update_actor(*x),\n",
    "            lambda _: (\n",
    "                emitter_state.actor_opt_state,\n",
    "                emitter_state.actor_params,\n",
    "                emitter_state.target_actor_params,\n",
    "            ),\n",
    "            operand=(\n",
    "                emitter_state.actor_params,\n",
    "                emitter_state.actor_opt_state,\n",
    "                emitter_state.target_actor_params,\n",
    "                emitter_state.critic_params,\n",
    "                transitions,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Create new training state\n",
    "        new_emitter_state = emitter_state.replace(\n",
    "            critic_params=critic_params,\n",
    "            critic_optimizer_state=critic_optimizer_state,\n",
    "            actor_params=actor_params,\n",
    "            actor_opt_state=actor_optimizer_state,\n",
    "            target_critic_params=target_critic_params,\n",
    "            target_actor_params=target_actor_params,\n",
    "            random_key=random_key,\n",
    "            steps=emitter_state.steps + 1,\n",
    "            replay_buffer=replay_buffer,\n",
    "        )\n",
    "\n",
    "        return new_emitter_state  # type: ignore\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _update_critic(\n",
    "        self,\n",
    "        critic_params: Params,\n",
    "        target_critic_params: Params,\n",
    "        target_actor_params: Params,\n",
    "        critic_optimizer_state: Params,\n",
    "        transitions: QDTransition,\n",
    "        random_key: RNGKey,\n",
    "    ) -> Tuple[Params, Params, Params, RNGKey]:\n",
    "\n",
    "        # compute loss and gradients\n",
    "        random_key, subkey = jax.random.split(random_key)\n",
    "        critic_loss, critic_gradient = jax.value_and_grad(self._critic_loss_fn)(\n",
    "            critic_params,\n",
    "            target_actor_params,\n",
    "            target_critic_params,\n",
    "            transitions,\n",
    "            subkey,\n",
    "        )\n",
    "        critic_updates, critic_optimizer_state = self._critic_optimizer.update(\n",
    "            critic_gradient, critic_optimizer_state\n",
    "        )\n",
    "\n",
    "        # update critic\n",
    "        critic_params = optax.apply_updates(critic_params, critic_updates)\n",
    "\n",
    "        # Soft update of target critic network\n",
    "        target_critic_params = jax.tree_map(\n",
    "            lambda x1, x2: (1.0 - self._config.soft_tau_update) * x1\n",
    "            + self._config.soft_tau_update * x2,\n",
    "            target_critic_params,\n",
    "            critic_params,\n",
    "        )\n",
    "\n",
    "        return critic_optimizer_state, critic_params, target_critic_params, random_key\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _update_actor(\n",
    "        self,\n",
    "        actor_params: Params,\n",
    "        actor_opt_state: optax.OptState,\n",
    "        target_actor_params: Params,\n",
    "        critic_params: Params,\n",
    "        transitions: QDTransition,\n",
    "    ) -> Tuple[optax.OptState, Params, Params]:\n",
    "\n",
    "        # Update greedy actor\n",
    "        policy_loss, policy_gradient = jax.value_and_grad(self._policy_loss_fn)(\n",
    "            actor_params,\n",
    "            critic_params,\n",
    "            transitions,\n",
    "        )\n",
    "        (\n",
    "            policy_updates,\n",
    "            actor_optimizer_state,\n",
    "        ) = self._actor_optimizer.update(policy_gradient, actor_opt_state)\n",
    "        actor_params = optax.apply_updates(actor_params, policy_updates)\n",
    "\n",
    "        # Soft update of target greedy actor\n",
    "        target_actor_params = jax.tree_map(\n",
    "            lambda x1, x2: (1.0 - self._config.soft_tau_update) * x1\n",
    "            + self._config.soft_tau_update * x2,\n",
    "            target_actor_params,\n",
    "            actor_params,\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            actor_optimizer_state,\n",
    "            actor_params,\n",
    "            target_actor_params,\n",
    "        )\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _mutation_function_pg(\n",
    "        self,\n",
    "        policy_params: Genotype,\n",
    "        emitter_state: QualityPGEmitterState,\n",
    "    ) -> Genotype:\n",
    "        \"\"\"Apply pg mutation to a policy via multiple steps of gradient descent.\n",
    "        First, update the rewards to be diversity rewards, then apply the gradient\n",
    "        steps.\n",
    "\n",
    "        Args:\n",
    "            policy_params: a policy, supposed to be a differentiable neural\n",
    "                network.\n",
    "            emitter_state: the current state of the emitter, containing among others,\n",
    "                the replay buffer, the critic.\n",
    "\n",
    "        Returns:\n",
    "            The updated params of the neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define new policy optimizer state\n",
    "        policy_optimizer_state = self._policies_optimizer.init(policy_params)\n",
    "\n",
    "        def scan_train_policy(\n",
    "            carry: Tuple[QualityPGEmitterState, Genotype, optax.OptState],\n",
    "            unused: Any,\n",
    "        ) -> Tuple[Tuple[QualityPGEmitterState, Genotype, optax.OptState], Any]:\n",
    "            emitter_state, policy_params, policy_optimizer_state = carry\n",
    "            (\n",
    "                new_emitter_state,\n",
    "                new_policy_params,\n",
    "                new_policy_optimizer_state,\n",
    "            ) = self._train_policy(\n",
    "                emitter_state,\n",
    "                policy_params,\n",
    "                policy_optimizer_state,\n",
    "            )\n",
    "            return (\n",
    "                new_emitter_state,\n",
    "                new_policy_params,\n",
    "                new_policy_optimizer_state,\n",
    "            ), ()\n",
    "\n",
    "        (emitter_state, policy_params, policy_optimizer_state,), _ = jax.lax.scan(\n",
    "            scan_train_policy,\n",
    "            (emitter_state, policy_params, policy_optimizer_state),\n",
    "            (),\n",
    "            length=self._config.num_pg_training_steps,\n",
    "        )\n",
    "\n",
    "        return policy_params\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _train_policy(\n",
    "        self,\n",
    "        emitter_state: QualityPGEmitterState,\n",
    "        policy_params: Params,\n",
    "        policy_optimizer_state: optax.OptState,\n",
    "    ) -> Tuple[QualityPGEmitterState, Params, optax.OptState]:\n",
    "        \"\"\"Apply one gradient step to a policy (called policy_params).\n",
    "\n",
    "        Args:\n",
    "            emitter_state: current state of the emitter.\n",
    "            policy_params: parameters corresponding to the weights and bias of\n",
    "                the neural network that defines the policy.\n",
    "\n",
    "        Returns:\n",
    "            The new emitter state and new params of the NN.\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample a batch of transitions in the buffer\n",
    "        random_key = emitter_state.random_key\n",
    "        replay_buffer = emitter_state.replay_buffer\n",
    "        transitions, random_key = replay_buffer.sample(\n",
    "            random_key, sample_size=self._config.batch_size\n",
    "        )\n",
    "\n",
    "        # update policy\n",
    "        policy_optimizer_state, policy_params = self._update_policy(\n",
    "            critic_params=emitter_state.critic_params,\n",
    "            policy_optimizer_state=policy_optimizer_state,\n",
    "            policy_params=policy_params,\n",
    "            transitions=transitions,\n",
    "        )\n",
    "\n",
    "        # Create new training state\n",
    "        new_emitter_state = emitter_state.replace(\n",
    "            random_key=random_key,\n",
    "            replay_buffer=replay_buffer,\n",
    "        )\n",
    "\n",
    "        return new_emitter_state, policy_params, policy_optimizer_state\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _update_policy(\n",
    "        self,\n",
    "        critic_params: Params,\n",
    "        policy_optimizer_state: optax.OptState,\n",
    "        policy_params: Params,\n",
    "        transitions: QDTransition,\n",
    "    ) -> Tuple[optax.OptState, Params]:\n",
    "\n",
    "        # compute loss\n",
    "        _policy_loss, policy_gradient = jax.value_and_grad(self._policy_loss_fn)(\n",
    "            policy_params,\n",
    "            critic_params,\n",
    "            transitions,\n",
    "        )\n",
    "        # Compute gradient and update policies\n",
    "        (\n",
    "            policy_updates,\n",
    "            policy_optimizer_state,\n",
    "        ) = self._policies_optimizer.update(policy_gradient, policy_optimizer_state)\n",
    "        policy_params = optax.apply_updates(policy_params, policy_updates)\n",
    "\n",
    "        return policy_optimizer_state, policy_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for the experiment script\n",
    "    \"\"\"\n",
    "    # Basic Configuration\n",
    "    seed: int\n",
    "    num_iterations: int\n",
    "    num_samples: int\n",
    "    env_batch_size: int  # Used in GA emitter and PG emitter\n",
    "    batch_size: int\n",
    "\n",
    "    # Archive Configuration\n",
    "    num_init_cvt_samples: int\n",
    "    num_centroids: int\n",
    "    policy_hidden_layer_sizes: Tuple[int, ...]\n",
    "    proportion_mutation_ga: float\n",
    "\n",
    "    # GA Emitter Configuration\n",
    "    iso_sigma: float\n",
    "    line_sigma: float\n",
    "\n",
    "    # PG Emitter Configuration\n",
    "    critic_hidden_layer_size: Tuple[int, ...]\n",
    "    num_critic_training_steps: int\n",
    "    num_pg_training_steps: int\n",
    "    replay_buffer_size: int\n",
    "    discount: float\n",
    "    reward_scaling: float\n",
    "    critic_learning_rate: float\n",
    "    actor_learning_rate: float\n",
    "    policy_learning_rate: float\n",
    "    noise_clip: float\n",
    "    policy_noise: float\n",
    "    soft_tau_update: float\n",
    "    policy_delay: int\n",
    "\n",
    "config = Config(\n",
    "    seed=0,\n",
    "    num_iterations=200,\n",
    "    num_samples=32,\n",
    "    batch_size=100,\n",
    "    env_batch_size=5096,\n",
    "    num_init_cvt_samples=50000,\n",
    "    num_centroids=1024,\n",
    "    policy_hidden_layer_sizes=[128, 128],\n",
    "    proportion_mutation_ga=0.5,\n",
    "    iso_sigma=0.005,\n",
    "    line_sigma=0.05,\n",
    "    critic_hidden_layer_size=[256, 256],\n",
    "    num_critic_training_steps=3000,\n",
    "    num_pg_training_steps=150,\n",
    "    replay_buffer_size=5096000, #2048000, #1_000_000,\n",
    "    discount=0.99,\n",
    "    reward_scaling=1.0,\n",
    "    critic_learning_rate=3e-4,\n",
    "    actor_learning_rate=3e-4,\n",
    "    policy_learning_rate=5e-3,\n",
    "    noise_clip=0.5,\n",
    "    policy_noise=0.2,\n",
    "    soft_tau_update=0.005,\n",
    "    policy_delay=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def tree_flatten(self):\\n        return ((self.mean, self.var, self.count, self.return_val), self.size)\\n    \\n    @classmethod\\n    def tree_unflatten(cls, aux_data, children):\\n        size = aux_data\\n        mean, var, count, return_val = children\\n        return cls(size=size, mean=mean, var=var, count=count, return_val=return_val)\\n    \\n        \\ntree_util.register_pytree_node(\\n    RewardNormalizer,\\n    RewardNormalizer.tree_flatten,\\n    RewardNormalizer.tree_unflatten\\n)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class Normalizer:\n",
    "    def __init__(self, size, epsilon=1e-8):\n",
    "        self.size = size\n",
    "        self.mean = jnp.zeros(size)\n",
    "        self.var = jnp.ones(size)\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = jnp.mean(x, axis=0)\n",
    "        batch_var = jnp.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "\n",
    "        self.mean, self.var, self.count = self._update_mean_var_count(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x - self.mean) / jnp.sqrt(self.var + EPS)\n",
    "\n",
    "    def _update_mean_var_count(self, mean, var, count, batch_mean, batch_var, batch_count):\n",
    "        delta = batch_mean - mean\n",
    "        tot_count = count + batch_count\n",
    "\n",
    "        new_mean = mean + delta * batch_count / tot_count\n",
    "        m_a = var * count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        return new_mean, new_var, new_count\n",
    "'''\n",
    "'''\n",
    "class Normalizer:\n",
    "    def __init__(self, size, epsilon=1e-8):\n",
    "        self.size = size  # Expecting size to be the dimensionality of the observation features (z)\n",
    "        self.mean = jnp.zeros(size)\n",
    "        self.var = jnp.ones(size)\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        # Flatten the first two dimensions (x, y) to treat as a single batch dimension\n",
    "        flat_x = x.reshape(-1, self.size)\n",
    "        batch_mean = jnp.mean(flat_x, axis=0)\n",
    "        batch_var = jnp.var(flat_x, axis=0)\n",
    "        batch_count = flat_x.shape[0]\n",
    "\n",
    "        self.mean, self.var, self.count = self._update_mean_var_count(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        # Normalize maintaining the original shape, using broadcasting\n",
    "        return (x - self.mean) / jnp.sqrt(self.var + 1e-8)\n",
    "\n",
    "    def _update_mean_var_count(self, mean, var, count, batch_mean, batch_var, batch_count):\n",
    "        delta = batch_mean - mean\n",
    "        tot_count = count + batch_count\n",
    "\n",
    "        new_mean = mean + delta * batch_count / tot_count\n",
    "        m_a = var * count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        return new_mean, new_var, new_count\n",
    "        \n",
    "'''\n",
    "\n",
    "from jax import tree_util\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Normalizer:\n",
    "    size: int\n",
    "    mean: jnp.ndarray = None\n",
    "    var: jnp.ndarray = None\n",
    "    count: jnp.ndarray = 1e-4\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.mean is None:\n",
    "            self.mean = jnp.zeros(self.size)\n",
    "        if self.var is None:\n",
    "            self.var = jnp.ones(self.size)\n",
    "            \n",
    "    def update(self, x):\n",
    "        # Flatten the first two dimensions (x, y) to treat as a single batch dimension\n",
    "        flat_x = x.reshape(-1, self.size)\n",
    "        batch_mean = jnp.mean(flat_x, axis=0)\n",
    "        batch_var = jnp.var(flat_x, axis=0)\n",
    "        batch_count = flat_x.shape[0]\n",
    "\n",
    "        new_mean, new_var, new_count = self._update_mean_var_count(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
    "        \n",
    "        return self.replace(mean=new_mean, var=new_var, count=new_count)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        # Normalize maintaining the original shape, using broadcasting\n",
    "        return (x - self.mean) / jnp.sqrt(self.var + 1e-8)\n",
    "\n",
    "    def _update_mean_var_count(self, mean, var, count, batch_mean, batch_var, batch_count):\n",
    "        delta = batch_mean - mean\n",
    "        tot_count = count + batch_count\n",
    "\n",
    "        new_mean = mean + delta * batch_count / tot_count\n",
    "        m_a = var * count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        return new_mean, new_var, new_count\n",
    "'''\n",
    "    def tree_flatten(self):\n",
    "        return ((self.mean, self.var, self.count), self.size)\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        size = aux_data\n",
    "        mean, var, count = children\n",
    "        return cls(size=size, mean=mean, var=var, count=count)\n",
    "\n",
    "# Register Normalizer as a pytree node with JAX\n",
    "tree_util.register_pytree_node(\n",
    "    Normalizer,\n",
    "    Normalizer.tree_flatten,\n",
    "    Normalizer.tree_unflatten\n",
    ")\n",
    "'''\n",
    "\n",
    "@dataclass\n",
    "class RewardNormalizer:\n",
    "    size: int\n",
    "    mean: jnp.ndarray = 0.0\n",
    "    var: jnp.ndarray = 1.0\n",
    "    count: jnp.ndarray = 1e-4\n",
    "    return_val: jnp.ndarray = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.return_val is None:\n",
    "            self.return_val = jnp.zeros((self.size,))\n",
    "\n",
    "         \n",
    "    def update(self, reward, done, gamma=0.99):\n",
    "        \n",
    "        def _update_column_scan(carry, x):\n",
    "            mean, var, count, return_val = carry\n",
    "            (reward, done) = x\n",
    "            \n",
    "            # Update the return value\n",
    "            new_return_val = reward + gamma * return_val * (1 - done)\n",
    "            \n",
    "            # Update the mean, var, and count\n",
    "            batch_mean = jnp.mean(new_return_val, axis=0)\n",
    "            batch_var = jnp.var(new_return_val, axis=0)\n",
    "            batch_count = new_return_val.shape[0]\n",
    "            \n",
    "            delta = batch_mean - mean\n",
    "            tot_count = count + batch_count\n",
    "            \n",
    "            new_mean = mean + delta * batch_count / tot_count\n",
    "            m_a = var * count\n",
    "            m_b = batch_var * batch_count\n",
    "            M2 = m_a + m_b + jnp.square(delta) * count * batch_count / tot_count\n",
    "            new_var = M2 / tot_count\n",
    "            new_count = tot_count\n",
    "            \n",
    "            normalized_reward = reward / jnp.sqrt(new_var + 1e-8)\n",
    "            \n",
    "            return (new_mean, new_var, new_count, new_return_val), normalized_reward\n",
    "        \n",
    "        (new_mean, new_var, new_count, _), normalized_rewards = jax.lax.scan(\n",
    "            _update_column_scan,\n",
    "            (self.mean, self.var, self.count, self.return_val),\n",
    "            (reward.T, done.T),\n",
    "        )\n",
    "\n",
    "        \n",
    "        return self.replace(mean=new_mean, var=new_var, count=new_count), normalized_rewards.T\n",
    "\n",
    "'''\n",
    "    def tree_flatten(self):\n",
    "        return ((self.mean, self.var, self.count, self.return_val), self.size)\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        size = aux_data\n",
    "        mean, var, count, return_val = children\n",
    "        return cls(size=size, mean=mean, var=var, count=count, return_val=return_val)\n",
    "    \n",
    "        \n",
    "tree_util.register_pytree_node(\n",
    "    RewardNormalizer,\n",
    "    RewardNormalizer.tree_flatten,\n",
    "    RewardNormalizer.tree_unflatten\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mobservation_size\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.observation_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in policy_network:  21256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/km2120/me-with-sample-based-drl/qdax/core/map_elites_pga.py:224: UserWarning: This type of repertoire does not store the extra scores computed by the scoring function\n",
      "  repertoire = MapElitesRepertoire.init(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'qdax.core.containers.mapelites_repertoire.MapElitesRepertoire'>\n"
     ]
    }
   ],
   "source": [
    "random_key = jax.random.PRNGKey(config.seed)\n",
    "\n",
    "# Init environment\n",
    "env = get_env('ant_uni')\n",
    "reset_fn = jax.jit(env.reset)\n",
    "normalizer = Normalizer(env.observation_size)\n",
    "reward_normalizer = RewardNormalizer(config.env_batch_size)\n",
    "\n",
    "# Compute the centroids\n",
    "centroids, random_key = compute_cvt_centroids(\n",
    "    num_descriptors=env.behavior_descriptor_length,\n",
    "    num_init_cvt_samples=config.num_init_cvt_samples,\n",
    "    num_centroids=config.num_centroids,\n",
    "    minval=0,\n",
    "    maxval=1,\n",
    "    random_key=random_key,\n",
    ")\n",
    "\n",
    "# Init policy network\n",
    "\n",
    "policy_layer_sizes = config.policy_hidden_layer_sizes + [env.action_size]\n",
    "policy_network = MLP(\n",
    "    layer_sizes=policy_layer_sizes,\n",
    "    kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "    final_activation=jnp.tanh,\n",
    ")\n",
    "\n",
    "# Init population of controllers\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, num=config.env_batch_size)\n",
    "fake_batch_obs = jnp.zeros(shape=(config.env_batch_size, env.observation_size))\n",
    "init_params = jax.vmap(policy_network.init)(keys, fake_batch_obs)\n",
    "\n",
    "param_count = sum(x[0].size for x in jax.tree_util.tree_leaves(init_params))\n",
    "print(\"Number of parameters in policy_network: \", param_count)\n",
    "\n",
    "# Define the fonction to play a step with the policy in the environment\n",
    "@jax.jit\n",
    "def play_step_fn(env_state, policy_params, random_key):\n",
    "    actions = policy_network.apply(policy_params, env_state.obs)\n",
    "    state_desc = env_state.info[\"state_descriptor\"]\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = QDTransition(\n",
    "        obs=env_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        actions=actions,\n",
    "        state_desc=state_desc,\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "        #desc=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "        #desc_prime=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "    )\n",
    "\n",
    "    return next_state, policy_params, random_key, transition\n",
    "\n",
    "# Prepare the scoring function\n",
    "bd_extraction_fn = behavior_descriptor_extractor['ant_uni']\n",
    "scoring_fn = functools.partial(\n",
    "    scoring_function,\n",
    "    episode_length=env.episode_length,\n",
    "    play_reset_fn=reset_fn,\n",
    "    play_step_fn=play_step_fn,\n",
    "    behavior_descriptor_extractor=bd_extraction_fn,\n",
    "    #normalizer=normalizer,\n",
    "    #reward_normalizer=reward_normalizer,\n",
    ")\n",
    "\n",
    "\n",
    "me_scoring_fn = functools.partial(\n",
    "sampling,\n",
    "scoring_fn=scoring_fn,\n",
    "num_samples=config.num_samples,\n",
    ")\n",
    "\n",
    "reward_offset = 0\n",
    "\n",
    "metrics_function = functools.partial(\n",
    "    default_qd_metrics,\n",
    "    qd_offset=reward_offset * env.episode_length,\n",
    ")\n",
    "\n",
    "# Define the PG-emitter config\n",
    "pga_emitter_config = PGAMEConfig(\n",
    "    env_batch_size=config.env_batch_size,\n",
    "    proportion_mutation_ga=config.proportion_mutation_ga,\n",
    "    critic_hidden_layer_size=config.critic_hidden_layer_size,\n",
    "    num_critic_training_steps=config.num_critic_training_steps,\n",
    "    num_pg_training_steps=config.num_pg_training_steps,\n",
    "    batch_size=config.batch_size,\n",
    "    replay_buffer_size=config.replay_buffer_size,\n",
    "    discount=config.discount,\n",
    "    reward_scaling=config.reward_scaling,\n",
    "    critic_learning_rate=config.critic_learning_rate,\n",
    "    #actor_learning_rate=config.algo.actor_learning_rate,\n",
    "    policy_learning_rate=config.policy_learning_rate,\n",
    "    noise_clip=config.noise_clip,\n",
    "    policy_noise=config.policy_noise,\n",
    "    soft_tau_update=config.soft_tau_update,\n",
    "    policy_delay=config.policy_delay,\n",
    ")\n",
    "\n",
    "# Get the emitter\n",
    "variation_fn = functools.partial(\n",
    "    isoline_variation, iso_sigma=config.iso_sigma, line_sigma=config.line_sigma\n",
    ")\n",
    "\n",
    "pg_emitter = PGAMEEmitter(\n",
    "    config=pga_emitter_config,\n",
    "    policy_network=policy_network,\n",
    "    env=env,\n",
    "    variation_fn=variation_fn,\n",
    ")\n",
    "\n",
    "# Instantiate MAP Elites\n",
    "map_elites = MAPElites(\n",
    "    scoring_function=scoring_fn,\n",
    "    emitter=pg_emitter,\n",
    "    metrics_function=metrics_function,\n",
    ")\n",
    "\n",
    "# compute initial repertoire\n",
    "repertoire, emitter_state, random_key, normalizer, reward_normalizer = map_elites.init(init_params, centroids, random_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QualityPGEmitterState' object has no attribute 'extra_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43memitter_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memitter_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_scores\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QualityPGEmitterState' object has no attribute 'extra_scores'"
     ]
    }
   ],
   "source": [
    "emitter_state.emitter_states[0].extra_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pga_emitter_config = QualityPGConfig(\n",
    "    env_batch_size=config.env_batch_size,\n",
    "    num_critic_training_steps=config.num_critic_training_steps,\n",
    "    num_pg_training_steps=config.num_pg_training_steps,\n",
    "    replay_buffer_size=config.replay_buffer_size,\n",
    "    critic_hidden_layer_size=config.critic_hidden_layer_size,\n",
    "    critic_learning_rate=config.critic_learning_rate,\n",
    "    actor_learning_rate=config.actor_learning_rate,\n",
    "    policy_learning_rate=config.policy_learning_rate,\n",
    "    noise_clip=config.noise_clip,\n",
    "    policy_noise=config.policy_noise,\n",
    "    discount=config.discount,\n",
    "    reward_scaling=config.reward_scaling,\n",
    "    batch_size=config.batch_size,\n",
    "    soft_tau_update=config.soft_tau_update,\n",
    "    policy_delay=config.policy_delay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pga_emitter = QualityPGEmitter(\n",
    "    config=pga_emitter_config,\n",
    "    policy_network=policy_network,\n",
    "    env=env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitnesses, descriptors, extra_scores, random_key, normalizer, reward_normalizer = scoring_fn(\n",
    "    init_params, random_key, normalizer, reward_normalizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_normalizer.return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.9133669 , 0.64897674, 0.46943998, 0.33254927, 0.13727005,\n",
       "       0.        , 0.        , 0.        , 0.4512778 , 0.5159382 ,\n",
       "       0.36667266, 0.69073284, 0.6368465 , 0.56640613, 0.86703014,\n",
       "       0.94544053, 0.7863475 , 0.35145256, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.8015611 ,\n",
       "       0.9226498 , 0.2591268 , 0.        , 0.5491032 , 0.11409856,\n",
       "       0.2736179 , 0.9644985 , 0.4596038 , 0.73729664, 0.44664145,\n",
       "       0.73134035, 0.9176377 , 0.5421054 , 0.5023163 , 0.6601326 ,\n",
       "       0.62721986, 0.584525  , 0.43393892, 0.46974394, 0.48667565,\n",
       "       0.37871093, 0.        , 0.22576216, 0.        , 0.        ,\n",
       "       0.23547144, 0.7468129 , 0.6553663 , 0.        , 0.        ,\n",
       "       0.4728555 , 0.56349236, 0.843907  , 0.38519058, 1.1742105 ,\n",
       "       1.0411208 , 0.76950645, 0.69206184, 0.04383034, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.33414796,\n",
       "       0.02556707, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.20137753, 0.        , 0.3412169 , 0.        , 0.        ,\n",
       "       0.36214423, 0.5981263 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.2636131 , 0.4397526 , 0.5482456 ,\n",
       "       0.44181094, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.6044747 , 0.76257205, 1.0177462 , 0.44373387,\n",
       "       0.5958781 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.08482134,\n",
       "       0.45335704, 0.46744096, 0.9225581 , 0.44115224, 0.61351687,\n",
       "       0.8106848 , 0.76854473, 0.79086936, 1.0359191 , 1.0832342 ,\n",
       "       0.7862243 , 0.31203395, 0.31075406, 0.11834931, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01909263, 0.04157398,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ],      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_scores[\"rewards\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.04110002, 0.02920406, 0.02112583, 0.01496618, 0.0061781 ,\n",
       "       0.        , 0.        , 0.        , 0.02031648, 0.02322963,\n",
       "       0.01651075, 0.0311061 , 0.02868279, 0.02551345, 0.03906013,\n",
       "       0.04259862, 0.0354357 , 0.01584024, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.03617386,\n",
       "       0.04164728, 0.0116992 , 0.        , 0.02480238, 0.0051549 ,\n",
       "       0.01236484, 0.04359643, 0.02077975, 0.0333433 , 0.02020397,\n",
       "       0.03309102, 0.0415314 , 0.02454173, 0.02274659, 0.02990126,\n",
       "       0.02841833, 0.02649132, 0.01967216, 0.02130143, 0.02207559,\n",
       "       0.0171833 , 0.        , 0.01024955, 0.        , 0.        ,\n",
       "       0.01069991, 0.03394569, 0.02979809, 0.        , 0.        ,\n",
       "       0.02151941, 0.02565213, 0.03842942, 0.01754604, 0.05350371,\n",
       "       0.04745412, 0.03508488, 0.0315637 , 0.00199965, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.01527321,\n",
       "       0.00116898, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00924993, 0.        , 0.01568261, 0.        , 0.        ,\n",
       "       0.01665927, 0.02752295, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01215135, 0.02027635, 0.02528598,\n",
       "       0.0203828 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.02793358, 0.03524902, 0.04705681, 0.0205221 ,\n",
       "       0.0275659 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.003933  ,\n",
       "       0.0210265 , 0.02168504, 0.04280879, 0.02047542, 0.0284823 ,\n",
       "       0.03764471, 0.03569634, 0.03674185, 0.04813743, 0.0503477 ,\n",
       "       0.03655131, 0.01450964, 0.01445337, 0.00550573, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00088917, 0.00193657,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ],      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_scores[\"transitions\"].rewards[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-08"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 3.8053678499534724\n",
      "Standard deviation: 0.0019505445173990574\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "def score():\n",
    "    scoring_fn(\n",
    "        init_params, random_key\n",
    "    )\n",
    "\n",
    "score()\n",
    "\n",
    "timer = timeit.Timer(score)\n",
    "results = timer.repeat(repeat=10, number=1)  # Adjust 'repeat' and 'number' as needed\n",
    "\n",
    "# Calculate mean time and standard deviation\n",
    "mean_time = sum(results) / len(results)\n",
    "standard_deviation = (sum((x - mean_time) ** 2 for x in results) / len(results)) ** 0.5\n",
    "\n",
    "# Now you can use `mean_time` and `standard_deviation` as needed\n",
    "print(\"Mean time:\", mean_time)\n",
    "print(\"Standard deviation:\", standard_deviation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1009695/3082900524.py:1: UserWarning: This type of repertoire does not store the extra scores computed by the scoring function\n",
      "  repertoire = MapElitesRepertoire.init(\n"
     ]
    }
   ],
   "source": [
    "repertoire = MapElitesRepertoire.init(\n",
    "    genotypes=init_params,\n",
    "    fitnesses=fitnesses,\n",
    "    descriptors=descriptors,\n",
    "    centroids=centroids,\n",
    "    extra_scores=extra_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emitter_state, random_key = pga_emitter.init(\n",
    "    random_key=random_key,\n",
    "    repertoire=repertoire,\n",
    "    genotypes=init_params,\n",
    "    fitnesses=fitnesses,\n",
    "    descriptors=descriptors,\n",
    "    extra_scores=extra_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 0.7683740634471178\n",
      "Standard deviation: 0.0012103786950750863\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "def update_state():\n",
    "    pga_emitter.state_update(emitter_state=emitter_state, extra_scores=extra_scores, repertoire=repertoire, genotypes=init_params, fitnesses=fitnesses, descriptors=descriptors)\n",
    "    \n",
    "    \n",
    "update_state()\n",
    "\n",
    "timer = timeit.Timer(update_state)\n",
    "results = timer.repeat(repeat=10, number=1)  # Adjust 'repeat' and 'number' as needed\n",
    "\n",
    "# Calculate mean time and standard deviation\n",
    "mean_time = sum(results) / len(results)\n",
    "standard_deviation = (sum((x - mean_time) ** 2 for x in results) / len(results)) ** 0.5\n",
    "\n",
    "# Now you can use `mean_time` and `standard_deviation` as needed\n",
    "print(\"Mean time:\", mean_time)\n",
    "print(\"Standard deviation:\", standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_genotype = jax.tree_map(lambda x: x[0], init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 0.02882700189948082\n",
      "Standard deviation: 0.005437930463612316\n"
     ]
    }
   ],
   "source": [
    "random_key = jax.random.PRNGKey(0)\n",
    "def mutation_fn():\n",
    "    pga_emitter._mutation_function_pg(policy_params=first_genotype, emitter_state=emitter_state)\n",
    "mutation_fn()\n",
    "\n",
    "timer = timeit.Timer(mutation_fn)\n",
    "results = timer.repeat(repeat=10, number=1)  # Adjust 'repeat' and 'number' as needed\n",
    "\n",
    "# Calculate mean time and standard deviation\n",
    "mean_time = sum(results) / len(results)\n",
    "standard_deviation = (sum((x - mean_time) ** 2 for x in results) / len(results)) ** 0.5\n",
    "\n",
    "# Now you can use `mean_time` and `standard_deviation` as needed\n",
    "print(\"Mean time:\", mean_time)\n",
    "print(\"Standard deviation:\", standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_batch = jax.tree_map(lambda x: x[:int(0.5*config.env_batch_size)], init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 5.0848997985944155\n",
      "Standard deviation: 0.002601589654325258\n"
     ]
    }
   ],
   "source": [
    "def emit():\n",
    "    pga_emitter.emit_pg(emitter_state=emitter_state, parents=half_batch)\n",
    "    \n",
    "emit()\n",
    "\n",
    "timer = timeit.Timer(emit)\n",
    "\n",
    "results = timer.repeat(repeat=10, number=1)  # Adjust 'repeat' and 'number' as needed\n",
    "\n",
    "# Calculate mean time and standard deviation\n",
    "mean_time = sum(results) / len(results)\n",
    "standard_deviation = (sum((x - mean_time) ** 2 for x in results) / len(results)) ** 0.5\n",
    "\n",
    "# Now you can use `mean_time` and `standard_deviation` as needed\n",
    "print(\"Mean time:\", mean_time)\n",
    "print(\"Standard deviation:\", standard_deviation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

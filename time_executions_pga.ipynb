{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  3 20:41:16 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A30                     Off | 00000000:01:00.0 Off |                   On |\n",
      "| N/A   29C    P0              26W / 165W |     50MiB / 24576MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                          |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                |        ECC|                       |\n",
      "|==================+================================+===========+=======================|\n",
      "|  0    1   0   0  |              25MiB / 11968MiB  | 28      0 |  2   0    2    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MPLCONFIGDIR'] = '/tmp/matplotlib'\n",
    "os.environ['WANDB_CACHE_DIR'] = '/tmp/wandb_cache'\n",
    "os.environ['JAX_LOG_COMPILATION'] = '1'\n",
    "#os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false'\n",
    "#os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import optax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "from qdax.core.containers.repertoire import Repertoire\n",
    "from qdax.core.emitters.emitter import Emitter, EmitterState\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition, ReplayBuffer\n",
    "from qdax.core.neuroevolution.losses.td3_loss import make_td3_loss_fn\n",
    "from qdax.core.neuroevolution.networks.networks import QModule\n",
    "from qdax.environments.base_wrappers import QDEnv\n",
    "from qdax.types import Descriptor, ExtraScores, Fitness, Genotype, Params, RNGKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from dataclasses import dataclass\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import serialization\n",
    "\n",
    "from qdax.core.containers.mapelites_repertoire import compute_cvt_centroids, MapElitesRepertoire\n",
    "from qdax.tasks.brax_envs import reset_based_scoring_function_brax_envs as scoring_function\n",
    "from qdax.environments import behavior_descriptor_extractor\n",
    "from qdax.core.map_elites_pga import MAPElites\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.core.emitters.pga_me_emitter import PGAMEConfig, PGAMEEmitter\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "from qdax.utils.plotting import plot_map_elites_results, plot_2d_map_elites_repertoire\n",
    "from set_up_brax import get_reward_offset_brax\n",
    "from qdax.utils.sampling import sampling \n",
    "from qdax.types import RNGKey, Genotype\n",
    "from typing import Any, Dict, Tuple, List, Callable\n",
    "import matplotlib.pyplot as plt\n",
    "from qdax import environments_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_name):\n",
    "    if env_name == \"hopper_uni\":\n",
    "        episode_length = 1000\n",
    "        \n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "    elif env_name == \"halfcheetah_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\n",
    "        \n",
    "    elif env_name == \"walker2d_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length)\t\n",
    "    elif env_name == \"ant_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=True)\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "\n",
    "        env = environments_v1.create(env_name, episode_length=episode_length, exclude_current_positions_from_observation=True)\t\n",
    "    '''\n",
    "    elif env_name == \"ant_omni\":\n",
    "        episode_length = 250\n",
    "        max_bd = 30.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length, use_contact_forces=False, exclude_current_positions_from_observation=False)\t\n",
    "    elif env_name == \"humanoid_uni\":\n",
    "        episode_length = 1000\n",
    "        max_bd = 1.\n",
    "\n",
    "        env = environments.create(env_name, episode_length=episode_length)\t\n",
    "    else:\n",
    "        ValueError(f\"Environment {env_name} not supported.\")\n",
    "    '''\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QualityPGConfig:\n",
    "    \"\"\"Configuration for QualityPG Emitter\"\"\"\n",
    "\n",
    "    env_batch_size: int = 256\n",
    "    num_critic_training_steps: int = 300\n",
    "    num_pg_training_steps: int = 100\n",
    "\n",
    "    # TD3 params\n",
    "    replay_buffer_size: int = 1000000\n",
    "    critic_hidden_layer_size: Tuple[int, ...] = (256, 256)\n",
    "    critic_learning_rate: float = 3e-4\n",
    "    actor_learning_rate: float = 3e-4\n",
    "    policy_learning_rate: float = 1e-3\n",
    "    noise_clip: float = 0.5\n",
    "    policy_noise: float = 0.2\n",
    "    discount: float = 0.99\n",
    "    reward_scaling: float = 1.0\n",
    "    batch_size: int = 100\n",
    "    soft_tau_update: float = 0.005\n",
    "    policy_delay: int = 2\n",
    "\n",
    "\n",
    "class QualityPGEmitterState(EmitterState):\n",
    "    \"\"\"Contains training state for the learner.\"\"\"\n",
    "\n",
    "    critic_params: Params\n",
    "    critic_optimizer_state: optax.OptState\n",
    "    actor_params: Params\n",
    "    actor_opt_state: optax.OptState\n",
    "    target_critic_params: Params\n",
    "    target_actor_params: Params\n",
    "    replay_buffer: ReplayBuffer\n",
    "    random_key: RNGKey\n",
    "    steps: jnp.ndarray\n",
    "\n",
    "\n",
    "class QualityPGEmitter(Emitter):\n",
    "    \"\"\"\n",
    "    A policy gradient emitter used to implement the Policy Gradient Assisted MAP-Elites\n",
    "    (PGA-Map-Elites) algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: QualityPGConfig,\n",
    "        policy_network: nn.Module,\n",
    "        env: QDEnv,\n",
    "    ) -> None:\n",
    "        self._config = config\n",
    "        self._env = env\n",
    "        self._policy_network = policy_network\n",
    "\n",
    "        # Init Critics\n",
    "        critic_network = QModule(\n",
    "            n_critics=2, hidden_layer_sizes=self._config.critic_hidden_layer_size\n",
    "        )\n",
    "        self._critic_network = critic_network\n",
    "\n",
    "        # Set up the losses and optimizers - return the opt states\n",
    "        self._policy_loss_fn, self._critic_loss_fn = make_td3_loss_fn(\n",
    "            policy_fn=policy_network.apply,\n",
    "            critic_fn=critic_network.apply,\n",
    "            reward_scaling=self._config.reward_scaling,\n",
    "            discount=self._config.discount,\n",
    "            noise_clip=self._config.noise_clip,\n",
    "            policy_noise=self._config.policy_noise,\n",
    "        )\n",
    "\n",
    "        # Init optimizers\n",
    "        self._actor_optimizer = optax.adam(\n",
    "            learning_rate=self._config.actor_learning_rate\n",
    "        )\n",
    "        self._critic_optimizer = optax.adam(\n",
    "            learning_rate=self._config.critic_learning_rate\n",
    "        )\n",
    "        self._policies_optimizer = optax.adam(\n",
    "            learning_rate=self._config.policy_learning_rate\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def batch_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            the batch size emitted by the emitter.\n",
    "        \"\"\"\n",
    "        return self._config.env_batch_size\n",
    "\n",
    "    @property\n",
    "    def use_all_data(self) -> bool:\n",
    "        \"\"\"Whether to use all data or not when used along other emitters.\n",
    "\n",
    "        QualityPGEmitter uses the transitions from the genotypes that were generated\n",
    "        by other emitters.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def init(\n",
    "        self,\n",
    "        random_key: RNGKey,\n",
    "        repertoire: Repertoire,\n",
    "        genotypes: Genotype,\n",
    "        fitnesses: Fitness,\n",
    "        descriptors: Descriptor,\n",
    "        extra_scores: ExtraScores,\n",
    "    ) -> Tuple[QualityPGEmitterState, RNGKey]:\n",
    "        \"\"\"Initializes the emitter state.\n",
    "\n",
    "        Args:\n",
    "            genotypes: The initial population.\n",
    "            random_key: A random key.\n",
    "\n",
    "        Returns:\n",
    "            The initial state of the PGAMEEmitter, a new random key.\n",
    "        \"\"\"\n",
    "\n",
    "        observation_size = self._env.observation_size\n",
    "        action_size = self._env.action_size\n",
    "        descriptor_size = self._env.state_descriptor_length\n",
    "\n",
    "        # Initialise critic, greedy actor and population\n",
    "        random_key, subkey = jax.random.split(random_key)\n",
    "        fake_obs = jnp.zeros(shape=(observation_size,))\n",
    "        fake_action = jnp.zeros(shape=(action_size,))\n",
    "        critic_params = self._critic_network.init(\n",
    "            subkey, obs=fake_obs, actions=fake_action\n",
    "        )\n",
    "        target_critic_params = jax.tree_util.tree_map(lambda x: x, critic_params)\n",
    "\n",
    "        actor_params = jax.tree_util.tree_map(lambda x: x[0], genotypes)\n",
    "        target_actor_params = jax.tree_util.tree_map(lambda x: x[0], genotypes)\n",
    "\n",
    "        # Prepare init optimizer states\n",
    "        critic_optimizer_state = self._critic_optimizer.init(critic_params)\n",
    "        actor_optimizer_state = self._actor_optimizer.init(actor_params)\n",
    "\n",
    "        # Initialize replay buffer\n",
    "        dummy_transition = QDTransition.init_dummy(\n",
    "            observation_dim=observation_size,\n",
    "            action_dim=action_size,\n",
    "            descriptor_dim=descriptor_size,\n",
    "        )\n",
    "\n",
    "        replay_buffer = ReplayBuffer.init(\n",
    "            buffer_size=self._config.replay_buffer_size, transition=dummy_transition\n",
    "        )\n",
    "\n",
    "        # get the transitions out of the dictionary\n",
    "        assert \"transitions\" in extra_scores.keys(), \"Missing transitions or wrong key\"\n",
    "        transitions = extra_scores[\"transitions\"]\n",
    "\n",
    "        # add transitions in the replay buffer\n",
    "        replay_buffer = replay_buffer.insert(transitions)\n",
    "\n",
    "        # Initial training state\n",
    "        random_key, subkey = jax.random.split(random_key)\n",
    "        emitter_state = QualityPGEmitterState(\n",
    "            critic_params=critic_params,\n",
    "            critic_optimizer_state=critic_optimizer_state,\n",
    "            actor_params=actor_params,\n",
    "            actor_opt_state=actor_optimizer_state,\n",
    "            target_critic_params=target_critic_params,\n",
    "            target_actor_params=target_actor_params,\n",
    "            replay_buffer=replay_buffer,\n",
    "            random_key=subkey,\n",
    "            steps=jnp.array(0),\n",
    "        )\n",
    "\n",
    "        return emitter_state, random_key\n",
    "\n",
    "    @partial(\n",
    "        jax.jit,\n",
    "        static_argnames=(\"self\",),\n",
    "    )\n",
    "    def emit(\n",
    "        self,\n",
    "        repertoire: Repertoire,\n",
    "        emitter_state: QualityPGEmitterState,\n",
    "        random_key: RNGKey,\n",
    "    ) -> Tuple[Genotype, ExtraScores, RNGKey]:\n",
    "        \"\"\"Do a step of PG emission.\n",
    "\n",
    "        Args:\n",
    "            repertoire: the current repertoire of genotypes\n",
    "            emitter_state: the state of the emitter used\n",
    "            random_key: a random key\n",
    "\n",
    "        Returns:\n",
    "            A batch of offspring, the new emitter state and a new key.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = self._config.env_batch_size\n",
    "\n",
    "        # sample parents\n",
    "        mutation_pg_batch_size = int(batch_size - 1)\n",
    "        print(type(repertoire))\n",
    "        parents, random_key = repertoire.sample(random_key, mutation_pg_batch_size)\n",
    "\n",
    "        # apply the pg mutation\n",
    "        offsprings_pg = self.emit_pg(emitter_state, parents)\n",
    "\n",
    "        # get the actor (greedy actor)\n",
    "        offspring_actor = self.emit_actor(emitter_state)\n",
    "\n",
    "        # add dimension for concatenation\n",
    "        offspring_actor = jax.tree_util.tree_map(\n",
    "            lambda x: jnp.expand_dims(x, axis=0), offspring_actor\n",
    "        )\n",
    "\n",
    "        # gather offspring\n",
    "        genotypes = jax.tree_util.tree_map(\n",
    "            lambda x, y: jnp.concatenate([x, y], axis=0),\n",
    "            offsprings_pg,\n",
    "            offspring_actor,\n",
    "        )\n",
    "\n",
    "        return genotypes, {}, random_key\n",
    "\n",
    "    @partial(\n",
    "        jax.jit,\n",
    "        static_argnames=(\"self\",),\n",
    "    )\n",
    "    def emit_pg(\n",
    "        self, emitter_state: QualityPGEmitterState, parents: Genotype\n",
    "    ) -> Genotype:\n",
    "        \"\"\"Emit the offsprings generated through pg mutation.\n",
    "\n",
    "        Args:\n",
    "            emitter_state: current emitter state, contains critic and\n",
    "                replay buffer.\n",
    "            parents: the parents selected to be applied gradients in order\n",
    "                to mutate towards better performance.\n",
    "\n",
    "        Returns:\n",
    "            A new set of offsprings.\n",
    "        \"\"\"\n",
    "        mutation_fn = partial(\n",
    "            self._mutation_function_pg,\n",
    "            emitter_state=emitter_state,\n",
    "        )\n",
    "        offsprings = jax.vmap(mutation_fn)(parents)\n",
    "\n",
    "        return offsprings\n",
    "\n",
    "    @partial(\n",
    "        jax.jit,\n",
    "        static_argnames=(\"self\",),\n",
    "    )\n",
    "    def emit_actor(self, emitter_state: QualityPGEmitterState) -> Genotype:\n",
    "        \"\"\"Emit the greedy actor.\n",
    "\n",
    "        Simply needs to be retrieved from the emitter state.\n",
    "\n",
    "        Args:\n",
    "            emitter_state: the current emitter state, it stores the\n",
    "                greedy actor.\n",
    "\n",
    "        Returns:\n",
    "            The parameters of the actor.\n",
    "        \"\"\"\n",
    "        return emitter_state.actor_params\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def state_update(\n",
    "        self,\n",
    "        emitter_state: QualityPGEmitterState,\n",
    "        repertoire: Optional[Repertoire],\n",
    "        genotypes: Optional[Genotype],\n",
    "        fitnesses: Optional[Fitness],\n",
    "        descriptors: Optional[Descriptor],\n",
    "        extra_scores: ExtraScores,\n",
    "    ) -> QualityPGEmitterState:\n",
    "        \"\"\"This function gives an opportunity to update the emitter state\n",
    "        after the genotypes have been scored.\n",
    "\n",
    "        Here it is used to fill the Replay Buffer with the transitions\n",
    "        from the scoring of the genotypes, and then the training of the\n",
    "        critic/actor happens. Hence the params of critic/actor are updated,\n",
    "        as well as their optimizer states.\n",
    "\n",
    "        Args:\n",
    "            emitter_state: current emitter state.\n",
    "            repertoire: the current genotypes repertoire\n",
    "            genotypes: unused here - but compulsory in the signature.\n",
    "            fitnesses: unused here - but compulsory in the signature.\n",
    "            descriptors: unused here - but compulsory in the signature.\n",
    "            extra_scores: extra information coming from the scoring function,\n",
    "                this contains the transitions added to the replay buffer.\n",
    "\n",
    "        Returns:\n",
    "            New emitter state where the replay buffer has been filled with\n",
    "            the new experienced transitions.\n",
    "        \"\"\"\n",
    "        # get the transitions out of the dictionary\n",
    "        assert \"transitions\" in extra_scores.keys(), \"Missing transitions or wrong key\"\n",
    "        transitions = extra_scores[\"transitions\"]\n",
    "\n",
    "        # add transitions in the replay buffer\n",
    "        replay_buffer = emitter_state.replay_buffer.insert(transitions)\n",
    "        emitter_state = emitter_state.replace(replay_buffer=replay_buffer)\n",
    "\n",
    "        def scan_train_critics(\n",
    "            carry: QualityPGEmitterState, unused: Any\n",
    "        ) -> Tuple[QualityPGEmitterState, Any]:\n",
    "            emitter_state = carry\n",
    "            new_emitter_state = self._train_critics(emitter_state)\n",
    "            return new_emitter_state, ()\n",
    "\n",
    "        # Train critics and greedy actor\n",
    "        emitter_state, _ = jax.lax.scan(\n",
    "            scan_train_critics,\n",
    "            emitter_state,\n",
    "            (),\n",
    "            length=self._config.num_critic_training_steps,\n",
    "        )\n",
    "\n",
    "        return emitter_state  # type: ignore\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _train_critics(\n",
    "        self, emitter_state: QualityPGEmitterState\n",
    "    ) -> QualityPGEmitterState:\n",
    "        \"\"\"Apply one gradient step to critics and to the greedy actor\n",
    "        (contained in carry in training_state), then soft update target critics\n",
    "        and target actor.\n",
    "\n",
    "        Those updates are very similar to those made in TD3.\n",
    "\n",
    "        Args:\n",
    "            emitter_state: actual emitter state\n",
    "\n",
    "        Returns:\n",
    "            New emitter state where the critic and the greedy actor have been\n",
    "            updated. Optimizer states have also been updated in the process.\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample a batch of transitions in the buffer\n",
    "        random_key = emitter_state.random_key\n",
    "        replay_buffer = emitter_state.replay_buffer\n",
    "        transitions, random_key = replay_buffer.sample(\n",
    "            random_key, sample_size=self._config.batch_size\n",
    "        )\n",
    "\n",
    "        # Update Critic\n",
    "        (\n",
    "            critic_optimizer_state,\n",
    "            critic_params,\n",
    "            target_critic_params,\n",
    "            random_key,\n",
    "        ) = self._update_critic(\n",
    "            critic_params=emitter_state.critic_params,\n",
    "            target_critic_params=emitter_state.target_critic_params,\n",
    "            target_actor_params=emitter_state.target_actor_params,\n",
    "            critic_optimizer_state=emitter_state.critic_optimizer_state,\n",
    "            transitions=transitions,\n",
    "            random_key=random_key,\n",
    "        )\n",
    "\n",
    "        # Update greedy actor\n",
    "        (actor_optimizer_state, actor_params, target_actor_params,) = jax.lax.cond(\n",
    "            emitter_state.steps % self._config.policy_delay == 0,\n",
    "            lambda x: self._update_actor(*x),\n",
    "            lambda _: (\n",
    "                emitter_state.actor_opt_state,\n",
    "                emitter_state.actor_params,\n",
    "                emitter_state.target_actor_params,\n",
    "            ),\n",
    "            operand=(\n",
    "                emitter_state.actor_params,\n",
    "                emitter_state.actor_opt_state,\n",
    "                emitter_state.target_actor_params,\n",
    "                emitter_state.critic_params,\n",
    "                transitions,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Create new training state\n",
    "        new_emitter_state = emitter_state.replace(\n",
    "            critic_params=critic_params,\n",
    "            critic_optimizer_state=critic_optimizer_state,\n",
    "            actor_params=actor_params,\n",
    "            actor_opt_state=actor_optimizer_state,\n",
    "            target_critic_params=target_critic_params,\n",
    "            target_actor_params=target_actor_params,\n",
    "            random_key=random_key,\n",
    "            steps=emitter_state.steps + 1,\n",
    "            replay_buffer=replay_buffer,\n",
    "        )\n",
    "\n",
    "        return new_emitter_state  # type: ignore\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _update_critic(\n",
    "        self,\n",
    "        critic_params: Params,\n",
    "        target_critic_params: Params,\n",
    "        target_actor_params: Params,\n",
    "        critic_optimizer_state: Params,\n",
    "        transitions: QDTransition,\n",
    "        random_key: RNGKey,\n",
    "    ) -> Tuple[Params, Params, Params, RNGKey]:\n",
    "\n",
    "        # compute loss and gradients\n",
    "        random_key, subkey = jax.random.split(random_key)\n",
    "        critic_loss, critic_gradient = jax.value_and_grad(self._critic_loss_fn)(\n",
    "            critic_params,\n",
    "            target_actor_params,\n",
    "            target_critic_params,\n",
    "            transitions,\n",
    "            subkey,\n",
    "        )\n",
    "        critic_updates, critic_optimizer_state = self._critic_optimizer.update(\n",
    "            critic_gradient, critic_optimizer_state\n",
    "        )\n",
    "\n",
    "        # update critic\n",
    "        critic_params = optax.apply_updates(critic_params, critic_updates)\n",
    "\n",
    "        # Soft update of target critic network\n",
    "        target_critic_params = jax.tree_map(\n",
    "            lambda x1, x2: (1.0 - self._config.soft_tau_update) * x1\n",
    "            + self._config.soft_tau_update * x2,\n",
    "            target_critic_params,\n",
    "            critic_params,\n",
    "        )\n",
    "\n",
    "        return critic_optimizer_state, critic_params, target_critic_params, random_key\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _update_actor(\n",
    "        self,\n",
    "        actor_params: Params,\n",
    "        actor_opt_state: optax.OptState,\n",
    "        target_actor_params: Params,\n",
    "        critic_params: Params,\n",
    "        transitions: QDTransition,\n",
    "    ) -> Tuple[optax.OptState, Params, Params]:\n",
    "\n",
    "        # Update greedy actor\n",
    "        policy_loss, policy_gradient = jax.value_and_grad(self._policy_loss_fn)(\n",
    "            actor_params,\n",
    "            critic_params,\n",
    "            transitions,\n",
    "        )\n",
    "        (\n",
    "            policy_updates,\n",
    "            actor_optimizer_state,\n",
    "        ) = self._actor_optimizer.update(policy_gradient, actor_opt_state)\n",
    "        actor_params = optax.apply_updates(actor_params, policy_updates)\n",
    "\n",
    "        # Soft update of target greedy actor\n",
    "        target_actor_params = jax.tree_map(\n",
    "            lambda x1, x2: (1.0 - self._config.soft_tau_update) * x1\n",
    "            + self._config.soft_tau_update * x2,\n",
    "            target_actor_params,\n",
    "            actor_params,\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            actor_optimizer_state,\n",
    "            actor_params,\n",
    "            target_actor_params,\n",
    "        )\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _mutation_function_pg(\n",
    "        self,\n",
    "        policy_params: Genotype,\n",
    "        emitter_state: QualityPGEmitterState,\n",
    "    ) -> Genotype:\n",
    "        \"\"\"Apply pg mutation to a policy via multiple steps of gradient descent.\n",
    "        First, update the rewards to be diversity rewards, then apply the gradient\n",
    "        steps.\n",
    "\n",
    "        Args:\n",
    "            policy_params: a policy, supposed to be a differentiable neural\n",
    "                network.\n",
    "            emitter_state: the current state of the emitter, containing among others,\n",
    "                the replay buffer, the critic.\n",
    "\n",
    "        Returns:\n",
    "            The updated params of the neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define new policy optimizer state\n",
    "        policy_optimizer_state = self._policies_optimizer.init(policy_params)\n",
    "\n",
    "        def scan_train_policy(\n",
    "            carry: Tuple[QualityPGEmitterState, Genotype, optax.OptState],\n",
    "            unused: Any,\n",
    "        ) -> Tuple[Tuple[QualityPGEmitterState, Genotype, optax.OptState], Any]:\n",
    "            emitter_state, policy_params, policy_optimizer_state = carry\n",
    "            (\n",
    "                new_emitter_state,\n",
    "                new_policy_params,\n",
    "                new_policy_optimizer_state,\n",
    "            ) = self._train_policy(\n",
    "                emitter_state,\n",
    "                policy_params,\n",
    "                policy_optimizer_state,\n",
    "            )\n",
    "            return (\n",
    "                new_emitter_state,\n",
    "                new_policy_params,\n",
    "                new_policy_optimizer_state,\n",
    "            ), ()\n",
    "\n",
    "        (emitter_state, policy_params, policy_optimizer_state,), _ = jax.lax.scan(\n",
    "            scan_train_policy,\n",
    "            (emitter_state, policy_params, policy_optimizer_state),\n",
    "            (),\n",
    "            length=self._config.num_pg_training_steps,\n",
    "        )\n",
    "\n",
    "        return policy_params\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _train_policy(\n",
    "        self,\n",
    "        emitter_state: QualityPGEmitterState,\n",
    "        policy_params: Params,\n",
    "        policy_optimizer_state: optax.OptState,\n",
    "    ) -> Tuple[QualityPGEmitterState, Params, optax.OptState]:\n",
    "        \"\"\"Apply one gradient step to a policy (called policy_params).\n",
    "\n",
    "        Args:\n",
    "            emitter_state: current state of the emitter.\n",
    "            policy_params: parameters corresponding to the weights and bias of\n",
    "                the neural network that defines the policy.\n",
    "\n",
    "        Returns:\n",
    "            The new emitter state and new params of the NN.\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample a batch of transitions in the buffer\n",
    "        random_key = emitter_state.random_key\n",
    "        replay_buffer = emitter_state.replay_buffer\n",
    "        transitions, random_key = replay_buffer.sample(\n",
    "            random_key, sample_size=self._config.batch_size\n",
    "        )\n",
    "\n",
    "        # update policy\n",
    "        policy_optimizer_state, policy_params = self._update_policy(\n",
    "            critic_params=emitter_state.critic_params,\n",
    "            policy_optimizer_state=policy_optimizer_state,\n",
    "            policy_params=policy_params,\n",
    "            transitions=transitions,\n",
    "        )\n",
    "\n",
    "        # Create new training state\n",
    "        new_emitter_state = emitter_state.replace(\n",
    "            random_key=random_key,\n",
    "            replay_buffer=replay_buffer,\n",
    "        )\n",
    "\n",
    "        return new_emitter_state, policy_params, policy_optimizer_state\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"self\",))\n",
    "    def _update_policy(\n",
    "        self,\n",
    "        critic_params: Params,\n",
    "        policy_optimizer_state: optax.OptState,\n",
    "        policy_params: Params,\n",
    "        transitions: QDTransition,\n",
    "    ) -> Tuple[optax.OptState, Params]:\n",
    "\n",
    "        # compute loss\n",
    "        _policy_loss, policy_gradient = jax.value_and_grad(self._policy_loss_fn)(\n",
    "            policy_params,\n",
    "            critic_params,\n",
    "            transitions,\n",
    "        )\n",
    "        # Compute gradient and update policies\n",
    "        (\n",
    "            policy_updates,\n",
    "            policy_optimizer_state,\n",
    "        ) = self._policies_optimizer.update(policy_gradient, policy_optimizer_state)\n",
    "        policy_params = optax.apply_updates(policy_params, policy_updates)\n",
    "\n",
    "        return policy_optimizer_state, policy_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for the experiment script\n",
    "    \"\"\"\n",
    "    # Basic Configuration\n",
    "    seed: int\n",
    "    num_iterations: int\n",
    "    num_samples: int\n",
    "    env_batch_size: int  # Used in GA emitter and PG emitter\n",
    "    batch_size: int\n",
    "\n",
    "    # Archive Configuration\n",
    "    num_init_cvt_samples: int\n",
    "    num_centroids: int\n",
    "    policy_hidden_layer_sizes: Tuple[int, ...]\n",
    "    proportion_mutation_ga: float\n",
    "\n",
    "    # GA Emitter Configuration\n",
    "    iso_sigma: float\n",
    "    line_sigma: float\n",
    "\n",
    "    # PG Emitter Configuration\n",
    "    critic_hidden_layer_size: Tuple[int, ...]\n",
    "    num_critic_training_steps: int\n",
    "    num_pg_training_steps: int\n",
    "    replay_buffer_size: int\n",
    "    discount: float\n",
    "    reward_scaling: float\n",
    "    critic_learning_rate: float\n",
    "    actor_learning_rate: float\n",
    "    policy_learning_rate: float\n",
    "    noise_clip: float\n",
    "    policy_noise: float\n",
    "    soft_tau_update: float\n",
    "    policy_delay: int\n",
    "\n",
    "config = Config(\n",
    "    seed=0,\n",
    "    num_iterations=200,\n",
    "    num_samples=32,\n",
    "    batch_size=100,\n",
    "    env_batch_size=5096,\n",
    "    num_init_cvt_samples=50000,\n",
    "    num_centroids=1024,\n",
    "    policy_hidden_layer_sizes=[128, 128],\n",
    "    proportion_mutation_ga=0.5,\n",
    "    iso_sigma=0.005,\n",
    "    line_sigma=0.05,\n",
    "    critic_hidden_layer_size=[256, 256],\n",
    "    num_critic_training_steps=3000,\n",
    "    num_pg_training_steps=150,\n",
    "    replay_buffer_size=5096000, #2048000, #1_000_000,\n",
    "    discount=0.99,\n",
    "    reward_scaling=1.0,\n",
    "    critic_learning_rate=3e-4,\n",
    "    actor_learning_rate=3e-4,\n",
    "    policy_learning_rate=5e-3,\n",
    "    noise_clip=0.5,\n",
    "    policy_noise=0.2,\n",
    "    soft_tau_update=0.005,\n",
    "    policy_delay=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def tree_flatten(self):\\n        return ((self.mean, self.var, self.count, self.return_val), self.size)\\n    \\n    @classmethod\\n    def tree_unflatten(cls, aux_data, children):\\n        size = aux_data\\n        mean, var, count, return_val = children\\n        return cls(size=size, mean=mean, var=var, count=count, return_val=return_val)\\n    \\n        \\ntree_util.register_pytree_node(\\n    RewardNormalizer,\\n    RewardNormalizer.tree_flatten,\\n    RewardNormalizer.tree_unflatten\\n)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class Normalizer:\n",
    "    def __init__(self, size, epsilon=1e-8):\n",
    "        self.size = size\n",
    "        self.mean = jnp.zeros(size)\n",
    "        self.var = jnp.ones(size)\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = jnp.mean(x, axis=0)\n",
    "        batch_var = jnp.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "\n",
    "        self.mean, self.var, self.count = self._update_mean_var_count(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x - self.mean) / jnp.sqrt(self.var + EPS)\n",
    "\n",
    "    def _update_mean_var_count(self, mean, var, count, batch_mean, batch_var, batch_count):\n",
    "        delta = batch_mean - mean\n",
    "        tot_count = count + batch_count\n",
    "\n",
    "        new_mean = mean + delta * batch_count / tot_count\n",
    "        m_a = var * count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        return new_mean, new_var, new_count\n",
    "'''\n",
    "'''\n",
    "class Normalizer:\n",
    "    def __init__(self, size, epsilon=1e-8):\n",
    "        self.size = size  # Expecting size to be the dimensionality of the observation features (z)\n",
    "        self.mean = jnp.zeros(size)\n",
    "        self.var = jnp.ones(size)\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        # Flatten the first two dimensions (x, y) to treat as a single batch dimension\n",
    "        flat_x = x.reshape(-1, self.size)\n",
    "        batch_mean = jnp.mean(flat_x, axis=0)\n",
    "        batch_var = jnp.var(flat_x, axis=0)\n",
    "        batch_count = flat_x.shape[0]\n",
    "\n",
    "        self.mean, self.var, self.count = self._update_mean_var_count(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        # Normalize maintaining the original shape, using broadcasting\n",
    "        return (x - self.mean) / jnp.sqrt(self.var + 1e-8)\n",
    "\n",
    "    def _update_mean_var_count(self, mean, var, count, batch_mean, batch_var, batch_count):\n",
    "        delta = batch_mean - mean\n",
    "        tot_count = count + batch_count\n",
    "\n",
    "        new_mean = mean + delta * batch_count / tot_count\n",
    "        m_a = var * count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        return new_mean, new_var, new_count\n",
    "        \n",
    "'''\n",
    "\n",
    "from jax import tree_util\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Normalizer:\n",
    "    size: int\n",
    "    mean: jnp.ndarray = None\n",
    "    var: jnp.ndarray = None\n",
    "    count: jnp.ndarray = 1e-4\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.mean is None:\n",
    "            self.mean = jnp.zeros(self.size)\n",
    "        if self.var is None:\n",
    "            self.var = jnp.ones(self.size)\n",
    "            \n",
    "    def update(self, x):\n",
    "        # Flatten the first two dimensions (x, y) to treat as a single batch dimension\n",
    "        flat_x = x.reshape(-1, self.size)\n",
    "        batch_mean = jnp.mean(flat_x, axis=0)\n",
    "        batch_var = jnp.var(flat_x, axis=0)\n",
    "        batch_count = flat_x.shape[0]\n",
    "\n",
    "        new_mean, new_var, new_count = self._update_mean_var_count(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
    "        \n",
    "        return self.replace(mean=new_mean, var=new_var, count=new_count)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        # Normalize maintaining the original shape, using broadcasting\n",
    "        return (x - self.mean) / jnp.sqrt(self.var + 1e-8)\n",
    "\n",
    "    def _update_mean_var_count(self, mean, var, count, batch_mean, batch_var, batch_count):\n",
    "        delta = batch_mean - mean\n",
    "        tot_count = count + batch_count\n",
    "\n",
    "        new_mean = mean + delta * batch_count / tot_count\n",
    "        m_a = var * count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        return new_mean, new_var, new_count\n",
    "'''\n",
    "    def tree_flatten(self):\n",
    "        return ((self.mean, self.var, self.count), self.size)\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        size = aux_data\n",
    "        mean, var, count = children\n",
    "        return cls(size=size, mean=mean, var=var, count=count)\n",
    "\n",
    "# Register Normalizer as a pytree node with JAX\n",
    "tree_util.register_pytree_node(\n",
    "    Normalizer,\n",
    "    Normalizer.tree_flatten,\n",
    "    Normalizer.tree_unflatten\n",
    ")\n",
    "'''\n",
    "\n",
    "@dataclass\n",
    "class RewardNormalizer:\n",
    "    size: int\n",
    "    mean: jnp.ndarray = 0.0\n",
    "    var: jnp.ndarray = 1.0\n",
    "    count: jnp.ndarray = 1e-4\n",
    "    return_val: jnp.ndarray = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.return_val is None:\n",
    "            self.return_val = jnp.zeros((self.size,))\n",
    "\n",
    "         \n",
    "    def update(self, reward, done, gamma=0.99):\n",
    "        \n",
    "        def _update_column_scan(carry, x):\n",
    "            mean, var, count, return_val = carry\n",
    "            (reward, done) = x\n",
    "            \n",
    "            # Update the return value\n",
    "            new_return_val = reward + gamma * return_val * (1 - done)\n",
    "            \n",
    "            # Update the mean, var, and count\n",
    "            batch_mean = jnp.mean(new_return_val, axis=0)\n",
    "            batch_var = jnp.var(new_return_val, axis=0)\n",
    "            batch_count = new_return_val.shape[0]\n",
    "            \n",
    "            delta = batch_mean - mean\n",
    "            tot_count = count + batch_count\n",
    "            \n",
    "            new_mean = mean + delta * batch_count / tot_count\n",
    "            m_a = var * count\n",
    "            m_b = batch_var * batch_count\n",
    "            M2 = m_a + m_b + jnp.square(delta) * count * batch_count / tot_count\n",
    "            new_var = M2 / tot_count\n",
    "            new_count = tot_count\n",
    "            \n",
    "            normalized_reward = reward / jnp.sqrt(new_var + 1e-8)\n",
    "            \n",
    "            return (new_mean, new_var, new_count, new_return_val), normalized_reward\n",
    "        \n",
    "        (new_mean, new_var, new_count, _), normalized_rewards = jax.lax.scan(\n",
    "            _update_column_scan,\n",
    "            (self.mean, self.var, self.count, self.return_val),\n",
    "            (reward.T, done.T),\n",
    "        )\n",
    "\n",
    "        \n",
    "        return self.replace(mean=new_mean, var=new_var, count=new_count), normalized_rewards.T\n",
    "\n",
    "'''\n",
    "    def tree_flatten(self):\n",
    "        return ((self.mean, self.var, self.count, self.return_val), self.size)\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        size = aux_data\n",
    "        mean, var, count, return_val = children\n",
    "        return cls(size=size, mean=mean, var=var, count=count, return_val=return_val)\n",
    "    \n",
    "        \n",
    "tree_util.register_pytree_node(\n",
    "    RewardNormalizer,\n",
    "    RewardNormalizer.tree_flatten,\n",
    "    RewardNormalizer.tree_unflatten\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mobservation_size\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.observation_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in policy_network:  21256\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 2) of type <class 'qdax.core.map_elites_pga.Normalizer'> for function reset_based_scoring_function_brax_envs is non-hashable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 125\u001b[0m\n\u001b[1;32m    118\u001b[0m map_elites \u001b[38;5;241m=\u001b[39m MAPElites(\n\u001b[1;32m    119\u001b[0m     scoring_function\u001b[38;5;241m=\u001b[39mscoring_fn,\n\u001b[1;32m    120\u001b[0m     emitter\u001b[38;5;241m=\u001b[39mpg_emitter,\n\u001b[1;32m    121\u001b[0m     metrics_function\u001b[38;5;241m=\u001b[39mmetrics_function,\n\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# compute initial repertoire\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m repertoire, emitter_state, random_key \u001b[38;5;241m=\u001b[39m \u001b[43mmap_elites\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/qdax/core/map_elites_pga.py:218\u001b[0m, in \u001b[0;36mMAPElites.init\u001b[0;34m(self, init_genotypes, centroids, random_key)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mInitialize a Map-Elites repertoire with an initial population of genotypes.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03mRequires the definition of centroids that can be computed with any method\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    and a random key.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# score initial genotypes\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m#normalizer = Normalizer(28)\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m#reward_normalizer = RewardNormalizer(5096)\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m fitnesses, descriptors, extra_scores, random_key, normalizer, reward_normalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scoring_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_genotypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reward_normalizer\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# init the repertoire\u001b[39;00m\n\u001b[1;32m    223\u001b[0m repertoire \u001b[38;5;241m=\u001b[39m MapElitesRepertoire\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m    224\u001b[0m     genotypes\u001b[38;5;241m=\u001b[39minit_genotypes,\n\u001b[1;32m    225\u001b[0m     fitnesses\u001b[38;5;241m=\u001b[39mfitnesses,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     extra_scores\u001b[38;5;241m=\u001b[39mextra_scores,\n\u001b[1;32m    229\u001b[0m )\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[0;32m/vol/bitbucket/km2120/QD-DRL/me-with-sample-based-drl/my_env/lib/python3.10/site-packages/jax/_src/api_util.py:288\u001b[0m, in \u001b[0;36margnums_partial_except\u001b[0;34m(f, static_argnums, args, allow_invalid)\u001b[0m\n\u001b[1;32m    286\u001b[0m static_arg \u001b[38;5;241m=\u001b[39m args[i]\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_hashable(static_arg):\n\u001b[0;32m--> 288\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-hashable static arguments are not supported, as this can lead \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto unexpected cache-misses. Static argument (index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(static_arg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is non-hashable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m   fixed_args\u001b[38;5;241m.\u001b[39mappend(_HashableWithStrictTypeEquality(static_arg))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 2) of type <class 'qdax.core.map_elites_pga.Normalizer'> for function reset_based_scoring_function_brax_envs is non-hashable."
     ]
    }
   ],
   "source": [
    "random_key = jax.random.PRNGKey(config.seed)\n",
    "\n",
    "# Init environment\n",
    "env = get_env('ant_uni')\n",
    "reset_fn = jax.jit(env.reset)\n",
    "normalizer = Normalizer(env.observation_size)\n",
    "reward_normalizer = RewardNormalizer(config.env_batch_size)\n",
    "\n",
    "# Compute the centroids\n",
    "centroids, random_key = compute_cvt_centroids(\n",
    "    num_descriptors=env.behavior_descriptor_length,\n",
    "    num_init_cvt_samples=config.num_init_cvt_samples,\n",
    "    num_centroids=config.num_centroids,\n",
    "    minval=0,\n",
    "    maxval=1,\n",
    "    random_key=random_key,\n",
    ")\n",
    "\n",
    "# Init policy network\n",
    "\n",
    "policy_layer_sizes = config.policy_hidden_layer_sizes + [env.action_size]\n",
    "policy_network = MLP(\n",
    "    layer_sizes=policy_layer_sizes,\n",
    "    kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "    final_activation=jnp.tanh,\n",
    ")\n",
    "\n",
    "# Init population of controllers\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, num=config.env_batch_size)\n",
    "fake_batch_obs = jnp.zeros(shape=(config.env_batch_size, env.observation_size))\n",
    "init_params = jax.vmap(policy_network.init)(keys, fake_batch_obs)\n",
    "\n",
    "param_count = sum(x[0].size for x in jax.tree_util.tree_leaves(init_params))\n",
    "print(\"Number of parameters in policy_network: \", param_count)\n",
    "\n",
    "# Define the fonction to play a step with the policy in the environment\n",
    "@jax.jit\n",
    "def play_step_fn(env_state, policy_params, random_key):\n",
    "    actions = policy_network.apply(policy_params, env_state.obs)\n",
    "    state_desc = env_state.info[\"state_descriptor\"]\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = QDTransition(\n",
    "        obs=env_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        actions=actions,\n",
    "        state_desc=state_desc,\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "        #desc=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "        #desc_prime=jnp.zeros(env.behavior_descriptor_length,) * jnp.nan,\n",
    "    )\n",
    "\n",
    "    return next_state, policy_params, random_key, transition\n",
    "\n",
    "# Prepare the scoring function\n",
    "bd_extraction_fn = behavior_descriptor_extractor['ant_uni']\n",
    "scoring_fn = functools.partial(\n",
    "    scoring_function,\n",
    "    episode_length=env.episode_length,\n",
    "    play_reset_fn=reset_fn,\n",
    "    play_step_fn=play_step_fn,\n",
    "    behavior_descriptor_extractor=bd_extraction_fn,\n",
    "    #normalizer=normalizer,\n",
    "    #reward_normalizer=reward_normalizer,\n",
    ")\n",
    "\n",
    "\n",
    "me_scoring_fn = functools.partial(\n",
    "sampling,\n",
    "scoring_fn=scoring_fn,\n",
    "num_samples=config.num_samples,\n",
    ")\n",
    "\n",
    "reward_offset = 0\n",
    "\n",
    "metrics_function = functools.partial(\n",
    "    default_qd_metrics,\n",
    "    qd_offset=reward_offset * env.episode_length,\n",
    ")\n",
    "\n",
    "# Define the PG-emitter config\n",
    "pga_emitter_config = PGAMEConfig(\n",
    "    env_batch_size=config.env_batch_size,\n",
    "    proportion_mutation_ga=config.proportion_mutation_ga,\n",
    "    critic_hidden_layer_size=config.critic_hidden_layer_size,\n",
    "    num_critic_training_steps=config.num_critic_training_steps,\n",
    "    num_pg_training_steps=config.num_pg_training_steps,\n",
    "    batch_size=config.batch_size,\n",
    "    replay_buffer_size=config.replay_buffer_size,\n",
    "    discount=config.discount,\n",
    "    reward_scaling=config.reward_scaling,\n",
    "    critic_learning_rate=config.critic_learning_rate,\n",
    "    #actor_learning_rate=config.algo.actor_learning_rate,\n",
    "    policy_learning_rate=config.policy_learning_rate,\n",
    "    noise_clip=config.noise_clip,\n",
    "    policy_noise=config.policy_noise,\n",
    "    soft_tau_update=config.soft_tau_update,\n",
    "    policy_delay=config.policy_delay,\n",
    ")\n",
    "\n",
    "# Get the emitter\n",
    "variation_fn = functools.partial(\n",
    "    isoline_variation, iso_sigma=config.iso_sigma, line_sigma=config.line_sigma\n",
    ")\n",
    "\n",
    "pg_emitter = PGAMEEmitter(\n",
    "    config=pga_emitter_config,\n",
    "    policy_network=policy_network,\n",
    "    env=env,\n",
    "    variation_fn=variation_fn,\n",
    ")\n",
    "\n",
    "# Instantiate MAP Elites\n",
    "map_elites = MAPElites(\n",
    "    scoring_function=scoring_fn,\n",
    "    emitter=pg_emitter,\n",
    "    metrics_function=metrics_function,\n",
    ")\n",
    "\n",
    "# compute initial repertoire\n",
    "repertoire, emitter_state, random_key = map_elites.init(init_params, centroids, random_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pga_emitter_config = QualityPGConfig(\n",
    "    env_batch_size=config.env_batch_size,\n",
    "    num_critic_training_steps=config.num_critic_training_steps,\n",
    "    num_pg_training_steps=config.num_pg_training_steps,\n",
    "    replay_buffer_size=config.replay_buffer_size,\n",
    "    critic_hidden_layer_size=config.critic_hidden_layer_size,\n",
    "    critic_learning_rate=config.critic_learning_rate,\n",
    "    actor_learning_rate=config.actor_learning_rate,\n",
    "    policy_learning_rate=config.policy_learning_rate,\n",
    "    noise_clip=config.noise_clip,\n",
    "    policy_noise=config.policy_noise,\n",
    "    discount=config.discount,\n",
    "    reward_scaling=config.reward_scaling,\n",
    "    batch_size=config.batch_size,\n",
    "    soft_tau_update=config.soft_tau_update,\n",
    "    policy_delay=config.policy_delay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pga_emitter = QualityPGEmitter(\n",
    "    config=pga_emitter_config,\n",
    "    policy_network=policy_network,\n",
    "    env=env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitnesses, descriptors, extra_scores, random_key = scoring_fn(\n",
    "    init_params, random_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([8.31227303e+00, 7.68757880e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.09409750e+00, 2.06991062e-01, 2.74492443e-01,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.29650593e-01, 4.53923136e-01,\n",
       "       1.72767967e-01, 3.33474994e-01, 2.51382917e-01, 2.55862921e-01,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.28316879e-01,\n",
       "       0.00000000e+00, 0.00000000e+00, 8.51324126e-02, 2.65911847e-01,\n",
       "       1.65822700e-01, 9.18955356e-02, 4.28652465e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.96743846e-01, 8.40749592e-02, 3.13334982e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.13950986e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.05478436e-01, 0.00000000e+00, 2.43633129e-02, 3.55284065e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.33292156e-02,\n",
       "       1.26161315e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 6.15899153e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.72038372e-02, 0.00000000e+00,\n",
       "       3.21297161e-02, 3.29503603e-02, 0.00000000e+00, 6.45661280e-02,\n",
       "       9.40590128e-02, 1.54633790e-01, 1.12420052e-01, 1.00820445e-01,\n",
       "       8.31576926e-06, 0.00000000e+00, 3.43912169e-02, 1.04046293e-01,\n",
       "       7.53272176e-02, 8.13802034e-02, 4.10961658e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.29567388e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.86811946e-02,\n",
       "       3.48562784e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.78281274e-02, 0.00000000e+00, 0.00000000e+00, 3.83745581e-02,\n",
       "       7.30600655e-02, 1.04210926e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.66115218e-02, 1.23716863e-02, 0.00000000e+00,\n",
       "       6.93397596e-02, 2.18585730e-02, 2.71221250e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.86316492e-03, 5.53805120e-02, 6.97331876e-02,\n",
       "       7.65780546e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.30769734e-02, 2.57869847e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 6.07025735e-02, 6.40791133e-02, 6.16294071e-02,\n",
       "       6.57074526e-02, 6.29455745e-02, 6.04636557e-02, 4.26761769e-02,\n",
       "       5.44264689e-02, 5.05352952e-02, 6.99248612e-02, 6.00317642e-02,\n",
       "       6.50496110e-02, 8.63085240e-02, 5.03270440e-02, 4.06935550e-02,\n",
       "       3.73219363e-02, 3.47318314e-02, 2.88816560e-02, 7.44426344e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.35793048e-02, 4.58239950e-02, 3.17813493e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.38170237e-03,\n",
       "       5.54881198e-03, 1.83291752e-02, 0.00000000e+00, 4.54307608e-02,\n",
       "       0.00000000e+00, 2.79031182e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.18235329e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.81641917e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.22400373e-02,\n",
       "       2.12323219e-02, 0.00000000e+00, 0.00000000e+00, 2.55880486e-02,\n",
       "       5.31570986e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.54690591e-02, 5.27775921e-02,\n",
       "       4.29967828e-02, 1.75109841e-02, 0.00000000e+00, 4.56002653e-02,\n",
       "       4.32388298e-02, 1.13614947e-02, 0.00000000e+00, 2.10147481e-02,\n",
       "       4.37734239e-02, 3.52143534e-02, 4.73074615e-03, 1.35617750e-02,\n",
       "       2.74507925e-02, 2.48954855e-02, 2.75713075e-02, 1.12423562e-02,\n",
       "       8.75960104e-03, 0.00000000e+00, 4.29175943e-02, 2.44494826e-02,\n",
       "       2.31633391e-02, 2.93609966e-02, 2.89643183e-02, 3.68810222e-02,\n",
       "       2.12486051e-02, 2.95880456e-02, 3.96780930e-02, 5.18146604e-02,\n",
       "       5.42302690e-02, 6.40139356e-02, 6.73295930e-02, 5.28744347e-02,\n",
       "       5.60926683e-02, 4.95231599e-02, 5.82385622e-02, 4.82871942e-02,\n",
       "       5.39255850e-02, 5.69907539e-02, 5.61923683e-02, 5.37150912e-02,\n",
       "       3.97265069e-02, 5.03787473e-02, 3.96415554e-02, 3.09278369e-02,\n",
       "       2.22043023e-02, 2.47722939e-02, 2.44757980e-02, 1.96151994e-02,\n",
       "       1.80898700e-02, 5.42913489e-02, 5.52096143e-02, 4.67736125e-02,\n",
       "       4.41293418e-02, 4.77348780e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.03374548e-02, 3.90200764e-02, 8.49783886e-04, 2.31145862e-02,\n",
       "       1.84742231e-02, 8.85383505e-03, 3.49709718e-03, 7.70135224e-03,\n",
       "       0.00000000e+00, 3.66363488e-02, 1.08126290e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.75007428e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.38079697e-02,\n",
       "       4.88557220e-02, 1.37249175e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.64365377e-02, 6.71675205e-02, 2.84638964e-02, 1.54257882e-02,\n",
       "       1.94634460e-02, 3.38904411e-02, 5.00554405e-02, 3.86560038e-02,\n",
       "       4.10249420e-02, 4.11784351e-02, 0.00000000e+00, 2.82833241e-02,\n",
       "       1.27822226e-02, 0.00000000e+00, 1.28141674e-03, 0.00000000e+00,\n",
       "       6.98287599e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.46081832e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.69772413e-02, 6.96415023e-04, 2.41306853e-02,\n",
       "       3.38065512e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.83855858e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.02318816e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.83852371e-02, 7.09529081e-03, 2.29372401e-02,\n",
       "       1.46845006e-03, 0.00000000e+00, 0.00000000e+00, 1.51000284e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.19938699e-02, 0.00000000e+00,\n",
       "       6.53874408e-03, 3.28308381e-02, 6.55948231e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.36965010e-02, 9.04793572e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.22629681e-02, 0.00000000e+00, 2.98109986e-02,\n",
       "       1.71240028e-02, 1.12475920e-02, 9.82573628e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.93088911e-02,\n",
       "       8.62214435e-03, 0.00000000e+00, 1.56702269e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.12272198e-02, 2.29201298e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.99266200e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.70765854e-02, 2.95037087e-02, 2.63334010e-02, 4.72935708e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.12267989e-02, 0.00000000e+00,\n",
       "       1.10918442e-02, 0.00000000e+00, 2.21860837e-02, 4.94733825e-02,\n",
       "       1.63785368e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.97771844e-02, 0.00000000e+00,\n",
       "       9.99119598e-03, 2.80543938e-02, 2.99819764e-02, 4.05731462e-02,\n",
       "       1.86277088e-02, 9.09787230e-03, 2.14275569e-02, 0.00000000e+00,\n",
       "       2.92488262e-02, 2.97648702e-02, 1.90852070e-03, 9.55645926e-03,\n",
       "       2.81581562e-03, 0.00000000e+00, 2.99902912e-02, 2.31359694e-02,\n",
       "       2.06361972e-02, 2.83495896e-02, 3.63234691e-02, 3.48623730e-02,\n",
       "       4.58359197e-02, 4.24830690e-02, 3.27110216e-02, 2.20294930e-02,\n",
       "       2.93734372e-02, 0.00000000e+00, 0.00000000e+00, 1.39762489e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.82689545e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.11675016e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 7.08790263e-03, 8.04169476e-02,\n",
       "       2.57655010e-02, 1.78848431e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.43355620e-03, 3.71360779e-02, 1.02490326e-02,\n",
       "       9.83100291e-03, 3.32167518e-04, 0.00000000e+00, 1.93344913e-02,\n",
       "       0.00000000e+00, 5.75721404e-03, 1.27615370e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       6.55333465e-03, 1.14759500e-03, 3.47877517e-02, 3.10870949e-02,\n",
       "       2.54467316e-02, 0.00000000e+00, 4.25324701e-02, 6.61463886e-02,\n",
       "       6.74008355e-02, 9.49614346e-02, 9.00334492e-02, 8.46989825e-02,\n",
       "       8.26057494e-02, 6.37604967e-02, 4.74558696e-02, 3.67328175e-03,\n",
       "       7.79028283e-04, 0.00000000e+00, 1.20520256e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.07345188e-02, 0.00000000e+00,\n",
       "       2.30541825e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.94105804e-02, 2.72072498e-02, 3.08549199e-02, 8.04072432e-03,\n",
       "       0.00000000e+00, 5.84420413e-02, 3.56944464e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.22568745e-03, 0.00000000e+00, 2.37532146e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.86931007e-02,\n",
       "       3.45964283e-02, 1.76896974e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.30345950e-02, 2.96911318e-02,\n",
       "       2.98928972e-02, 6.73264312e-03, 0.00000000e+00, 4.28452119e-02,\n",
       "       5.29811792e-02, 3.86895128e-02, 3.60082351e-02, 1.25196353e-02,\n",
       "       0.00000000e+00, 4.20920365e-02, 4.00577076e-02, 3.94364856e-02,\n",
       "       5.29123582e-02, 5.89547157e-02, 4.86985072e-02, 3.12864482e-02,\n",
       "       9.30829119e-05, 3.73597443e-02, 8.16084258e-03, 1.74776204e-02,\n",
       "       0.00000000e+00, 1.27477674e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.73689406e-02, 1.61674004e-02, 2.58160569e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.97562890e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.01573968e-02, 2.57219318e-02,\n",
       "       1.81978829e-02, 2.62267310e-02, 4.49852869e-02, 5.28244860e-02,\n",
       "       2.62675174e-02, 5.14990613e-02, 1.60712507e-02, 8.39617942e-03,\n",
       "       2.03687195e-02, 1.22166282e-04, 1.35808205e-02, 1.12398621e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.16313345e-03,\n",
       "       2.81982846e-03, 3.10690999e-02, 0.00000000e+00, 2.40696818e-02,\n",
       "       3.28368954e-02, 7.77011644e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.32978431e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.13255859e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 9.40406229e-03, 0.00000000e+00,\n",
       "       7.80736911e-04, 0.00000000e+00, 0.00000000e+00, 2.72913147e-02,\n",
       "       0.00000000e+00, 3.50679643e-02, 0.00000000e+00, 4.21606489e-02,\n",
       "       3.92358787e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.07223464e-02,\n",
       "       7.69039523e-03, 1.21770287e-02, 2.15785410e-02, 2.52991216e-03,\n",
       "       1.11258039e-02, 2.13143993e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       5.37308306e-03, 1.57577619e-02, 3.32396813e-02, 2.70012040e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.38547358e-02, 5.23818359e-02, 8.42388719e-02, 7.76209533e-02,\n",
       "       4.33845222e-02, 7.45486021e-02, 6.25571460e-02, 8.86635855e-03,\n",
       "       1.38859041e-02, 0.00000000e+00, 0.00000000e+00, 2.24703196e-02,\n",
       "       5.79468347e-03, 1.69776063e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.98912877e-02, 2.58937329e-02, 2.07410417e-02, 1.63084529e-02,\n",
       "       4.34093475e-02, 3.50992344e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.85724977e-02, 5.72889522e-02,\n",
       "       1.23372469e-02, 4.16800827e-02, 1.53384916e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.29960177e-02, 3.17301080e-02, 3.28009948e-02,\n",
       "       0.00000000e+00, 3.79004404e-02, 4.47322652e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.52477396e-02, 2.99393907e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.33057402e-02, 2.09627468e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.13959047e-02, 4.31823954e-02, 3.32097225e-02,\n",
       "       4.87900339e-02, 4.24546078e-02, 4.07119058e-02, 2.99183913e-02,\n",
       "       1.44916726e-02, 3.77726555e-02, 3.45513336e-02, 3.02726272e-02,\n",
       "       2.20007431e-02, 3.53807956e-02, 1.78783759e-02, 3.33339684e-02,\n",
       "       2.22967360e-02, 2.63210777e-02, 3.86196002e-02, 4.50606123e-02,\n",
       "       2.38757115e-02, 2.44653728e-02, 4.81553329e-03, 4.20100540e-02,\n",
       "       3.64641435e-02, 0.00000000e+00, 1.31246389e-03, 2.65280232e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 5.37049538e-03, 0.00000000e+00,\n",
       "       3.41985337e-02, 0.00000000e+00, 2.73628272e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.16541544e-02, 3.39798331e-02, 1.19624119e-02,\n",
       "       0.00000000e+00, 5.66679053e-03, 1.92245897e-02, 3.44812572e-02,\n",
       "       2.62026563e-02, 0.00000000e+00, 3.46002728e-02, 3.89981270e-02,\n",
       "       1.44117344e-02, 0.00000000e+00, 2.45331302e-02, 0.00000000e+00,\n",
       "       8.80032498e-03, 2.33635195e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.96610875e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.57166645e-02, 5.25025986e-02, 2.09953245e-02, 1.38879186e-02,\n",
       "       1.39841344e-02, 1.35633191e-02, 3.29830647e-02, 3.27083357e-02,\n",
       "       3.28891762e-02, 1.40648813e-03, 4.73400541e-02, 5.06046377e-02,\n",
       "       2.85951160e-02, 6.17310777e-02, 7.22577944e-02, 5.81911504e-02,\n",
       "       1.01351671e-01, 5.90083264e-02, 3.94241065e-02, 2.55954377e-02,\n",
       "       2.20766221e-03, 0.00000000e+00, 1.52499508e-02, 0.00000000e+00,\n",
       "       3.52864973e-02, 0.00000000e+00, 3.20937783e-02, 2.98305266e-02,\n",
       "       1.60516556e-02, 5.74479215e-02, 6.23689443e-02, 2.89027430e-02,\n",
       "       1.95582975e-02, 3.02708037e-02, 1.92019902e-02, 3.66776437e-02,\n",
       "       3.11037917e-02, 1.24086449e-02, 6.40100613e-02, 5.81514984e-02,\n",
       "       6.13854267e-02, 7.26185888e-02, 4.90981862e-02, 5.32894358e-02,\n",
       "       4.18153740e-02, 1.89823229e-02, 3.21717858e-02, 5.19160703e-02,\n",
       "       4.24831696e-02, 3.41272354e-02, 1.92351602e-02, 3.05754319e-02,\n",
       "       3.29637490e-02, 3.33327688e-02, 2.94143986e-02, 4.10659909e-02,\n",
       "       3.64071652e-02, 3.52889039e-02, 3.40891257e-02, 3.23061049e-02,\n",
       "       2.66211219e-02, 2.12240126e-02, 1.91327035e-02, 4.42557447e-02,\n",
       "       2.03671064e-02, 2.88120750e-02, 0.00000000e+00, 2.04388611e-02,\n",
       "       4.01728824e-02, 5.50836371e-03, 3.61885577e-02, 5.28631965e-03,\n",
       "       1.88487377e-02, 2.86959987e-02, 5.97423082e-03, 3.47444937e-02,\n",
       "       3.08626164e-02, 3.34220529e-02, 3.26905586e-02, 3.79653163e-02,\n",
       "       3.63997817e-02, 3.22614498e-02, 2.01413371e-02, 3.45428623e-02,\n",
       "       3.87882926e-02, 4.04128283e-02, 4.75011840e-02, 4.18354198e-02,\n",
       "       3.92142534e-02, 3.60663459e-02, 3.54250781e-02, 3.54771875e-02,\n",
       "       3.05087641e-02, 2.98457164e-02, 3.46589088e-02, 2.89189201e-02,\n",
       "       2.91929618e-02, 3.95058356e-02, 3.60088535e-02, 2.90935114e-02,\n",
       "       3.75339277e-02, 1.61879405e-03, 2.51005031e-02, 2.92978790e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 9.74778272e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.63331367e-02, 5.91851920e-02, 6.69474378e-02,\n",
       "       4.57466431e-02, 2.12531723e-02, 4.75548245e-02, 7.19571188e-02,\n",
       "       3.09347156e-02, 6.04369976e-02, 5.25062717e-02, 3.59617248e-02,\n",
       "       1.40377712e-02, 2.61717215e-02, 5.87163866e-03, 3.77578996e-02,\n",
       "       4.39722016e-02, 1.37386601e-02, 5.46824466e-03, 3.86876939e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       6.25880528e-03, 3.72798406e-02, 4.24735509e-02, 2.41497643e-02,\n",
       "       1.33391386e-02, 3.29943821e-02, 2.27231700e-02, 4.02078629e-02,\n",
       "       3.42497602e-02, 1.52067672e-02, 0.00000000e+00, 4.88304021e-03,\n",
       "       9.32586286e-03, 2.35466454e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.13552993e-02, 1.48479724e-02, 3.16744223e-02,\n",
       "       2.97526680e-02, 1.26928911e-02, 3.74378823e-02, 3.49038802e-02,\n",
       "       4.32218388e-02, 3.32434997e-02, 5.14659770e-02, 1.03822025e-02],      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_scores[\"transitions\"].rewards[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-08"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 3.8053678499534724\n",
      "Standard deviation: 0.0019505445173990574\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "def score():\n",
    "    scoring_fn(\n",
    "        init_params, random_key\n",
    "    )\n",
    "\n",
    "score()\n",
    "\n",
    "timer = timeit.Timer(score)\n",
    "results = timer.repeat(repeat=10, number=1)  # Adjust 'repeat' and 'number' as needed\n",
    "\n",
    "# Calculate mean time and standard deviation\n",
    "mean_time = sum(results) / len(results)\n",
    "standard_deviation = (sum((x - mean_time) ** 2 for x in results) / len(results)) ** 0.5\n",
    "\n",
    "# Now you can use `mean_time` and `standard_deviation` as needed\n",
    "print(\"Mean time:\", mean_time)\n",
    "print(\"Standard deviation:\", standard_deviation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1009695/3082900524.py:1: UserWarning: This type of repertoire does not store the extra scores computed by the scoring function\n",
      "  repertoire = MapElitesRepertoire.init(\n"
     ]
    }
   ],
   "source": [
    "repertoire = MapElitesRepertoire.init(\n",
    "    genotypes=init_params,\n",
    "    fitnesses=fitnesses,\n",
    "    descriptors=descriptors,\n",
    "    centroids=centroids,\n",
    "    extra_scores=extra_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emitter_state, random_key = pga_emitter.init(\n",
    "    random_key=random_key,\n",
    "    repertoire=repertoire,\n",
    "    genotypes=init_params,\n",
    "    fitnesses=fitnesses,\n",
    "    descriptors=descriptors,\n",
    "    extra_scores=extra_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 0.7683740634471178\n",
      "Standard deviation: 0.0012103786950750863\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "def update_state():\n",
    "    pga_emitter.state_update(emitter_state=emitter_state, extra_scores=extra_scores, repertoire=repertoire, genotypes=init_params, fitnesses=fitnesses, descriptors=descriptors)\n",
    "    \n",
    "    \n",
    "update_state()\n",
    "\n",
    "timer = timeit.Timer(update_state)\n",
    "results = timer.repeat(repeat=10, number=1)  # Adjust 'repeat' and 'number' as needed\n",
    "\n",
    "# Calculate mean time and standard deviation\n",
    "mean_time = sum(results) / len(results)\n",
    "standard_deviation = (sum((x - mean_time) ** 2 for x in results) / len(results)) ** 0.5\n",
    "\n",
    "# Now you can use `mean_time` and `standard_deviation` as needed\n",
    "print(\"Mean time:\", mean_time)\n",
    "print(\"Standard deviation:\", standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_genotype = jax.tree_map(lambda x: x[0], init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 0.02882700189948082\n",
      "Standard deviation: 0.005437930463612316\n"
     ]
    }
   ],
   "source": [
    "random_key = jax.random.PRNGKey(0)\n",
    "def mutation_fn():\n",
    "    pga_emitter._mutation_function_pg(policy_params=first_genotype, emitter_state=emitter_state)\n",
    "mutation_fn()\n",
    "\n",
    "timer = timeit.Timer(mutation_fn)\n",
    "results = timer.repeat(repeat=10, number=1)  # Adjust 'repeat' and 'number' as needed\n",
    "\n",
    "# Calculate mean time and standard deviation\n",
    "mean_time = sum(results) / len(results)\n",
    "standard_deviation = (sum((x - mean_time) ** 2 for x in results) / len(results)) ** 0.5\n",
    "\n",
    "# Now you can use `mean_time` and `standard_deviation` as needed\n",
    "print(\"Mean time:\", mean_time)\n",
    "print(\"Standard deviation:\", standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_batch = jax.tree_map(lambda x: x[:int(0.5*config.env_batch_size)], init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 5.0848997985944155\n",
      "Standard deviation: 0.002601589654325258\n"
     ]
    }
   ],
   "source": [
    "def emit():\n",
    "    pga_emitter.emit_pg(emitter_state=emitter_state, parents=half_batch)\n",
    "    \n",
    "emit()\n",
    "\n",
    "timer = timeit.Timer(emit)\n",
    "\n",
    "results = timer.repeat(repeat=10, number=1)  # Adjust 'repeat' and 'number' as needed\n",
    "\n",
    "# Calculate mean time and standard deviation\n",
    "mean_time = sum(results) / len(results)\n",
    "standard_deviation = (sum((x - mean_time) ** 2 for x in results) / len(results)) ** 0.5\n",
    "\n",
    "# Now you can use `mean_time` and `standard_deviation` as needed\n",
    "print(\"Mean time:\", mean_time)\n",
    "print(\"Standard deviation:\", standard_deviation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
